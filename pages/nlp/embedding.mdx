## 코사인 유사도 실습

F는 pd.read.exe F는 pd.read.exe F는 pd.read.exe F는 pd.read.exe F는 pd.read.exe F는 pd.read.exe F는 pd.read.exe F는 pd.read.exe F는 pd.read.exe F는 pd.read.exe F는 pd.read.exe F는 pd.read.exe min은 최소, minimum, df는 document frequency, 문서, frequency는 빈도, 최소한 문서가 몇 개는 되어야 한다.

강의자료에서 min, df는 10, 최소한 10개의 문서에 나온 단어만 포함해라.

우리가 문서 유사도를 계산할 거잖아요.

A라는 문서하고 B라는 문서하고 서로 비교를 할 건데 거기 나오는 단어는 최소한 A에도 나오고 B도 나오고 이래야 되겠죠.

단순히 많이 나온 단어가 아니라 여러 문서에서 공통적으로 나오는 단어야

그걸 가지고 비교를 하는 걸 하겠죠.

비교를 하는 건 이쪽에도 있고 저쪽에도 있어야 비교를 하는 거잖아요.

예를 들면 우리가 국가별 전 세계에 200개 정도 국가가 있는데 국가들 비교를 하려고 하는데 김치를 먹느냐 이런 걸 가지고 비교를 하면 아예 비교가 성립을 안 하겠죠.

김치 먹는 나라가 없으니까.

예를 들면 빵을 먹느냐 밥을 먹느냐 하면 빵 먹는 나라도 많고 밥 먹는 나라도 많으니까 비교가 되잖아요.

그런데 김치를 먹는 것 가지고 비교하면 비교할 나라가 없습니다.

나만하고 국한만 비교하면 더 이상 끝나잖아요.

다 김치 안 먹으니까.

최소한 여러 개의 문서에서 공통적으로 나와야 우리가 비교를 할 수 있기 때문에 우리가 min, df로 설정을 해 줍니다.

이것도 절대적인 숫자는 없는데 그래도 한 10개는 나와야 하지 않겠어?

이렇게 생각을 합니다.

이거는 이제 뭐 맨날 하는 거고 그래서 이제 뭐 특허 관련해서 샴푸 관련된 특허에 나오는 고빈도 단어들 한번 봅시다.

그래서 이제 많이 나오는 단어 보면 샴푸 관련된 특허니까 일단 샴푸가 많이 나오고요.

그다음에 이제 물 많이 나오고 그다음에 특허라는 게 발명 특허니까 발명 많이 나오고 그다음에 이제 조성, 샴푸를 뭐 가지고 만들었냐.

샴푸의 조성 많이 나오고 그다음에 중량, 상기는 그 특허에서 자주 쓰는 표현인데 위에서 기재한 이런 뜻입니다.

상기와 같이 이런 식으로 많이 쓰거든요.

그런 거고.

그다음에 이제 모발을 적용하는 거니까 모발이라고 나오고, 추출물 포함 어쩌고저쩌고 이렇게.

그래서 이거를 보면은 아 뭐 이 샴푸 관련 특허들이 뭔가 추출물 이런 게 많구만 사실 우리 샴푸에 낼 수 있는 특허가 뭐 있겠어요?

어차피 그냥 비눗물인데요.

그래서 이제 맨날 이것도 집어넣고 우리가 생각할 수 있는 거의 모든 풀 종류는 다 들어가거든요.

아까도 보면 첫 번째 특허가 무슨 개운죽 무슨 특허잖아요.

그러니까 무슨 추출물을 가지고 이제 샴푸를 만들었냐

이런 건데.

그러면은 우리가 이제 이 DTM에서 DTM이라는 게 문서단어 행렬이라는 게 문서단어 행렬이라는 게 문서단어 행렬이라는 게 문서단어 행렬이라는 게 이렇게 행렬이 있으면 이 행렬의 가로방향, 이 가로방향 행이 문서가 됩니다.

그리고 이렇게 열이 단어가 되죠.

단어가 되죠.

그러면은 이 DTM에서 행을 뽑아내려면 DTM하고 각과로 한 다음에 0번 하면 0번째 행.

파이서는 숫자를 0부터 세기 때문에 우리가 일반적으로 말하는 기준으로는 첫 번째가 됩니다.

첫 번째 행이라는 게.

그래서 0 이렇게 써도 되고 0 콤마 땡땡 이렇게 써도 됩니다.

땡땡은 뭐냐 하면 모든 이런 뜻입니다.

그래서 행번호를 먼저 쓰고 열 번호를 그 다음에 쓰는데 열 번호를 따로 쓰지 않고 그냥 생략을 하거나 아니면 땡땡 이렇게 쓰면 0번 행에 모든 열 이런 뜻입니다.

0번 행 전체 이런 얘기죠.

그래서 이제 이걸 그냥 생략을 하고 앞뒤처럼 써도 된다.

그 다음에 뒤에 점 A라고 나오는데요.

이거는 뭐냐 하면 우리가 이제 DTM의 0번 이렇게 해서 써보면 이 내용을 안 보여주고 그냥 1 곱하기 362 스팔스 매트릭스 어쩌고 이렇게 나와요.

그래서 여기 보시면 이제 Compressed라는 말이 나오는데 Compressed는 압축된 이런 뜻입니다.

그러니까 여기서 Sparse라는 얘기는 대부분 숫자가 0이다

이런 뜻입니다.

대부분 숫자가 0이기 때문에 용량을 많이 잡아 먹는다면 그러면은 이걸 굳이 데이터를 다 저장해봤자 어차피 대부분 0이니까 너무 메모리를 많이 잡아 먹죠.

그래서 압축을 해놨습니다.

압축이 되어 있기 때문에 우리한테 내용이 안 보여요.

그래서 이거를 압축을 풀어서 보려면 뒤에다가 대문자 A를 이렇게 점 A 이렇게 붙여주시면 됩니다.

그러면은 대부분 0이죠.

왜 대부분 0일까요?

어차피 이 특허에 모든 단어가 다 들어가 있지는 않을 거 아니에요.

어떤 단어는 들어가고 어떤 단어는 안 들어가고 이렇게 때문에 대부분 0입니다.

그래서 이 Sparse 매트릭스의 Sparse라는 뜻이 대부분 0이다

이런 뜻이고 그래서 쓸데없는 대부분 0이니까 대부분은 쓸데없는 공간만 자릴만 차지한단 말이에요.

그래서 이거를 이제 압축을 해놓은 형태입니다.

그래서 우리가 압축을 풀어서 보려면 그래서 이게 0번, 아까 무슨 개운주가 어쩌고 특허인데 그 특허에는 이 단어 한 번 나오고 이 단어 한 번 나오고 이 단어 두 번 나오고 이런 식의 특허가 됩니다.

그 다음에 이제 우리가 단어 벡터를 한번 볼 건데요.

일단 단어도 0번 단어를 보면 되죠.

이 단어 벡터를 한번 볼 건데요.

일단 단어도 0번 단어를 보면 되지만 추출이라는 단어를 찾아가지고 이 추출이라는 단어의 번호에 해당하는 열을 한번 보도록 하겠습니다.

그래서 우리가 일단 먼저 여기 Get Feature Names Out 하면은 단어 목록이 쭉 나오거든요.

단어 목록을 ToList 함수를 이용해가지고 리스트 자료형으로 바꿔줘요.

그러면 이제 리스트로 왜 바꾸냐면 리스트에는 index라는 함수가 있습니다.

그래서 index에다가 추출 이렇게 해보면은 번호가 나오거든요.

그러면 이제 이 j가 280번이니까 j가 280번이니까 j가 280번이니까 j가 280번이니까 추출이라는 단어가 저 단어 목록에서 몇 번째에 있는지를 보려고 한 거예요.

그러면은 이 j가 280번인데 그러면 이제 DTM에서 0,0,0,0,0 하면은 이 자리에다 0,0 이렇게 써줬는데 지금은 이제 행이 앞에 나오기 때문에 앞에 나오는 거를 생략을 할 수는 없어요.

왜냐하면 이제 열의 위치는 행 열이라서 열이 두 번째니까 행 자리에 뭐라도 써줘야 되거든요.

행 자리에 그냥 0,0 이렇게 써주면 모든 행의 제이번째 열 이런 뜻이랍니다.

이것도 이제 그냥 치면은 압축되어 있다고 나와요.

그래서 점 A 이렇게 대문자로 써주면은 압축이 풀려서 나오는데 이거 열이니까 이제 세로로 쭉 나오고 있어요.

그러면은 이제 이 얘기는 뭡니까?

추출이라는 단어는 0번 문서에는 0번 나왔어요, 안 나왔어요.

그 다음에 1번 문서에 한 번 나왔고, 그 다음에 또 쭉 안 나오다가 여기서 한 번 나왔고 또 안 나오다가 여기서 세 번 나왔고, 쭉 안 나오다가 한 번 나왔고, 쭉 안 나오다가 여기서 한 번 나왔고, 또 안 나오다가 여기서 세 번 나왔고, 쭉 안 나오다가 한 번 나오고, 네 번 나오고 뭐 이렇게 되어있습니다.

그럼 이 단어는 문서마다 나오고 안 나오고 이런 게 차이가 있는데 그럼 우리가 이거를 어떤 이 단어를 표현하는 한 가지 방법으로 쓸 수가 있어요.

이 문서에 나왔다, 안 나왔다.

그러면은 우리가 가정을 해볼 수 있는 거죠.

예를 들어서 두 단어가 비슷하다는 거는 같은 문서에는 이 단어가 나오면 저 단어가 나오고 만약에 이 단어가 안 나오면 저 단어가 안 나오고 그럼 두 단어는 같은 단어라고는 할 수 없어도 뭔가 관련이 있다고 할 수 있겠죠.

나올 때는 같이 나오고, 안 나올 때는 같이 안 나오고 이런 뭐가 비슷하다는 거잖아요.

그럼 이거를 우리가 어떤 단어들 간에 유사도를 계산하는 한 가지 소재로 쓸 수가 있습니다.

지금 우리가 이제 이게 세로로 길쭉하게 나오니까 보기가 좀 싫거든요.

그래서 이거를 끝에다가 점 플랫튼이라고 붙여주면 아까 그 문서 행렬처럼 그냥 옆으로 평평하게 이렇게 나오게 됩니다.

플랫튼은 평평하게 해줘라

이런 거.

그래서 이제 이렇게 나오게 됩니다.

그러면 이제 우리가 코사인 유사도를 계산을 해야 되는데 코사인 유사도 계산해 주는 함수가 있습니다.

코사인 시밀러리티라고 있는데 코사인 유사도 계산해 주는 함수에 넣기 전에 dtm.t 해가지고 대문자 t를 붙여주거든요.

이거를 왜 붙여주냐

하면 전치 행렬이라는 게 있는데요.

전치 행렬이라는 게 있는데 전치 행렬이라는 게 아니라 전치 행렬이라는 게 있는데 전치 행렬이라는 게 있는데 전치 행렬이라는 게 있는데 전치 행렬은 전치가 뒤집는다는 뜻입니다.

뒤집는데 행하고 열을 뒤집는 거를 전치시킨다고 해요.

근데 이게 조금 처음 들으면 되게 헷갈리거든요.

왜냐하면 우리가 행하고 열을 뒤집는다고 생각하면 어떻게 생각하시냐 하면 종이가 이렇게 있으면 이렇게 돌린다고 생각을 해요.

근데 이게 아닙니다.

좀 복잡한데 종이가 이렇게 있으면 꼭지점을 잡고 이렇게 뒤집는 거예요.

그래서 헷갈려요.

이거를 이렇게 회전시키는 게 아닙니다.

이렇게 뒤집는 거예요.

그래서 왜 그렇게 이상하게 하냐?

이렇게 싱글 돌리면 안 됩니까?

왜 그러냐 하면 여기 보시면 어떻게 되냐면 숫자가 123456789 이렇게 돼 있죠.

그럼 가로로 첫 번째 행에 123이 들어가잖아요.

전치를 시키면 세로로 첫 번째 열에 123이 들어가요.

이거를 그냥 빙글 이렇게 돌리면 어떻게 돼요?

123이 제일 오른쪽으로 가버리겠죠.

그냥 돌리면 이 순서가 다 뒤집혀 버리거든요.

이거를 상상력이 필요한데 빙글 돌리면 첫 번째 줄이 이렇게 돌려버리면 제일 오른쪽으로 가잖아요.

순서가 안 맞는단 말이에요.

첫 번째 행이 첫 번째 열로 가고 두 번째 행은 두 번째 열로 가고 이렇게 해야 하기 때문에 뒤집을 때 이렇게.

그래서 이게 말로 설명이 되긴 하는데 익숙해지면 괜찮은데 처음에는 우리가 이렇게 뒤집는 일이 잘 없기 때문에 좀 헷갈립니다.

사실 엑셀에도 복사해서 선택해서 붙여넣기 보면 전치시키는 게 있어요.

행과 열 바꾸기 이렇게 있는데 똑같이 이렇게 됩니다.

어쨌든 이게 이제 우리 데이터 분석에서 되게 많이 사용하는 거기 때문에 알아주시면 좋아요.

이렇게 전치를 시키면 행이 열로 가고 열이 행으로 가는데 그러면 문서 단어 행렬에서는 행이 문서고 열이 단어잖아요.

그러면 이거를 전치를 시키면 어떻게 되냐면 단어 문서 행렬이 됩니다.

그래서 행이 단어 열이 문서 이렇게 바뀌어요.

그래서 이 코스아인 유사도 함수는 기본적으로 행하고 행의 유사도를 계산을 해요.

그래서 그냥 DTM을 넣으면 어떻게 되냐면 문서와 문서의 유사도를 구해줍니다.

그럼 우리는 단어와 단어의 유사도를 구하고 싶으면 어떻게 되냐.

이렇게 .t를 붙여서 전치를 시켜줍니다.

그러면 이거는 DTM.t 하면 문서 단어 행렬이 아니라 단어 문서 행렬이기 때문에 그러면 이제 행하고 행의 유사도를 구하면 단어와 단어에 유사도가 구해져요.

좀 헷갈리죠?

그래서 워드 시뮬러리티의 쉐입을 보면은 360이 이렇게 나오는데 DTM의 쉐입을 보면은 439, 360이 이렇게 나오죠.

439가 문서의 개수고 362개가 단어의 개수입니다.

그래서 이제 단어하고 단어를 비교했기 때문에 가로 362, 세로 362 이렇게 나오죠.

그러면은 우리가 아까 제2번째 추출이라는 단어하고 비슷한 단어들을 유사도 순으로 볼 건데 여기 코드가 조금 복잡해요.

그래서 이제 워드 시뮬러리티 j 하면은 추출이라는 단어 j번째가 제2번째가 유사도 순으로 볼 건데 그러면은 우리가 아까 제2번째 추출이라는 단어하고 비슷한 단어들을 유사도 순으로 볼 건데 여기 코드가 조금 복잡해요.

그래서 이제 워드 시뮬러리티 j 하면은 추출이라는 단어 j번째가 지금 추출이잖아요.

추출하고 단어들의 유사도가 쭉 나옵니다.

하나씩 보여줘요.

워드 시뮬러리티 j 이렇게 하면은 이제 다른 단어하고 우리가 362개의 단어에 대해서 0번 단어는 추출이랑 유사도가 0.072에요.

거의 0이니까 유사도가 거의 없다

이렇게 볼 수 있겠죠.

그 다음에 1번 단어는 0.13입니다.

유사도가 좀 높죠.

이런 식으로 2번 단어는 추출이랑 0.1, 3번 단어는 0.01 뭐 이런 식으로 유사도가 쭉 있어요.

유사도가 높다는 건 뭡니까?

같은 문서에 자주 나오더라.

예를 들면 우리가 경찰인데 어떤 사람이 용의자로 잡혀왔어요.

아유, 저는 그 사람은 몰라요.

너 김철수라는 사람 알지?

아, 저 김철수 모른대요.

거짓말하고 있어.

너 김철수 가는 데마다 너 똑같이 CCTV 보니까 김철수 찍히면 너도 찍혀있다만.

이러면 아유, 저는 김철수 모른대요.

하지만 CCTV에 맨날 김철수가 잡힐 때마다 같이 나오면 수상하잖아요.

범인이라는 증거는 안 되겠지만.

그러니까 이 단어는 0.21이라는 건 뭐냐 하면 아주 완전히 똑같이 나오지는 않더라도 대체로 추출이라는 단어가 나올 때마다 어쨌든 조금 나오는 겨향이 있기는 있다라는 수상한 거죠.

그러면 우리가 이제 이거를 유사도가 높은 순서대로 보고 싶은데 그러면 이걸 정렬을 해야겠죠.

그때 하는 방법 중에 하나가 ad-sort라는 함수를 써주면 됩니다.

ad-sort라는 함수는 좀 특이한데 그냥 sort를 하면 숫자 자체를 순서대로 정렬을 해주거든요.

숫자를 순서대로 정렬해버리면 예를 들면 지금 유사도가 쭉 뒤로 가보면 제일 높은 게 1이고 그다음이 0.36인데 1은 자기 자신이에요.

자기 자신과 자기 자신의 유사도가 항상 1입니다.

100% 똑같겠죠.

이거는 별로 중요하지 않고 두 번째가 중요한데 그런데 문제는 0.36이 뭐였는지 알 수가 없잖아요.

이렇게 되면.

그래서 그냥 sort를 하지 않고 ad-sort를 하면 숫자 자체를 정렬을 해주는 게 아니라 그 숫자가 몇 번째인지를 정렬을 해줍니다.

무슨 말이냐면 280번이 제일 뒤로가 있잖아요.

기억나실지 모르겠지만 아까 추출이 몇 번이었냐면 280번입니다.

280번하고 280번이 제일 똑같겠죠.

왜냐하면 나랑 제일 똑같은 사람 찾으면 누굽니까?

나겠죠.

세상에서 나랑 제일 똑같은 사람 찾으면 나 자신일 거 아니에요.

나 자신보다 나랑 더 똑같은 사람이 있을 순 없잖아요.

그럼 이상한 거죠.

아니 나보다 더 나 같은 사람이 있어?

그럼 나는 누구지?

이렇게 되는 거잖아요.

보통 우리가 이렇게 하면 찾을 때 나는 빼고 얘기하는 거 아닌가 하는데 지금은 나도 포함이 되어 있으니까 280번하고 제일 비슷한 건 280번이에요.

그런데 그 다음으로 비슷한 거는 295번입니다.

그 다음은 273번.

그 다음은 327번.

이런 식으로 간다.

ad-sort는 숫자 자체를 정렬하는 게 아니라 그래서 정렬 됐을 때 걔가 원래 몇 번이었냐를 보여줍니다.

그러면 우리 이제 끝에서부터 뒤로 쭉 가면 되겠죠.

280번, 295번, 273번 이렇게 가는데 여기서 이제 파이썬 문법이 하나 있어요.

뭐냐 하면 각바로 하고 마이너스 1 이런 식으로 하면 맨 끝에서 첫 번째 이런 뜻입니다.

마이너스 2 하면 맨 끝에서 두 번째 이런 뜻입니다.

그래서 지금 강의 자료를 보면 마이너스 2, 0, 0, 마이너스 11 이렇게 되어 있는데 이건 뭐냐면 뒤에서 두 번째부터 마이너스 2, 마이너스 3, 마이너스 4, 마이너스 5, 마이너스 6 이렇게 가라는 거예요.

그래서 마이너스 11까지 가라는 건데 파이썬은 마지막 숫자 포함 안 하니까 마이너스 10까지 보여달라고 하죠.

근데 그냥 이렇게 하면 아무것도 안 나와요.

왜 아무것도 안 나오냐면 파이썬은 기본적으로 숫자를 셀 때 플러스로 셉니다.

마이너스 2였으면 그 다음에 플러스 1에서 마이너스 1로 가야 되거든요.

그럼 마이너스 2에서 마이너스 11로 가지를 못해요.

그래서 끝에다가 0, 0하고 마이너스 1을 하나 더 붙여주는데 이건 뭐냐면 숫자를 셀 때 역순으로 셀 수 있어요.

마이너스 2에다가 마이너스 1 더 하면 어떻게 됩니다.

마이너스 3, 마이너스 4, 마이너스 5 이렇게 가라는 거예요.

그러면 295, 273, 327, 95, 334 이렇게 있는데 이게 뒤에서 두 번째, 뒤에서 세 번째, 뒤에서 네 번째 이렇게 하는 거예요.

이건 약간 파이썬 문법에 여러 가지 알려 드리면 마이너스 1 대신에 마이너스 2 하면 어떻게 될까요?

두 칸씩 건너뛰는다.

그래서 295에서 327로 바로 가요.

마이너스 1 하면 295, 273, 327이 안 되니까 327이...

두 칸씩 건너서 하려면 마이너스 2 하면 되는데 지금은 두 칸씩 건너서 할 필요는 없겠죠.

그래서 이제 4K, 이렇게 해서 프린트 워즈에서 K번째 출력해라

이렇게 하면은 네, 추출이랑 제일 비슷한 단어, 제일 같은 문서에 자주 나온 단어는 탈모가 되고요.

그다음에 첨가, 탐유, 방법, 혼합, 제조 이런 식으로 나갑니다.

그러니까 추출이랑 탈모가 같은 문서에 자주 나오는 이유가 뭘까요?

보통 샴푸에 기능성을 더해야 되는데 제일 좋은 게 탈모 방지 이런 거죠.

그러니까 탈모 방지 이런 거.

이런 식의 얘기가 많이 나오기 때문에 추출하고 탈모가 유사도가 제일 높긴 합니다.

근데 이제 일리는 있는데 문제는 추출하고 탈모는 직접적으로 똑같은 단어는 아니잖아요.

예를 들면 우리 아까 많이 나온 단어 목록에 보면 추출 무리 473번이 나왔어요.

근데 추출하고 탈모가 유사도가 높은 단어는 473번이 나왔어요.

근데 추출하고 추출물은 거의 똑같은 단어잖아요.

근데 유사도가 별로 높게 나오질 않습니다.

왜 그러냐, 보통 우리가 추출이라는 단어를 쓰면 추출 물이라는 단어를 굳이 안 써도 되겠죠.

예를 들면 내가 개운죽을 추출해서 샴푸를 만들겠다.

이러면 추출물이라는 단어를 또 안 써도 되잖아요.

반대로 추출물이라는 단어를 쓰면 당연히 추출물은 뭔가를 추출한 거니까 추출이 한 단어를 안 써도 된다는 거예요.

그래서 아이러니하게도 두 단어가 너무 의미가 비슷해져 버리면 한 쪽을 쓰면 다른 쪽을 굳이 쓸 필요가 없으니까 잘 안 나오게 됩니다.

그래서 오히려 유사 단어인데 오히려 안 겹치는 그런 경우가 생길 수가 있어요.

우리가 이런 식으로 어떤 단어 유사도를 볼 수도 있는데 이렇게 보면은 어떤 일반적으로 국어사전에 나오는 이런 식의 유사도라기보다는 두 단어가 같은 맥락에서 자주 사용된다.

이 정도를 보여주는 거지

정말 우리가 일반적으로 이 단어와 비슷한 단어는 그 비슷한 단어를 찾는 방법은 아니게 됩니다.

그런데 우리가 이런 게 필요할 때도 있고 예를 들면 내가 추출이라는 단어가 어떤 단어와 함께 자주 쓰이는지를 보고 싶을 때는 이 방법을 쓰는 게 맞겠지만 정말 추출이라는 단어가 의미상 비슷한 단어를 찾고 싶을 때는 이렇게 하면 별로 좋은 방법이 아니겠죠.

그러면 어떻게 하느냐?

그거는 잠깐 쉬었다가 알겠습니다.

## 코사인 유사도와 상관계수

퍼스타인 유사도가 처음 들으시면은 딱 지금 생각하시는 것처럼 대충 반응성이 같다는 걸 알겠는데 그게 실제로 어떻게 연관이 되느냐

이게 좀 의아하시기가 쉽지 않아요.

그래서 일단 코사인 유사도가 뭐랑 연관되는 개념이냐면 여러분 상관계수 혹시 아십니까?

상관계수랑 똑같은 개념이거든요.

구하는 방식은 다르지만 결과적으로 똑같은 개념이에요.

상관계수란 게 뭡니까?

뭔가 하나가 클 때 예를 들면 그림으로 그리면 뭐가 숫자가 클 때 높고 낮을 때 낮고 이런 식으로 나오면 상관계수가 높게 나오죠.

상관계수가 낮다는 건 뭐예요?

반대로 나오면은 상관계수가 마이너스인데 상관계수가 0이다

이러면은 특별한 방향성 없이 이렇게 나오면 상관계수가 0인 거잖아요.

그래서 퍼스타인 유사도는 여러 가지 방식으로 생각할 수가 있는데 일단 수학적으로 상관계수랑 사실상 똑같습니다.

그래서 어떻게 생각할 수 있냐

면 아까 제가 설명드렸듯이 어떤 단어가 있는데 A라는 문서에 이 단어가 나오면 저 단어도 나와요.

B라는 문서에는 이 단어가 안 나왔는데 저 단어가 안 나왔어요.

그러면은 상관계수의 개념이 그거잖아요.

높을 때는 높고 낮을 때는 같이 높을 때는 같이 높고 낮을 때는 같이 낮고 이러면 상관계수가 높은 거잖아요.

그래서 커사인 유사도도 그럴 때 높습니다.

똑같은 개념이기 때문에.

그렇게 이해할 수도 있고 상관계수로서 상관계수와 같은 의미로 이해할 수도 있고 아니면 우리가 방향성이라는 게 잘 안 와닿는 이유가 우리가 문서 1이 있고 문서 2가 있고 문서가 400개가 있기 때문에 우리가 상상력이 필요한데 400 몇십 차원의 문서가 몇 개 있었죠?

문서가 439개가 있거든요.

단어 유사도를 비교한다는 거는 439 차원 공간에서의 방향성을 비교를 한다는 거예요.

근데 우리가 인간이 한 3차원을 넘어간 상상력의 한계 때문에 상상이 안 되거든요.

여러분 4차원을 한번 상상을 해보세요.

3차원인데 축이 거기다 3차원까지 상상이 되잖아요.

가로등 높이가.

4차원부터 우리가 상상이 안 돼요.

그래서 사실 그래서 힘든 건데 편의사

그래서 좀 이해하기 쉽게 3차원 3차원까지만 생각을 해보면 어떤 단어가 있는데 이 단어가 문서 1에서는 많이 나왔어요.

3차원도 힘드니까 2차원 간격만 가져갑시다.

문서 1에서는 많이 나왔는데 문서 2에서는 별로 안 나왔다.

그러면 화살표로 그리면 이런 식으로 되겠죠.

문서 1에서는 많이 나왔으니까 이렇게 되겠죠.

근데 만약에 어떤 단어가 문서 2에서는 많이 나왔는데 문서 1에서는 별로 안 나왔다.

화살표가 이렇게 되겠죠.

만약에 어떤 단어가 문서 1에도 많이 나오고 문서 2에도 많이 나왔다.

그럼 화살표가 이렇게 됩니다.

그래서 이렇게 되면 이 두 단어가 방향성이 이렇게 좁아, 각도가 좁아지니까 코사인 유사도는 각도가 좁아야 올라옵니다.

근데 만약에 어떤 단어가 문서 1에도 많이 나오고 문서 2에도 별로 안 나왔다.

그럼 이제 각도가 거의 똑같아지겠죠.

이해됐어요.

그러니까 이거를 이렇게 각도로 이해하면은 이게 코사인 유사도의 개념이고 상관계수로 이해하면은 여기는 축이 달라요.

이거는 이제 축이 단어와 단어가 됩니다.

단어 1 아, 단어 1, 단어 2가 아니지.

잠깐만요.

저도 헷갈려요.

여기서는 축이 축이 뭐지?

잠깐만요.

갑자기 헷갈려요.

축이 축이 이제 여기서 단어 1을 단어 2가 1, 단어 2 그래서 이제 이 점들은 뭐냐면 여기서 이제 점들이 문서가 돼요.

문서 1, 문서 2, 문서 3, 문서 3 그래서 단어 1하고 단어 2를 다 같이 넣으면은 문서 3, 문서 4, 문서 5 그래서 단어 5가 문서 2가 되니까 문서 4 그래서 그 단어 1하고 단어 2를 비교하는데 문서 1에서는 둘 다 많이 나왔어요.

문서 4에서는 둘 다 별로 안 나왔어요.

그러면은 이게 이제 상관계수는 이런 식으로 똑같은 내용인데 관점을 달리해서 보는 거예요.

그럼 우리가 이제 그냥 쉽게 보기 위해서 문서 1하고 문서 2만 있다고 봅시다.

그러면은 이럴 때는 이제 상관계수가 높아지거든요.

문서 1에서는 둘 다 많이 나왔고 문서 2에서는 둘 다 별로 안 나왔고 이렇게 되면은 상관계수가 높다

이렇게 얘기하는데 이거를 그림을 바꿔서 그리면 문서 1에서는 둘 다 많이 나왔고 문서 2에서는 둘 다 별로 안 나왔죠.

이쪽으로는 방향성이 없으니까 그러면은 코사인 유산으로 높다

이렇게 보니까 코사인 유산으로랑 상관계수는 똑같은 개념입니다.

이렇게 볼 수도 있고 저렇게 볼 수도 있는데 이게 그냥 근데 한 번 들으셔가지고 약간 이해가 되고 이러지는 않습니다.

그다음에 이제 이 방향성이 같다는 게 좀 이해가 안 되실 수 있는데 여기서 말하는 방향성은 나올 때 가치 많이 나오느냐

이 의미의 방향성입니다.

이거를 그림으로 이렇게 그려서 그런데 또 다른 게 이해하면 우리가 이제 단어 문서 행렬을 그리면 단어 1이 있고 단어 2가 있으면 문서 1, 문서 2, 문서 3 뭐 이런 식으로 있겠죠.

그러면은 방향성이 비슷하다는 건 뭐냐면 문서 1에서 단어 1이 많이 나왔어요.

그럼 단어 2도 어때야 됩니까?

유산으로 높다면.

많이 나와야겠죠.

만약에 문서 2에서 단어 1이 별로 안 나왔어요.

0이라고 합시다.

그럼 단어 2도 0이든지 아니면 뭐 좀 숫자가 작게 나와야 됩니다.

뭐 똑같이 해.

그다음에 문서 3에서 만약에 단어 1이 많이 나왔어요.

그러면 단어 2도 많이 나와야 됩니다.

이거를 그림으로 그리면 이거를 그림으로 그리면 문서 1이 있고 문서 2가 있고 3천원이죠.

문서 3이 있으면 문서 1에서는 둘 다 많이 나왔으니까 단어 1이나 단어 2나 다 이쪽에 있겠죠.

그다음에 문서 2에서는 둘 다 안 나왔으니까 둘 다 이쪽에 있겠죠.

문서 3에서는 많이 나왔으니까 이쪽에 있겠죠.

그러면은 그림으로 그림 어떻게 되냐면 화살표가 이렇게 있거든요.

이게 근데 이제 이게 왔다 갔다 하는 게 굉장히 힘들거든요.

상상력이 좀 필요합니다.

이게 공단으로 생각했다가 숫자로 생각했다가 왔다 갔다 하려면 힘들어요.

그래서 이제 방향성이라는 개념이 헷갈리시는데 특히 이제 그래서 이거는 편한 방법으로 이해하시면 됩니다.

다 왔다 갔다 하면서 이해하려고 하면은 굉장히 헷갈려요.

그래서 이게 사람마다 스타일이 있거든요.

이게 어떤 말이나 숫자로 이해하는 게 편하신 분이 있고 그림으로 이해하시는 게 편한 분이기 때문에 개인적인 스타일이 있잖아요.

자기 스타일을 이해하는 게 편하시잖아요.

그래서 이게 이게 이게 그림으로 이해하시는 게 편한 분이기 때문에 개인적인 스타일이 있잖아요.

자기 스타일에 맞춰서 이해하시면 됩니다.

세 가지 방식으로 설명을 드렸는데 이게 다 결국 같은 설명이에요.

이게 또 왜 같은 설명인지 하면 또 되게 헷갈리니까 그냥 자기가 원하는 방식 하나를 골라서 그 방식으로 이해하시면 됩니다.

그래서 일단 표로 이해를 하면은 하나가 숫자가 높을 때 다른 하나가 같이 높고 하나가 낮을 때 다른 하나가 같이 낮고 화살표라고 할 수 있어요.

여기가 높을 때 같이 높고 높고 난

에 패턴이 똑같다.

그러면은 같다고 볼 수 있는 겁니다.

사람으로 이해하면 어떤 사람은 국어를 잘하고 수학은 좀 못하는데 과학을 잘해요.

또 다른 사람을 봤더니 그 사람도 국어를 잘하고 수학은 좀 못하는데 과학을 잘해요.

그러면 두 사람은 좀 성적 패턴이 비슷하다고 할 수 있잖아요.

유사도가 높다고 할 수 있잖아요.

그렇죠?

너도 국어랑 과학 좋아해?

나도 그럼 국어랑 과학 좋아해.

이렇게 얘기할 수 있잖아요.

그렇죠?

그래서 이제 이거를 숫자로 이해하면 이런 식으로 이해하고 이거를 그림으로 그리는데 문서를 축으로 해가지고 단어를 화살표로 그리면 얘네가 화살표 이런 식으로 되는 애들은 화살표가 과양성이 비슷하게 된다면 그럴 수밖에 없겠죠.

애초에 숫자가 그런데 이제 그 과양성 얘기가 예를 들면 화살표로 제가 그려놔서 그런데 예를 들어서 얘가 여기서는 5번 여기서는 안 나오고 여기서는 5번 나왔어요.

얘는 여기서 3번 안 나오고 3번 나왔어요.

그럼 숫자 자체는 다르죠.

화살표로 그리면 어떻게 되냐면 이런 식으로 나오게 됩니다.

하나는 5까지 갔고 하나는 3까지 갔는데 예를 들면 어떤 사람은 국어랑 과학을 좋아해가지고 국어랑 과학을 좋아하면 전공을 뭘 해야 될까?

모르겠네.

국어랑 과학을 좋아해가지고 과학기자가 됐어요.

국어랑 과학을 좋아하면 과학기자가 됩니다.

또 다른 사람도 국어랑 과학을 좋아해서 그런데 신문사에 들어가려니까 성적이 안 돼서 과학 블러버가 됐어요.

둘 다 어쨌든 성향은 비슷한 거죠.

실력에 좀 차이가 있는 거지.

둘 다 국어랑 과학을 잘하는데 다른 것보다는 나보고 수학을 시키라고 하면 차라리 국어랑 과학을 하려는 그래서 이 사람은 국어랑 과학을 되게 잘해.

그래서 되게 유명하신 분의 과학기자가 됐어요.

어떤 사람이 국어랑 과학을 좋아하긴 하는데 기잘 만큼 잘하지 못해.

다른 것보다는 계속 국어랑 과학을 잘해.

그래서 아, 나 뭐 할까 하다가 과학 블러버를 해야겠다.

과학 유튜버를 해야겠다.

자격은 필요 없잖아요.

다른 것보다는 내가 국어랑 과학을 잘하니까 과학에 관련해서 말로 설명하는 건 남들보다 잘하니까 나는 과학 유튜버가 되겠다.

그럼 국어랑 과학을 잘해서 이상은 과학 유튜버가 됐어.

그러면 과학기자는 시험도 보고 해야 되겠지만 유튜버는 그냥 자격이 없다고 하면 시험도 보고 해야 되겠지만 유튜버는 자격이 없다고 하면 되잖아요.

어쨌든 실력과는 무관하고 어쨌든 두 사람은 성향 자체에는 비슷한다면 이해 되실까?

가능성이 간단하게 그 얘기예요.

개별적인 숫자 자체가 아니라 얘네가 어떤 방향으로 가있냐

이걸 보겠다.

숫자로 쓰면 이렇게 되고 그림으로 그리면 이렇게 그릴 수도 있는데 그림을 이렇게 그릴 수도 있습니다.

이렇게 그리면 상관계수의 그림이 되는 거고 이렇게 그리면 코사인 유산도의 그림이 되는 거고 근데 이걸 그냥 숫자로 나타내면 결국 이렇게 된다.

그 얘기입니다.

방향성이 같다는 거는 이렇게 이해할 수 있습니다.

됐을까요?

이게 지금 이해 되신 것 같아도 내일 되면 또 헷갈리거든요.

그래서 영상을 다시 한번 보시면 됩니다.

저도 보면 설명하다가 헷갈리잖아요.

원래 헷갈려요.

다 같다 합니다.

그래서 제일 좋은 방법은 남들한테 설명해주시면 돼요.

세 가지 중에 하나를 골라서 남들한테 설명을 해보시면 제일 기억에 오르납니다.

그 경우에는 많은 문장비대기 전부장에 크고 작고 중간에 있다

이런 경우에는 유사도 그날로 끌 거고 가능성이 없는 거니까 그것에 대해서 갈 수도 지지 좀 해주세요.

저 정확합니다.

아니면 제가 듣다가 생각나는 건데 예를 들면 아빠랑 아들이랑 닮았다라고 할 때 아빠는 키가 크고 아들은 키가 작지만 그래도 닮았잖아요.

그러니까 숫자 크기가 지금 코사인 유사도의 핵심 아이디어는 뭐냐면 아빠는 키가 크고 아들은 키가 작고 아빠는 얼굴이 크고 아들은 얼굴이 작고 아빠는 얼굴이 크고 아들은 얼굴이 작겠지만 그래도 어떤 다양성은 비슷하잖아요.

그래서 우리가 닮았다고 하는 거잖아요.

코사인 유사도는 숫자 크기 자체가 아니라 클 때는 크고 작을 때는 작은데 예를 들면 아빠는 코가 크고 입술이 작아요.

근데 아들도 코가 크고 입술이 작은데 전체적인 사이즈는 다 작겠죠.

아들은.

그 숫자 자체가 아니라 클 때 크고 작을 때 작은 그 패턴이 같으냐

다르냐를 보는 거죠.

## PPMI

자 그럼 다음으로 이제 넘어가서 우리가 이제 PPMI라는 거를 한번 알아볼게요.

PPMI는 사실 우리가 텍스트 분석을 기준으로 얘기를 드리는데 어차피 텍스트 분석이라고 해도 이것도 결국 통제나 머신러닝 데이터 사이언스에 속하는 거기 때문에 여기 나오는 개념들이 다른 데도 나옵니다.

그래서 텍스트 분석하다가 좀 잘 이해가 안 되시면 또 다른 데 가면은 통계적인 다른 개념에서 또 나오거든요.

아 그때 텍스트 분석에서 하는 얘기가 이해기였군.

또 이해 안 되시면 또 딴 데 가면 또 나와요.

그래서 계속 공부하시다 보면은 그러다 보면은 자기 입맛에 맞는 어떤 경우가 딱 생기거든요.

텍스트 분석에서 이해가 안 되던 게 이미지 분석으로 예를 들면은 또 이해가 되시는 분이 있고 이미지 분석에서 이해가 안 되면 또 딴 데 가서 이해가 되는 경우가 있고 그러니까는 수업들 간에 비슷비슷한 걸 또 비교해서 보시다 보면은 또 이해가 갈 때가 있고 그렇습니다.

그래서 뭐 당장 이해가 안 되셔도 너무 그렇게 답답하게 생각하지 마시고 다른 데에 또 비슷한 얘기가 나올 테니까 마음 한 구석에 담아주세요.

자 그 다음에 이제 우리가 이번 시간에 할 거는 PPMI인데 일단 PPMI는 PMI 앞에 P가 더 붙은 거예요.

일단 PMI가 뭔지 알겠죠?

이것도 이름이 굉장히 어려운데 Pointwise, Mutual, Information 줄여서 PM, PMI라고 합니다.

Pointwise는 이제 Point은 덤이죠.

Wise가 붙으면 이제 뭐만스러운 이런 뜻입니다.

그냥 이제 점 이런 거고.

Mutual은 서로, 상호 이런 거고.

Information은 정보 이런 건데.

사실 이제 이거는 전형적으로 이름만 보면은 무슨 뜻인지 알 수 없는 그런 용어입니다.

그러니까 그래서 사실 이제 이런 거는 우리가 이제 용어가 어려우면 좀 싫어하시는 분들이 많이 있거든요.

용어가 어려우면 좋아하셔야 됩니다.

왜냐하면 이제 남들한테 가서 잘난 척을 할 수 있잖아요.

아 이게 Pointwise, Mutual, Information 하고 사람들이 뭐야

뭐야

그러면은 아 그게 말로 설명하기는 어려워.

그래서 어차피 그 여러분들이 이 문을 딱 저 문을 열고 나가면 세상 사람들 PMI라는 게 있는지도 몰라요.

그런데 우리는 어쨌든 이 용어를 알잖아요.

그러면은 이 용어 뜻이 뭔지 몰라도 어쨌든 남들보다는 한 걸음 앞서 있는 겁니다.

그렇게 생각하시면 용어가 어려우면 오히려 좋은 거다.

왜냐하면 이제 우리가 빅데이터 이런 건 남들도 아 빅데이터 뭐 이런 거 아니야.

뭐 안한 척한단 말이에요.

근데 기분 나쁘잖아요.

토요일마다 2년이나 열심히 공부해서 AI, 빅데이터 이렇게 1억세 하나 썼는데 남들이 막 AI, 빅데이터 이런 거 아니야.

이러면 되게 기분 나쁘단 말이에요.

나는 2년이나 공부했는데 자기는 공부도 안하고 뭐 안한 척.

그러면 PMI 이런 건 남들이 그게 뭐야?

이러면 아 역시 내가 공부하는 보람이 있구나.

하하 이런 거죠.

그래서 용어가 어려우면 오히려 좋다.

어쨌든 PMI라는 게 있는데 PMI가 말 자체가 되게 어렵습니다.

두 단어가 독립일 경우 함께 나타나는 확률에 비해 실제 확률이 얼마나 높은지를 나타내.

그러면 되게 말이 어려운데 하나씩 보면은 일단 독립이라는 건 뭐냐면 동전 던지기 할 때 동전을 두 개를 던지면 어떻게 됩니까?

동전 하나가 나올 확률이 앞면이 나올 확률이 50%고 또 하나가 동전이 앞면이 나올 확률이 50%면 둘 다 앞면이 나올 확률은 우리 학부대 때 배우죠.

곱하기 하죠.

그래서 25%가 됩니다.

근데 그러면은 이 밑에가 그거예요.

두 개의 확률을 곱하는 거.

근데 예를 들면은 어떤 두 개의 동전이 있는데 그러면 둘 다 던졌을 때 둘 다 앞면이 나올 확률이 25%가 되어야 되는데 그게 아니에요.

예를 들면 둘 다 이상하게 앞면이 같이 자주 나와요.

그러면은 이게 이제 실제로 이거는 이론적으로 폭패 공고고 실제로는 앞면, 앞면 나오는 경우가 훨씬 많아요.

아니면 훨씬 적어요.

그럼 뭐가 이상한 거죠.

뭔가 서로 간에 뭔가 정보가 있는 겁니다.

독립이라는 거는 동전을 두 개 던지면 동전 끼리 얘기해서 동전이 땡그러로 굴러서 앞면이 나왔어요.

그 다음 동전 던지는데 첫 번째 동전이 야 나 앞면 나왔다.

너도 앞면 나오라.

이렇게 하면은 그건 독립이 아니잖아요.

뭔가 서로 관련이 있는 거잖아요.

그러니까 두 개가 뭔가 확률이 곱한 거랑 똑같이 나와야 되는데 이 숫자가 커진다는 거는 뭔가 똑같이 안 나온다는 거니까 뭔가 둘이 연관이 돼있다는 거.

그래서 그거를 알아내는 방법이에요.

이거는 밑에가 이론이면 이론이면 위엔나 실제죠.

이론에 대비해서 실제로 얼마나 높게 나오는지 이걸 보네요.

만약에 이론하고 실제하고 다르다.

그럼 이론이 틀린 거죠.

그죠?

이론하고 실제하고 비교했는데 이론적으로는 25만큼 나와야 되는데 실제로는 30, 40, 50 이렇게 나온다.

그러면 그 이론이 틀린 거잖아요.

그 이론이 뭡니까?

독립이라는 거죠.

독립이 아니라는 증거가 생기죠.

그래서 여기에서 로그가 있는데 이거를 싹 지우면 연관분석이라는 데이터 분석 방법이 있어요.

거기에서 말하는 향상도랑 똑같게 됩니다.

연관분석은 어디에다 쓰냐면 여러분 그 얘기 아세요?

맥주랑 기저귀 얘기.

맥주랑 기저귀 얘기가 굉장히 사실은 오래된 일인데 이게 80 몇 년에 나온 거니까.

모르시는 분이 얘기 들으면 어느 슈퍼마켓 체인에서 사람들이 한 장바구니에 같이 결제한 데이터를 분석을 하는데 우리가 마트 가서 결제하면 풍로기 띠디디디디 하고 올라온 다음에 결제 누르면 그게 하나로 데이터 베이스에 들어가겠죠.

그거를 분석을 해보니까 같은 장바구니 안에 이상하게 맥주랑 기저귀가 같이 있는 경우가 많다.

맥주랑 기저귀가 예를 들면 맥주를 살 확률이 10%예요.

기저귀를 살 확률이 5%예요.

그러면 얘네 둘이 별로 관련이 없잖아요.

맥주랑 기저귀가 무슨 관련이 있지?

그러면 얘네 둘이 동시에 장바구니에 들어가 있을 확률이 10%랑 5%랑 0.5%가 되어야 되는데 그것보다 훨씬 높게 나오는 거예요.

그러면 두 개가 독립이 아니라는 거잖아요.

서로 뭔가 관련이 있다는 거죠.

독립이라는 거는 관련이 없다는 얘기죠.

근데 독립이 아니라는 거는 관련이 있다는 얘기죠.

조사를 해보니까 미국에서 나온 건데 미국은 마트랑 집이랑 멀단 말이에요.

마트 한 번 오려면 차 타고 30분씩 가야 되는데 그렇게 말하면 우리나라도 차 타고 30분 가야 되는 거 똑같이 차 타고 가야 되는데 집에서 애 키우는데 엄마가 애 보는데 기저귀가 안 떨어졌어요.

그러면 보통 남편을 발로 차서 마트 가서 기저귀 사와

이러면 남편이 투덜투덜 대면서 차를 끌고 30분을 가서 마트까지 갔는데 기저귀를 사다가 마트까지 모으니까 온 김에 자기 먹을 맥주도 사가는 거죠.

그래서 맥주랑 기저귀랑 값이 팔리더라.

이거를 차단했다.

맥주랑 기저귀를 패키지로 파니까 더 잘 팔렸다.

이런 게 있거든요.

근데 이 얘기가 재밌는 게 뭐냐면 80년대에 나온 얘기예요.

사실 빅데이터 이런 말도 없던 시절인데 80년대에는 주로 무슨 그때는 주로 무슨 얘기를 했냐면 데이터 마이닝이라는 얘기를 많이 했거든요.

80년대까지는 90년대...

아, 제가 헷갈렸어요.

아마 90년대일 겁니다.

90년대에 데이터 마이닝이라는 얘기를 많이 했어요.

이때 처음 나온 얘기거든요.

근데 이제 세월이 흐르고 흐르고 흐르고 흘러서 지금 빅데이터를 얘기하는 빅데이터도 요즘에 약간 얘기를 잘 안 하기 시작하는데 빅데이터 시대에도 여전히 이 얘기를 하거든요.

맥주랑 기저귀.

여기서 우리가 알 수 있는 게 뭐냐.

첫 번째는 진짜 소재가 없구나.

지금 한 30년을 우려먹고 우리가 빅데이터도 하고 사실 다 같은 얘기인데 빅데이터를 분석을 해가지고 마이닝이 돼 광산을 파고드는 거잖아요.

파고들어서 캐오는 건데 이제 신문 이런 데 보면 빅데이터 소개할 때 되게 많이 듣는 얘기가 이 얘기란 말이에요.

근데 이게 사실은 빅데이터보다 이전 시대에 하던 얘기거든요.

사실 참 뭔가

빅데이터를 하면 우리가 뭐가 좋으냐 라고 했을 때 사람들 귀에 솔깃할만한 참신한 소재가 되게 없다.

생각보다.

이런 거를 보여주는 그런 얘기라고 할 수 있습니다.

그래서 연관분석이라고 해서 장바구니 분석 이런 거 할 때 실제로 기업에서 많이 합니다.

예를 들어 우리 너랑 같이 많이 팔리나

이러면서 근데 변수증이 원래 많이 팔린 게 있어요.

예를 들면 카레랑 당근이랑 카레랑 감자랑 많이 팔린다.

이거는 그냥 상식적인 거죠.

이거는 원래 걔네는 연관이 돼있습니다.

이런 거는 뻔한 건데 의외의 연관성이 있을 수 있거든요.

그런 거를 찾을 때 이 방법을 씁니다.

그래서 이렇게 하고 그 다음에 단어에 대해서도 마찬가지로 할 수 있겠죠.

만약에 이 단어랑 이 단어가 있는데 걔네가 만약에 어떤 단어가 나올 확률이 10%고 다른 단어가 나올 확률이 5%면 둘이 동시에 나올 확률은 0.5%니까 근데 실제로는 얘네가 같이 나오는 경우가 4% 이렇게 된다.

원래 0.5%여야 되는데 4%가 됐으니까 몇 배가 되겠다.

8배가 많이 나온 거죠.

그러면 이론치에 비해서 8배가 많이 나왔으니까 뭔가 관련이 굉장히 강하다.

이렇게 볼 수 있습니다.

그래서 PMI가 높다는 거는 두 단어가 서로 관련이 강하다.

이런 의미로 볼 수 있습니다.

PMI 자체도 일종의 유사노어하고 비슷한 개념으로 볼 수 있습니다.

그 다음에 PPMI는 뭐냐면 PMI가 이 앞에 로그가 붙어있는데 이 로그를 붙이면 어떻게 되냐면 로그함수가 어떻게 생겼냐면 1을 기준으로 해서 이렇게 생겼거든요.

그래서 1보다 크면 플러스고 1보다 작으면 마이너스로 들어갑니다.

그래서 이 앞에 로그를 붙여놓으면 만약에 이 두 개가 똑같다.

그러면 분모랑 분제랑 똑같으면 1이 되겠죠.

그럼 앞에 로그를 붙이면 0이 됩니다.

PMI가 0이라는 건 뭐냐면 이론치랑 실제치랑 똑같다.

PMI가 플러스가 되면 이론치보다 실제치가 높다.

이론치보다 실제치가 높다.

마이너스가 되면 이론치보다 실제치가 더 작다.

그러면 그러면 무슨 얘기냐면 원래는 독립이면 똑같이 나와야 되는데 독립인 것 보다도 더 적으면 서로 배제하는 관계일 수도 있고 아니면 데이터가 부족해서 숫치가 마이너스로 0이어야 되는데 마이너스 0.1 이렇게 숫자가 위아래로 튈 수 있거든요.

약간 아래로 튄 걸 수 있어요.

데이터가 작다 보면 왜냐하면 예를 들어서 이론치가 10% 곱하기 5% 하면 아까 0.5%라고 했잖아요.

그럼 뭐가 0.5% 일이 벌어지려면 0.5%라는 건 뭡니까?

200번에 한 번 정도 나온다는 얘기잖아요.

그럼 최소한 200번을 해봐야 한 번 나올까 말까 하는 얘기 아니겠어요?

200번 해도 안 나올 수도 있겠죠.

그러니까 문서가 150개밖에 없어요.

그러면 이론적으로는 0.5% 나와야 되는데 전혀 안 나올 수도 있는 거죠.

이론치는 0.5인데 실제로는 0이 나올 수도 있습니다.

이론치보다 실제치가 낮으니까 PMI가 마이너스가 됩니다.

그래서 PMI가 마이너스인 경우는 이쪽인지 이쪽인지

모르겠으니까 그냥 0으로 퉁치자.

이게 PPMI. 그래서 이 앞에 P는 POSTIVE. 플러스인 거랑 여인 거만 남기고 마이너스인 거는 전부 0으로 올려주자.

서로 배제하는 관계일 수도 있고 데이터가 부족해서 그런 걸 수도 있는데 그냥 얘네는 관리 없는 걸로 치는 게 PPMI. 그래서 우리가 텍스트 분석에서는 PMI보다 주로 PPMI를 더 있어요.

그래서 PPMI는 계산을 하는 방식이 약간 좀 복잡한데 일단 우리가 DTM에서 숫자가 이제 이 단어가 나올 확률을 그려야 하기 때문에 데이터의 실제 개수보다 얘가 나왔냐

안 나왔냐만 봅니다.

그래서 나왔으면 0보다 크면 초

안 나오면 0이니까 크지 않죠.

그래서 거짓이 됩니다.

그래서 이제 이 두 개를 비교를 하고요.

그래서 DTM이 0보다 크다

이렇게 하면은 이게 또 압축이 돼 있는 상태죠.

압축을 좀 풀어줍시다.

점 A 이렇게 하면은 이제 false false 이렇게 나오는데 false라는 거는 아까도 얘기 드렸지만 0하고 똑같은 경우에요.

true는 1, 2, 1이든 3이든 다 상관없죠.

그래서 이거를 지금 true false로 나오는데 edge type 한 다음에 int 이렇게 바꿔주면 이제 숫자로 바뀌는데 여기가 1로 나오죠.

이거는 원래 true란 얘기입니다.

그래서 파이선 언어 특성상 거짓은 0으로 취급하고 참은 1로 취급해요.

이거는 대부분 다른 언어도 마찬가지입니다.

그래서 대부분 언어에서 거짓은 0으로 취급하고 숫자로 바꿀 때 참은 1로 바꿀 때 그래서 이거를 DTM bin이라고 해줍니다.

여기서 이제 bin은 binary, binary는 0 아니면 1 이런 뜻이에요.

그 다음에 이 DTM bin을 꼭 해 줍니다.

여기 앞에 보시면 수확이 들어가서 복잡해지는데 전치를 시켜주고 그 다음에 이렇게 되죠.

그러면은 어떻게 되냐면 이게 이제 코어크 이렇게 되는데 DTM bin.t 골뱅이 해보면은 362, 362 이렇게 나오죠.

그러면 362가 뭐냐면 단어 개수입니다.

단어와 단어에 ppmr을 구한 거에요.

그래서 여기 요식은 뭐냐면 여기 골뱅이는 행렬에 곱셈을 하라는 뜻입니다.

학교 다닐 때 배운 게 기억이 나실지 모르지만 행렬도 곱셈이 되거든요.

구체적으로 어떻게 곱하는지는 일단 논외로 하고 어쨌든 곱하기가 됩니다.

그래서 이걸 뒤집어 가지고 곱하기를 시키면 코어워커 매트릭스가 되는데 코어워커는 공기 행렬이라고 해가지고 그래서 이제 공기는 우리 숨 쉬는 공기를 말하는 게 아니고 이 공은 공동할 때 공기 짭니다.

함께 이런 뜻이에요.

공은 함께 이런 뜻이고 이 기는 어떤 무슨 기짜냐면 아침에 기상할 때 그 기짜입니다.

기상 이게 뭡니까?

일어나

이거죠.

그래서 공기 행렬이 한 게 뭐냐면 함께 일어난다.

두 단어가 같이 일어나는 거예요.

여기서 일어난다는 건 사건이 일어난다 할 때 일어난다.

두 단어가 함께 일어난다는 건 두 단어가 함께 발생한다.

함께 나타난다.

공기 행렬이 보여주는 건 뭐냐면 두 단어가 함께 나타나는 경우가 얼마나 되는지를 보여줘요.

그래서 코어워커를 보면 여기 이렇게 보면 이렇게 보면 지금 보시면 72 이렇게 나오는데 이건 뭐냐면 0번 단어가 총 72번 나왔다는 거예요.

왜냐하면 자기 자신과 함께 일어난 경우가 72회라고 하니까 자기 자신과 자기 자신은 항상 같이 일어나죠.

일어날 땐 같이 일어날까?

그래서 이 대각선 원소는 그 단어가 총 일어난 개수가 되는 거고 여기 이제 2, 6, 15, 3, 4 이렇게 나오는데 여기 2는 뭐냐면 여기 0번 행의 1번 열이죠.

0번 단어랑 1번 단어가 함께 일어난 경우가 두 번 있다.

두 단어가 함께 나온 경우는 두 번 있다.

6은 뭐냐면 두 단어가 함께 나온 경우가 여섯 번 있었다.

이해 되시죠?

그러면 6번 행의가 함께 일어난 경우가 두 번 있다.

그러면 예를 들면 여기 15가 있는데 이거는 뭐냐면 이 단어랑, 그리고 세로축으로 이 단어랑 함께 나온 경우가 열다섯 번 있다.

그럼 문제가 뭐냐면 예를 들면 여기는 두 번 같이 나왔고 여기는 열다섯 번 같이 나왔는데 열다섯 번 같이 나온 게 더 많이 나온다.

하면 꼭 그렇지는 않을 수도 있어요.

왜냐하면 두 번 같이 나온 단어는 애초에 열다섯 번 밖에 안 나온 단어.

얘는 열다섯 번 나오는 중에 두 번 같이 나온 거고 예를 들면 어떤 거 해야 할까?

예를 들면 어떤 연예인이 있는데 유재석 씨랑 같이 예능을 찍은 적이 있어요.

그럼 이게 이 연예인하고 유재석하고 관련이 있을까요?

없을까요?

별로 없겠죠.

그냥 유퀴즈 한번을 한번 더 바꿨죠.

유재석 씨는 워낙 예능에 많이 나오니까 어떤 사람이 예능에 나왔어요.

근데 유재석이랑 같은 예능에 나왔으면 신기한 일이 아니죠.

이해되시나요?

유재석은 워낙 예능에 많이 나오니까 유재석이랑 같은 예능에 나온 거는 그냥 당연한 거잖아요.

그런데 예능에 잘 안 나온 사람 누구 있을까요?

요즘에 잘 안 나온 정준화 씨.

어떤 사람이 올해 예능에 나왔는데 정준화 씨랑 같은 예능에 나왔어요.

그중에서 제일 중요한 거잖아요.

정준화 씨 요즘 텔레비전에 잘 안 나오니까.

이해되시나요?

예를 들면 여기 보면 여기 3인데 이 단어는 전체가 10번 밖에 안 된단 말이에요.

10번 밖에 안 되는 중에 3번 같이 나온 거하고 어떤 사람이 평생 텔레비전에 10번 밖에 안 나왔는데 그중에 3번을 유재석이랑 같이 나왔어요.

아니면 어떤 사람은 텔레비전에 되게 많이 나왔는데 그중에 100번을 유재석이랑 같이 나왔어요.

그러면 절대적인 숫자가 중요한 게 아니라 상대적으로 얼마나 많이 나왔냐

이게 중요하단 말이에요.

이해되시나요?

그래서 그거를 구하는 게 PMI입니다.

그래서 이제 이거는 계산이 되게 복잡해지는데 그거를 구하는 거예요.

PMI를 구한 거예요.

어떻게 하냐면 그 단어가 실제로 나올 확률을 구하고 그 다음에 두 개가 같이 나올 때 확률을 구하고 그 다음에 실제로 함께 나타나는 확률을 구해서 그거를 비율을 구한 겁니다.

이 스펙티드가 여기가 이론적 이게 이론치가 되고 그게 실제치의가 됩니다.

그래서 마지막으로 0보다 작은 거를 다 0으로 바꿔주면 PPMI가 됩니다.

얘기가 좀 복잡하죠?

여기 중간 계산 과정은 조금 복잡하니까 자세하게 설명 안 하고 그냥 넘어가겠습니다.

이거는 그냥 제가 드린 공식대로 하시면 돼요.

그래서 PPMI도 이상하게 파워포인트 콜레버로 붙여넣기가 안 되더라고요.

어쨌든 그러면 우리 제 2번째 단어가 뭡니까?

아까 그 추출이었죠?

추출하고 PPMI 값에 그 다음에 그리고 PPMI 값에 유사도가 높은 그 단어들을 뽑아보면은 교반, 용액, 나무, 가열, 첨가, 한약제, 추출 이렇게 나오게 됩니다.

그래서 아까 보시면은 이거는 탈모, 첨가 함유는 뭐냐면 두 단어가 있을 때 두 단어가 같은 문서에 나오느냐

안 나오느냐를 가지고 유사도를 계산을 한 거예요.

그러면 추출이랑 유사도 높은 단어를 찾으면 추출하고 의미상 비슷하다기보다는 추출하면 그에서 뭐 할 건데?

탈모에 도움이 되고 추출한 거를 첨가를 하겠죠.

이런 맥락에서 유사도가 계산이 되는데 지금 좀 복잡하지만 PPMI를 계산을 해서 그걸로 유사도가 높은 단어를 뽑으면 추출액이라던가 한약제라던가 교반이 섞는다는 얘기죠.

그러면은 용액에다가 추출하겠죠.

그러니까 좀 더 의미상 추출하고 좀 더 직접적으로 비슷한 단어들이 뽑혀 나오게 됩니다.

그래서 이런 식으로 볼 수도 있다.

## 임베딩

그런데 PPMI는 사실 이렇게 할 수도 있는데 단어 유사도를 계산할 때 이렇게는 실제로는 많이 하진 않아요.

그래서 왜 안 하냐면 여러 가지 이유가 있는데 더 좋은 방법이 있기 때문에 그렇습니다.

그럼 더 좋은 방법은 뭐냐?

다음으로 넘어가죠.

그러면은 오늘 강의자료를 넘어오는데 인베딩이라는 방법을 사용을 해요.

PPMI는 계산할 때 어떻게 합니까?

결국에는 PPMI도 제가 아까 유사도나 이런 관련도인데 PPMI를 계산하면 결국에는 단어 개수만큼 숫자가 나오거든요.

그래서 PPMI를 계산하면 단어 개수만큼 숫자가 나오거든요.

아까 보시면 PPMI가 결국에는 362개의 단어가 있으면 각각의 단어를 다른 362개의 단어와의 PPMI로 나타내니까 단어 하나마다 숫자가 362개씩 있어요.

숫자가 너무 많단 말이에요.

그래서 그럼 이제 문제가 뭐냐면 단어가 예를 들어서 우리가 보통 성인이 사용하는 단어가 10만개쯤 되거든요.

일반적으로.

고유 명사 제외하고도 10만개쯤 됩니다.

우리가 보통 아는 단어.

종이에다 한번 써보시면 10만개 쓸 수 있어요.

그러면 PPMI식 논리로 하면 단어 하나마다 나머지 10만개하고 어떤 PPMI가 있는지 관련도가 있는지를 봐야 되니까 10만 곱하기 10만짜리 행렬을 만들어야 된단 말이에요.

그럼 일단 행렬이 엄청나게 크겠죠.

그렇게 큰 행렬을 만들려면 텍스트도 엄청나게 많아야 됩니다.

그러니까 왜냐하면 10만 곱하기 10만이면 그 행렬에 들어가는 그 칸의 개수가 10만 곱하기 10만이니까 100억개의 칸이 생기거든요.

그럼 100억 칸을 숫자를 채우려면 데이터가 얼마나 많아야겠어요.

어마어마하게 많은 데이터가 필요하겠죠.

그래서 이제 뭐 별로 좋은 방법이라고는 할 수가 없고 요즘에 더 많이 사용하는 방법은 오늘 강의에서 다룰 인베딩.

그래서 인베딩이 뭐냐 인베딩은 여러분들이 제일 쉽게 찾아볼 수 있는 게 핸드폰에 얼굴로 잠금해제가 되잖아요.

얼굴을 비추면 잠금해제가 되잖아요.

그거 할 때 인베딩을 씁니다.

근데 이게 되게 신기한 기술인데 왜냐하면 내가 날마다 얼굴이 다르잖아요.

그죠?

어제 얼굴 다르고 어제 술 많이 먹고 일어나면 오늘 얼굴 붓고 초췌하고 그러다가 밥 잘 먹으면 다시 뽀송뽀송해지고 얼굴이 계속 달라진단 말이에요.

조명도 다르고 날씨도 다르고 옷도 다르고 근데 어떻게 내 얼굴인지 알아보냐

이게 되게 신기하죠.

이때 인베딩이라는 걸 씁니다.

그래서 이 핸드폰에 얼굴을 저장을 할 때 얼굴 자체를 저장하는 게 아니라 어떤 얼굴의 특징값을 몇 개만 뽑아요.

200개 이런 식으로 뽑습니다.

그래서 다음에 내가 잠금해제를 하고 얼굴을 딱 비치면 다시 특징값을 200개 뽑아요.

그럼 원래 저장된 200개의 특징값하고 지금 들어온 200개의 특징값하고 이걸 비교해서 얘네가 비슷하면 같은 사람이구나

이렇게 해서 잠금을 풀어주고 만약에 특징값이 다르면 잠금을 안 풀어줍니다.

그 특징값만 비교하면 되니까 굉장히 빨리 비교를 할 수 있겠죠.

저장을 할 때 용량도 적게 찾아요.

그래서 일종의 디지털 지문이다

이런 식으로 얘기를 하는데 지문이랑은 다르고 특징을 나타내는 숫자 몇 개를 뽑아낸 거를 인베딩이라고 합니다.

그래서 보통은 인베딩은 숫자의 개수가 작아요.

절대적으로 몇 개부터 작다

이런 건 아닌데 상대적으로 작습니다.

그래서 우리가 어떤 단어를 표현할 때 이 단어를 굉장히 적은 개수의 숫자에 표현한 그런 걸 봐요.

인베딩이라는 영어 말 자체가 뭔가

되게 좁은 데다 끼어넣는 거를 우리 뭐 쓰레기 같은 거 있는데 어디 바닥에 버리기도 뭐하고 그렇다고 들고 다니기도 뭐하고 틈새 같은 데 끼어넣는 사람들 있잖아요.

그게 인베딩입니다.

내가 좁은 데다 이렇게 막 쑤셔넣는 거예요.

이런 틈새나 도의 같은 거 끼어넣는 사람들 있잖아요.

꼭 보면 하지 말라고 해도 그게 인베딩이거든요.

뭔가 좁은 데다 끼어넣는 건데

그러니까 원래 단어라는 건 굉장히 풍부하고 다양한 의미를 가지고 있는데 아니면 사람 얼굴이라는 건 굉장히 복잡한 형상을 가지고 있는데 이걸 작은 개수의 어떤 숫자 안에다가 끼어넣은 거예요.

그 숫자만 비교하자 이거죠.

그래서 우리가 인베딩을 잘 만들면 굉장히 그 적은 개수의 숫자만 비교해도 우리 얼굴로 잠금해서 해보면 나랑 좀 비슷하게 생겼어도 이게 안 풀리거든요.

물론 쌍둥이다

이런 거 풀릴 수도 있는데 그래서 이제 어떤 비교하기 쉽게 어떤 숫자를 만들어 놓은 그런 거를 이제 적은 개수로 만들어 놓은 걸 인베딩이라고 합니다.

그래서 인베딩하고 좀 비슷한 게 예를 들면 여러분 도서관에 가보시면은 도서관에 책마다 번호가 붙어있어요.

이걸 10진 분류 코드라고 하는데 이런 것도 어떻게 생각해보면 일종의 인베딩이라고 볼 수도 있습니다.

왜냐하면 책의 내용을 몇 개의 숫자를 표현한 거잖아요.

그래서 보시면은 엥?

크롬이 죽어버렸다.

도서관에서 이렇게 텍스트 분석을 검색을 해보면 이렇게 이렇게 이렇게 이렇게 이렇게 이렇게 이렇게 이렇게 이렇게 이렇게 이렇게 응?

여기 보시면 이제 410번 이런 식으로 이렇게 번호가 붙어있어요.

다른 책을 보면은 여기는 492 string 그리고 495번 이렇게 번호가 붙어있거든요.

그러니까 이제 410번은 410번 끼리 그 495번은 495번 끼리 같은 서가 있는데 그러면은 이제 보시면은 이제 보면은 또 410번 이렇게 되있죠.

텍스트 분석 관련된 책들은 대충 400 몇번 떄 서가에 이렇게 몰려있습니다.

그럼 만약에 여러분이 국립중앙도서관에 가면은 막 이쪽 뭐 막 3층 이쪽 끝에 갔다가 5층 저쪽 끝에 갔다가 이런 식으로 가는 게 아니라 410번이 몰려있는 그 쪽 커너에 가면 다 텍스트 분석 관련된 책들이 이렇게 몰려가 있습니다.

서점도 그렇죠

보통.

그래서 이제 이런 것도 일종의 인베딩이라고 할 수도 있는데 이거는 그냥 사람이 손으로 분류를 해놓은 거예요.

인베딩하고는 좀 다른 거죠.

그래서 이제 우리가 문서랑 단어도 비슷하게 인베딩을 만들 수가 있습니다.

어떤 머신 러닝 기법을 이용해서 그래서 이거를 이제 중요한 거는 어떤 적은 개수의 수로 표현을 하는 게 우리의 중요한 문제가 됩니다.

그러면서 동시에 의미를 보존해야 되요.

인베딩이 가져야 되는 두 가지 특성이 첫 번째가 의미를 보존을 해야 되고 두 번째가 숫자가 적어야 됩니다.

그냥 숫자만 적으면 의미가 보존이 안되겠죠.

그래서 잠금 해제 하는데 막 아무 얼굴이나 비춰도 잠금이 풀면 안되잖아요.

의미를 보존한다는 게 뭡니까?

이미지 같은 경우에는 두 사람이 같은 사람이라는 거는 보존을 하면서도 동시에 숫자 개수는 줄여야 돼요.

희한하죠.

어떻게 그게 되나.

또 그게 되는 게 신기하다.

그래서 문서를 우리가 비교할 때도 문서 인베딩을 만든다는 거는 문서의 의미를 보존하는데 그 문서를 구성하는 단어를 하나하나 다 얘기하는 게 아니고 문서의 어떤 핵심 정보만 추려서 적은 개수의 숫자로 표현을 하면 문서 인베딩이 되는 거고 단어도 마찬가지로 그 단어가 어느 문서에 나왔는지

일일이 그 모든 정보를 다 하는 게 아니고 예를 들면 우리가 술 먹으면 얼굴이 붓고 멀쩡하면 얼굴이 뽀송뽀송하고 밥을 못 먹으면 얼굴이 헬스케지는데 그런 거는 그 사람이냐 아니냐를 구별하는 게 중요한 문제가 아니라는 거 이해되시죠.

그런 세부사항은 다 날려버리고 어떤 핵심 정보만 남겨서 적은 개수의 숫자로 표현하는 게 인베딩이 됩니다.

그러면 인베딩을 어떻게 만드느냐 하면 인베딩을 만드는 방법이 굉장히 여러 가지가 있는데 잠재의미분석이라는 기법부터 알아보도록 하죠.

그래서 시간이 11시라서 잠깐 쉬었다가 하도록 하겠습니다.

혹시 질문 있으신가요?

질문은 쉬는 시간 끝나고 해주시고요.

그러면 11시 10분에 다시 시작하도록 하겠습니다.

## 잠재 의미 분석

그러면 잠재의미분석이 뭐냐 말만 들으면 잠재의식에 있는 의미를 하여튼 그런 거 같지만 그런 의미는 아니고 우리가 이제 어떤 텍스트가 있을 때 우리가 관찰한 건 뭐냐면 문서나 행렬을 관찰했죠.

그러면은 우리가 직접적으로 관찰한 건 아니지만 뭔가 숨겨진 젠체가 있어가지고 우리가 실제로 보지는 못했어요.

하지만 뭔가

있어가지고 그게 이 X를 뭔가 결정을 할 거다.

이런 가정 하에 하는 분석을 비지도 않습니다.

그래서 우리가 지금 이 X를 우리가 알고 있는 Y를 예측하면 이걸 지도학습이라고 합니다.

그래서 이제 우리가 머신러닝에서 지도학습이 있고 지도학습은 우리가 관찰한 걸 가지고 분석을 하는 겁니다.

X랑 Y가 있으면 이게 우리가 둘 다 관찰을 한 겁니다.

그러면 우리가 알고 있는 X로 우리가 알고 있는 Y를 예측을 하면 지도학습이라고 하고요.

X는 우리가 관찰을 했는데 우리가 관찰을 하지 못한 아니면 관찰할 수 없는 Z가 있어요.

그 Z가 있다 치고 그걸로 X를 설명을 해보는 겁니다.

그래서 이제 여기서는 인베딩이나 또는 이걸 특별히 톱픽이라고 하는데 톱픽이라는 게 특별한 의미는 없고 그냥 어떤 잠재면수를 텍스트 분석에서 잠재면수를 톱픽이라고 해요.

그래서 머가 있어가지고 여기다가 영향을 미칠 거다.

이런 식으로 생각을 하는 거죠.

사실 우리가 잠재면수가 여러 가지가 있는데 원래 잠재면수의 이 아이디어가 어디서 나왔냐면 심리학에서 나왔거든요.

왜냐하면 심리학자들은 마음 자체가 관찰이 안 되잖아요.

여러분 마음을 볼 수 있습니까?

마음 보이지 않죠?

그래서 마음이라는 거는 이렇게 이 딱 치고 분석을 하는 겁니다.

이해 되시죠?

그래서 원래 심리학에서 데이터 분석을 하는 과정에서 이게 나와요.

여러분이 머가 잠재면수인지 아닌지 약간 헷갈리면 그걸 심리학에서 연구를 할까

안 할까 생각해보시면 조금 쉽습니다.

예를 들면 어떤 사람이 똑똑하다

이거 심리학에서 연구할 거야

안 할까?

그겠죠?

흔히 말하는 지능 지능 검사하잖아요.

그러면 여러분 그걸 구별하셔야 되는데 지능 검사 점수는 우리가 관찰이 된 거여야 안 되고 그건 관찰된 거죠.

보면 알잖아요.

너 지능 검사 점수 이거 이번에 시험 받는데 100점 만점에 80점 받았어?

그건 우리가 관찰한 거죠.

근데 우리가 관찰한 거는 뭐냐면 시험 검사 점수예요.

근데 심리학에서는 어떻게 가정을 하냐면 이 뒤에 머가 있어가지고 검사 점수를 결정했을 거야.

그거를 지능이라고 부르자.

여러분들 그걸 구별하셔야 되는데 검사 점수하고 검사 점수의 배경에 있는 잠재면수는 다른 변수입니다.

이게 사람들이 이걸 구별 못하거든요.

IQ 테스트 결과하고 그 사람의 지능은 별개예요.

IQ 테스트 점수는 우리가 관찰한 거고 지능은 우리가 관찰할 수가 없습니다.

아니면 성격 그러면 성격 검사 결과하고 이 사람의 성격은 같은 겁니까

다른 겁니까?

다른 겁니다.

별개예요.

이 사람의 뭔가 성격이 있을 건데 그 성격이 어떤 식으로든 검사에 영향을 미칠 거다.

이런 식으로 가정을 하듯이.

다른 것도 마찬가지입니다.

예를 들면 우리가 고객의 의도 아니면 고객의 만족도 우리 고객 만족도 조사하는 거 우리 고객 만족도 조사를 하면 뭔가 설문을 하잖아요.

그럼 그 설문 결과가 고객의 만족입니까?

아니죠.

고객의 만족은 우리가 관찰을 못 합니다.

관찰을 못 하는데 그게 어떤 식으로는 그 검사 결과에 영향을 주겠죠.

이해 되세요?

그래서 그게 우리가 이제 BG도 학습의 중요한 과정이 됩니다.

검사 결과하고 우리가 관찰할 거하고 배경에 있는 거하고 구별을 하는 거.

물론 이제 이게 영향을 주는데 여기에 그냥 약간의 오차만 들어갈 거다.

이렇게 생각을 하면 이 오차라는 거는 플러스로도 붙고 마이너스로도 붙으니까 일단 뭐 사실 그렇게 큰 문제가 아닐 수 있는 거죠.

그죠?

예를 들면 어떤 이 사람은 진짜 실력이 있고 시험점수가 있는데 진짜 실력하고 시험점수는 다르지만 어차피 플러스 마이너스 얼마겠지

이렇게 생각하면 사실 큰 문제는 아니거든요.

왜냐하면 약간 플러스 아니면 약간 마이너스니까 그렇게 똑같은 게 아니라고 해도 본질적인 문제는 아닐 수 있는데 어떤 경우에는 되게 문제가 될 수도 있습니다.

왜냐하면 그냥 플러스 마이너스만 다른 게 아니라 아예 뭐 상관이 없다든가 아니면 굉장히 다른 변수에 영향을 많이 받다든가.

이런 게 복잡하게 됩니다.

그래서 우리가 이런 식으로 접근하는 걸 텍스트를 비지도학습을 하는 걸 톱핑 모델링이라고 하고요.

그래서 이 지이가 인베닉 또는 톱핑이 되는데 그러면은 이렇게 비지도학습을 하는 방법이 여러 가지가 있습니다.

우리가 오늘 알아볼 거는 그 중에 LSA라는 방법입니다.

그래서 잠재, 의미, 분석입니다.

그래서 이 방법을 제가 지금 제가 알게 된 방법은 의미, 분석입니다.

그래서 여기 앞에 잠재라는 말이 붙는 이유는 우리가 이거를 관찰을 안 하기 때문에 관찰을 못 하기 때문에 앞에 잠재라고 있습니다.

우리가 관찰하지 못한 하지만 있다고 뭔가 가정된 그런 거를 분석합니다.

그래서 심리학 같은 데서 그런 얘기 하거든요.

예를 들면은 우리가 다른 사람에 대해서 되게 기분이 나쁠 때가 있어요.

보통 기분이 나쁜 게 다른 사람의 행동 때문에 기분이 나쁜 경우는 살았거든요.

보통은 뭐 때문에 기분이 나쁘냐고 어떤 행동을 보고 그 의도를 우리가 추정을 해가지고 그 의도 때문에 기분이 나쁘죠.

예를 들면 여러분 오늘 아침에 학교에 왔는데 누구랑 복도에서 마주쳤어요.

근데 인사도 없이 지나가네.

그러면 기분이 나쁘잖아요.

그러니까 인사를 안 해서예요?

아니면 인사를 안 할 이유가 저 사람이 나를 무시해서라고 생각을 하기 때문이에요.

무시하기 때문이라고 생각하니까 기분이 나쁜 거죠.

아니면 못 봤다든가 내심 나를 좋아하는데 창피해서 도망간 거라면서 일생하면 인사를 안 해놓으면 자식 이렇게 생각이 되잖아요.

결국에는 사실은 내가 기분이 좋고 나쁘고가 사실 그 자체에 있는 게 아니라 뭐라고 추정했느냐에 따라서 기분이 좋고 나쁜 건데 항상 부정적으로 추정을 하는 사람들이 있어요.

예를 들면 우울증이 있는 사람들은 똑같은 것도 항상 부정적으로 해석이 되거든요.

뇌에 약간 호르몬이나 이런 거에 문제가 생겨가지고 보통 사람들이면 별 의미 없는 것도 항상 뭔가

다 부정적으로 해석이 됩니다.

사실은 심리 상담 이런 데서 하는 게 이렇게 구별하는 거 이런 거 얘기를 많이 하거든요.

우리가 이제 데이터 분석에서도 똑같이 할 수 있는 거죠.

뭐가 있다라고 가정을 하고 하는데 이거를 뭐 긍정적으로 해석할 수도 있고 부정적으로 해석할 수도 있고 다양한 해석 방법이 있을 수가 있습니다.

그래서 기법도 여러 가지가 되는 거죠.

어떤 식으로 물론 이게 기법들이 긍정적 부정적으로 해석한다는 의미는 아니고 이렇게 해석할 수도 있고 저렇게 해석할 수도 있는데 그런 이렇게 저렇게 방법이 여러 가지가 있게 됩니다.

그래서 이제 우리가 첫 번째로 알아볼 LSA 라는 방법은 가장 오래된 방법이에요.

오래된다고 해봐야 기쁘고 오래된 건 아니지만 여튼 여기에 있는 방법들 중에는 상대적으로 좀 오래된 방법입니다.

오래된 방법이 나쁘냐 하면은 그렇진 않아요.

통계에서는 오래된 방법들이 좋은 경우가 꽤 있습니다.

왜 좋냐 하면 보통 오래된 방법들은 옛날에 만들어졌는데 옛날에는 데이터도 별로 없고 컴퓨터도 별로 좋지 않던 시절이에요.

그래서 그때 만들어진 기법은 적은 데이터 그 다음에 낮은 컴퓨터 성능에도 잘 돌아가게 만들어졌어요.

단점은 뭐냐면 빅데이터 아이시드잖아요.

데이터도 많고 요즘에 컴퓨터도 좋단 말이에요.

그러면은 데이터도 많고 컴퓨터도 좋은 데서는 굳이 쓸 필요가 없는 방법들일 수는 있죠.

근데 문제는 뭐냐면 우리가 실제로 가진 데이터가 빅데이터가 아닌 경우가 여전히 굉장히 많습니다.

그러니까 빅데이터라는 게 엄격한 기준이 있는 건 아니지만 예를 들면 텍스트를 분석한다 그럼 텍스트가 천만 개 일억 개 있으면 최신 방법들이 좋아요.

근데 예를 들면 아까 우리 특허 이런 걸 보면 특허가 천만 개 일억 개 이렇게 있을 수는 없거든요.

특허 해봐야 찾아보면은 한 소재로 몇 백 개 뭐 이 정도밖에 없단 말이에요.

그러면은 현재 나오는 최신 방법을 가지고 하기에는 데이터가 너무 좋은 거죠.

이럴 때는 차라리 옛날 방법들이 더 좋을 수도 있다.

고전적인 방법들이 이런 적은 데이터에 맞춰져서 만들어졌기 때문에 좀 더 장점이 있을 수도 있습니다.

그래서 여전히도 많이 사용되는 방법들이에요.

그래서 이 잠재미 분석은 기본적으로 행렬의 곱셈을 이용합니다.

행렬의 곱셈을 이용을 하는데 우리가 이제 곱셈이라는 거를 제가 지난 시간에도 한 번 얘기 드렸지만 현대수학에서는 뭔가 이렇게 되게 구체적인 의미를 다 빼버려요.

예를 들면 우리가 1,2,3,4,5 이렇게 있으면 사과 한 개, 두 개 이런 식으로 생각하지 않습니다.

현대수학에서는 그냥 뭔가를 할 수 있으면 그게 다 자연수고 그러면은 더샘이라는 거는 특별한 의미가 없어요.

특정한 방식으로 정의된 계산이에요.

그 계산을 그냥 이름을 편의상 더샘이라고 부르는데 사실 그 더샘의 의미가 없습니다.

그러니까 수학과에 가면 1,1은 2라는 걸 증명을 하거든요.

증명하는 방식을 보면은 약간 우리가 원하는 그 증명이 아니에요.

뭐냐 하면 그냥 더샘이라는 걸 뭐를 정의를 하고 이 수 라는 걸 정의를 하면 1,1이 2가 될 수 밖에 없다.

이런 식으로 나오거든요.

뭔 소리야

약간 이런 느낌이 좀 듭니다.

그런 식으로 하면 1,1이 3도 되겠네.

수학자들은 아무 생각이 없어요.

그쵸?

3도 될 수 있죠.

3도 되고 4도 되고.

3도 되고 4도 되고.

그러니까는 우리가 왜 수 중에 허수라고 있잖아요.

루트 마이너스 1 하면은 i가 되는데 이걸 허수라고 부르거든요.

허수가 영어로 상상의 수 이런 뜻인데 수학자들은 이걸 상상의 수라고 생각하지 않아요.

이름이 그렇게 붙어 있으니까 그치?

우리가 상식적으로 야 마이너스 1에 루트 씌우는 게 말이 되냐?

이렇게 생각할 수 있는데 어차피 수학자들은 그런 상식하고 무관하게 생각을 합니다.

그냥 다 계산해요.

특정한 방식을 정의된 계산이 있는 거고.

그냥 그 계산을 하면 그게 그냥 그런 겁니다.

그걸 꼭 현실에 대응이 될 수도 있고 안 될 수도 있고 대응이 되면 좋고 안 되는 거 아니잖아요.

그러니까 완전히 어떤 뭐랄까 수학적 어떤 가상의 세계가 있어가지고 그 안에서 그냥 수학이라는 건 벌어지는 거예요.

여러분들도 어떤 수학적 개념을 보실 때 이게 현실에 꼭 뭐랑 대응이 되냐

이렇게 생각하면은 약간 굴치가 아프거든요.

그냥 그래서 곱셈 이런 것도 우리가 초등학교 때 배우듯이 사과가 3개 있고 3개씩 5무더기가 있으면 사과가 15개 있습니다.

이렇게 할 때 곱셈이 아니고 그냥 곱셈이라고 부르는 어떤 종류의 계산이 있다.

수학자들은 그걸 곱셈이라고 부른다.

이렇게 생각을 하시면 돼요.

행렬의 곱셈이라는 게 행렬이 결국 표잖아요.

표를 어떻게 곱하냐

이렇게 생각하시면 안 됩니다.

곱셈이라는 무언가가 있어서 그냥 그걸 하는 거예요.

그게 뭐냐

이런 거는 현대수학에서 더 이상 의미가 없습니다.

그게 뭔지가 중요한 게 아니고 그냥 곱셈이라는 무언가가 있는 거예요.

그걸 기계적으로 하는 겁니다.

그래서 우리가 행렬을 곱할 수가 있는데 어떤 행렬 A가 있고 어떤 행렬 B가 있으면 얘네를 곱할 수가 있어요.

왜 곱할 수 있냐를 생각하시면 안 됩니다.

곱셈이라는 걸 정해놓은 거예요.

이런 걸 하는데 그걸 곱셈이라고 하자.

이렇게 해서 정해놓은 거예요.

정해놓은 걸 하면은 다른 행렬이 이렇게 나옵니다.

근데 곱셈 행렬에 곱셈을 할 수 있는데 그러면 우리가 이런 거 생각해보세요.

예를 들면 숫자가 12가 있어요.

그럼 12를 두 수의 곱셈으로 나타낼 수 있겠죠.

2 곱하기 6이라고 할 수도 있을 거고 3 곱하기 4라고 할 수도 있고 어쨌든 우리가 곱셈이라는 게 정의가 돼 있으면 반대로도 할 수 있는 거죠.

그러니까 2하고 6을 주고 곱하면 얼마야?

라고 물어볼 수도 있고 아니면 12를 주고 얘를 뭐 곱하기 몇이야?

라고 물어볼 수도 있는 거죠.

곱셈을 정의를 했습니다.

그럼 행렬에 곱셈이 어떻게 정의되어 있는지에 관해 행렬에 곱셈이라는 걸 정의하면 반대로도 할 수 있습니다.

어떤 행렬을 X를 하나 뒀을 때 얘를 이렇게 두 개로 쪼개봐라 라고 하는 것도 할 수는 있는 거죠.

그게 뭐든지 가요.

그렇죠.

반대가 되겠죠.

두 개를 꼭 할 수 있으면 하나를 그 곱의 형태로 바꿀 수 있겠죠.

그게 뭐든지 가요.

그래서 LSA의 기본 아이디어는 뭐냐면 우리가 문서나너 행렬이 있어요.

문서나너 행렬이 있는데 문서나너 행렬도 행렬이란 말이에요.

그러면 얘를 무언가와 무언가의 곱셈 형태로 바꿔보자 라는 게 LSA의 아이디어입니다.

왜냐하면 우리가 행렬에 곱셈을 정의를 했으면 그 행렬을 곱셈 형태로 바꿀 수 있고 문서나너 행렬도 행렬이니까 곱셈 형태로 나타날 수 있겠네요.

그래서 다시 앞으로 가서 곱셈 행렬로 하는데 곱셈이니까 기본적으로 두 개가 나오겠죠.

그래서 이 두 개를 앞에 거를 문서 토픽 행렬이라고 생각을 하고 뒤에 거를 토픽 단어 행렬이라고 생각을 하는 겁니다.

왜 네 마음대로 그렇게 생각하세요?

라고 할 수 있는데

그냥 그렇게 생각을 하는 겁니다.

생각해서 나쁠 거 없잖아요.

우리 어렸을 때 보면 그냥 장난감 같은 거 놓고 얘네가 북한군이고 얘네가 남았냐

얘네가 왜 북한군이고 남았냐

야 이렇게 생각하는데 얘가 왜 북한군이고 남았냐 내 마음이지.

그냥 상상을 하는 거예요.

그냥 그렇게 생각을 하는 거예요.

어렸을 때 배운 수학은 항상 정답이 있거든요.

1단위는 얼마야?

2요.

2단위 3은 4요.

빡 때리고 오지

왜 사야?

에이 이러잖아요.

항상 우리가 수학이 정답이 있다고 수십 년 동안 배웠는데 그 말 수 있지만 꼭 정답이 있는 건 아닙니다.

사실은 현대 수학에서 다 뭔가 정학이 나뉘고요.

1덕 2인 것도 그냥 그렇게 정한 거예요.

1덕 2sd 3인 수학도 만들 수 있습니다.

4인 수학도 만들 수 있고 5인 수학도 만들 수 있고 마찬가지로 우리가 행렬을 곱할 수 있는 수학도 만들면 됩니다.

내가 그렇게 만드는 거예요.

그리고 그러면은 그냥 정학이 나름이기 때문에 문서 단어 행렬을 두 개의 곱셈으로 나타낼 수도 있고 그거를 이렇게 볼 수도 있고 그거를 이렇게 볼 수도 있습니다.

그렇게 하자고 하면은 할 수 있는 거죠.

근데 우리가 1덕 2인 수학도 만들 수 있고 3인 수학도 만들 수 있고 4인 수학도 만들 수 있다.

다 만들 수 있는데 수학적으로만 보면 이게 다 맞는 얘기예요.

다 정학이 나름이기 때문에 이렇게 정할 수도 있고 저렇게 정할 수도 있는데 우리가 학교에서 이 수학만 가르치는 이유는 뭐냐면 이게 제일 쓸모가 있으니까요.

예를 들면은 우리가 편의점에서 1플러스 1으로 팔면 2개를 줘야 되잖아요.

그죠?

이 수학이 편의점에서 쓸 수 있는 수학이다.

이 수학을 쓰면 어떻게 됩니까?

1플러스 1을 가져왔더니 갑자기 3개가 돼.

이상하잖아요?

편의점에서 못 쓰는 수학인 거죠?

이해 되세요?

그러니까 정학이 나름인데 그렇게 정했을 때 쓸모가 있느냐는 또 다른 문제거든요.

그러니까 우리가 이렇게 쪼개서 볼 수 있는 건 그냥 우리 마음이 있는 거.

그게 쓸모가 있느냐는 또 다른 얘기예요.

근데 해보니까 쓸모가 있더라면 그럼 우리는 이걸 쓰기만 하자.

쓸모가 있으니까.

그래서 그게 잠재의미 분석입니다.

정학인 것까지는 마음대로 정하는데 그렇게 해보니까 꽤 쓸모가 있더라.

사실 텍스트 분석에는 그런 게 되게 많아요.

왜 이렇게 하나요?

그냥 그렇게 할 수 있으니까.

근데 그게 뭔가 수학적으로 이렇게 무조건 해야 된다

이런 건 아니고 이렇게 할 수도 있고 저렇게 할 수도 있는데 이렇게 해보는 것도 해보니까 괜찮더라.

약간 저항감이 많이 느껴져요.

실제로 이런 거를 배우면은 우리가 이때까지 익숙하던, 알던 세계와 좀 다르기 때문에 우리는 항상 수학이라는 게 답이 있고 이렇게 생각하는데 이래도 되고 저래도 된다

이러면 되게 기분 나빠거든요.

아니 왜 그럼

나는 혼나면서 이렇게 여러 가지 수학을 배웠나.

기분이 나쁘지만 보통 중고등학교 때 배우는 건 제일 쓸모 있는 수학 하나를 가르쳐줍니다.

여러 가지 수학 조직선.

그래서 어쨌든 이렇게 보면 여기 가정 이런 말이 나와요.

다 정하는 겁니다.

그냥 그렇다고 시작.

그래서 하는 거예요.

그래서 이 앞에 거를 문서 토픽 행렬이고 뒤에는 토픽 단어 행렬이 되는데 그래서 원래는 문서 단어 행렬이죠.

문서 단어 행렬이면 문서 단어 행렬에서는 이 방향으로 가면 문서가 있고 숫자들이 뭡니까?

그 문서에 어떤 단어가 몇 번 나왔는지 되겠죠.

그러면 문서 토픽 행렬은 뭐냐면 어떤 문서에 어떤 토픽이 몇 번 나왔는지가 단어가.

그래서 토픽이라는 거는 어떤 되게 추상적인 개념인데 그냥 여러분들이 아시는 토픽이라고 생각하셔도 돼요.

그러니까 토픽이랑 말 무슨 말인지 알 수 있어요.

어떤 주제 이런 거잖아요.

이 문서에는 예를 들면 환경과 관련된 주제가 많이 나오고 정치와 관련된 주제가 좀 많이 나왔는데 경제와 관련된 주제는 좀 적게 나왔어.

이렇게 얘기할 수도 있고 그런 식으로 우리가 다르게 보는 겁니다.

근데 이 토픽 자체는 잠재면수에요.

우리가 직접 관찰 된 건 아닌데 이렇게 행렬의 분해를 하면 뭔가 숫자가 나오긴 나오거든요.

그래서 신기한 그런 개념이죠.

뭐냐면 우리가 직접 관찰하지 않았는데 행렬의 분해를 이용을 해서 우리가 관찰하지 못한 그 무엇에다가 어떤 숫자를 부여를 할 수가 있습니다.

본 적도 없는 걸 그냥 저렇게 계산해서 부여를 하면 되냐?

이런 질문 하면 안 된다는 거 아시겠죠?

그냥 그렇게 하기로 한 거예요.

그냥 우리 마음대로 정한 거예요.

누구 마음대로 그렇게 정해?

제 마음뿐입니다.

중요한 건 뭐냐면 마음대로 할 수 있는데 쓸모 있냐

없냐가 문제지 쓸모...

마음대로는 할 수 있습니다.

그거는 약간 극복하셔야 돼요.

마음대로 할 수 있다는 건 그래서 그냥 우리 마음대로 이거를 토픽이라고 생각하기로 한 겁니다.

그 다음에 이제 여기는 토픽 단어 행렬이니까 이것도 이걸 뒤집어서 생각하면 단어 픽으로 바꿀 수 있겠죠?

전치시키면.

그럼 어떻게 됩니까?

단어가 있으면 그 단어에는 어떤 토픽이 많이 들었는지 적게 들었는지 예를 들면 우리가 전기차 이런 단어를 생각해봅시다.

예를 들면 전기차라는 단어에는 어떤 환경이라는 토픽이 많이 들어 있어요 적게 들었어요.

많이 들어 있다고 생각할 수 있죠.

누가 그게 객관적으로 그래라고 하면 그건 아니죠.

그냥 우리 마음대로 생각하는 거죠.

근데 그렇게 생각하는 게 쓸모로 있잖아요.

어떤 사람은 전기차와 환경이랑 사실 무슨 상관이냐?

그러면 그 사람 말도 맞아요.

생각하기 나름이거든요.

전기차와 환경이라는 토픽이 어떤 사람은 많이 들어 있다고 생각할 수도 있고 어떤 사람은 별로 안 들었다고 생각할 수 있는데 그거는 잠재변수이기 때문에 우리가 관찰할 수 있는 게 아닙니다.

정담이 있는 게 아니에요.

그냥 생각입니다.

근데 예를 들면 어떤 사람이 전기차는 환경이랑 관련이 있어.

근데 요즘에 환경에 관찰이 있으니까 사람들이 전기차를 많이 살 거야 라고 생각을 했어요.

근데 실제로 많이 팔리네.

그러면 그게 많이 팔리는 이유가 전기차와 환경이랑 관련이 있어 있을 수도 있고 아닐 수도 있고 그거는 모르는데 그거 자체는 영원히 모르지만 내가 그렇게 생각한 거 자체가 나한테 쓸모가 있는 거죠.

그래서 전기차 주식에 투자했는데 돈을 많이 벌었어요.

돈은 많이 벌었잖아요.

내 생각이 맞는지 틀린지는 모르지만 어쨌든 그렇게 생각한 게 쓸모는 있었다는 거죠.

그래서 이런 식으로 접근하는 게 잠재변비분석입니다.

그래서 잠재변비분석의 특징은 계산이 비교적 쉬워요.

왜냐하면 두 개를 꼽하는 형태로 쓰는 거기 때문에 복잡한 게 별로 없고 우리가 그 계산을 실제로 하지도 않습니다.

성격은 다 해주니까.

그 다음에 통계학에서 굉장히 많이 사용하는 기법이고.

근데 단점이 뭐냐면 아까 제가 얘기했듯이 예를 들면 전기차라는 개념을 분석하면 전기차가 무슨 환경이랑 관리해서 딱 떨어지게 나오지는 않아요.

뭔가 숫자들이 몇 개가 나오고 끝납니다.

이 숫자의 의미가 뭐냐

이런 거를 좀 해석하기 힘들어요.

뭔가 어떤 숫자가 나옵니다.

그 다음에 좀 이따 얘기하겠지만 차원이 있는데 차원의 수를 결정하기가 어려워요.

그 다음에 또 하나 문제가 해가 무수히 많습니다.

아까 12를 뭐와 뭐의 굽셈으로 나타낼 때 3 곱하기 4라고 할 수도 있고 2 곱하기 6이라고 할 수도 있죠.

답이 여러 개가 있잖아요.

그렇죠?

근데 LSA도 답이 여러 개가 한 개입니다.

굽셈 형태이기 때문에 그 굽셈이 되는 게 종류가 무수히 많아요.

그러면은 이거는 장점인데 앞에 두 개는 장점인데 뒤에 세 개는 좀 단점이죠.

이거를 어떻게 변곡하느냐 이거는 좀 이따 얘기를 해보도록 하고.

어쨌든 일단 단순하기 때문에 많이 애용되는 그런 것입니다.

그러면은 일단 핵열부대를 해야 되는데 우리가 핵열부대를 하는 방식이 여러 가지가 있습니다.

왜냐하면 그냥 우리가 뭐 자연수의 곱셈 보다는 핵열에 곱셈이 복잡하기 때문에 부대를 하는 방법도 굉장히 많아요.

자연수 같은 경우도 지금 12만 두 개의 곱셈 형태로 나타내려고 해도 3 곱하기 4도 되고 2 곱하기 6도 되고 여러 가지가 있는데 핵열으로 곱셈 형태로 풀어내려면 이것도 종류가 되게 많거든요.

근데 우리가 ls에서 쓰는 방법은 특이값 분해를 사용합니다.

왜 라고 물어보면 안 되는 거 아시겠죠?

왜까요?

그냥 그렇게 하기로 한 거예요.

그래서 특이값 분해는 어떻게 분해를 하는 거냐면 가로로는 m계의 행이 있고 세로로는 n계의 열이 있는데 이렇게 3개로 분해를 하는 겁니다.

그래서 아까 운선 포픽 핵열이 첫 번째가 되고 세 번째가 포픽 단어 핵열이 되고 이런 식으로 분해를 하는 겁니다.

직교 핵열대각 핵열나

이런 거 있는데 여기서는 중요한 건 아니고 어쨌든 이렇게 분해를 한다.

근데 이제 여러 가지 중에서 특이값 분해를 사용하는 이유가 있어요.

특이값 분해를 하면 신기하게도 전체 핵열이 있는데 그중에 일부 회색으로 칠한 데 있죠?

일부의 값만 뽑아내서 이렇게 핵열을 줄일 수가 있어요.

신기한 게 뭐냐면 이렇게 3개를 곱한 거랑 이렇게 잘라내고 크기를 줄인 거 u스타, 시그마스타, v스타를 곱한 거랑 3개가 똑같진 않은데 비슷한 결과가 나와요.

이렇게 곱하나

이렇게 곱하나 결과가 똑같진 않지만 거의 비슷해요.

이게 SVD의 특징입니다.

그래서 우리가 이걸 써요.

이걸 쓰면 왜 좋으냐

그러면 이렇게 쓰면 u스타는 문서 토픽 행렬인데 문서가 m계가 있어요.

이 k가 뭐냐면 토픽이거든요.

토픽의 갯수를 이게 무슨 얘기예요?

토픽을 확 줄여도 결과적으로 만들어지는 문서 단어 행렬은 거의 똑같다는 얘기예요.

원래 문서 단어 행렬이 있는데 이거를 약간 오차가 흔들긴 하지만 아주 적은 수의 토픽으로 표현을 할 수가 있어요.

이 문서가 있을 때 굉장히 적은 개수의 토픽으로 표현을 할 수가 있죠.

단어도 마찬가지입니다.

k가 토픽 개수인데 단어가 m계가 있는데 이 m계의 단어를 아주 적은 k개의 토픽만으로 표현을 할 수가 있게 됩니다.

그래서 이거를 truncated SVD라고 합니다.

SVD는 이렇게 하는 거라 SVD라고 하고 트윗 값도 싱글러, 밸류, 디커포스 트럼페이트는 잘라낸다는 뜻이에요.

그래서 잘라낸 SVD라는 뜻인데 이렇게 잘라내면 어떤 문서가 있을 때 m계의 문서를 k가지의 토픽만으로 표현을 할 수 있게 됩니다.

k가 아주 작기 때문에 우리가 차원을 축소를 해서 차원이라는 것은 데이터를 표현하는 방법의 가짓수인데 그 가짓수를 굉장히 줄여서 얘기를 할 수 있다.

그래서 이런 것들을 생각하시고 예를 보면 성격 같은 걸 제가 아까 얘기했는데 성격 같은 거로도 여러분들 요즘에 많이 아는 mbti 같은 거를 보면 mbti는 차원이 몇 개입니까?

4차원이죠?

e냐, i냐 n이냐, s냐 f냐, t냐 p냐, j냐

그래서 4가지 충으로 어떤 사람을 나타내는 거잖아요?

가끔 mbti에 대해서 얘기를 할 때 사람을 어떻게 4가지로 얘기해?

이렇게 얘기하신 분들은 통계를 몰라서 얘기하시거든요.

mbti가 문제가 있는 건 맞는데 그 문제가 4차원이라서 문제인 건 아닙니다.

사람들이 mbti에 대해서 비판을 하는데 되게 웃긴 게 뭐냐면 mbti가 비과학적이라고 얘기를 하거든요.

mbti의 문제는 비과학적이어서 문제가 있는 건 맞아요.

비과학적이어서 문제인 건 아닙니다.

제가 항상 재미있게 생각하는데 사람들이 흔히 생각하는 사실 우리 생활을 보면 우리 중고등학교 때 과학 별로 안 좋아하잖아.

근데 나이가 들면 본인이 중고등학교 때 과학을 그렇게 좋아하지도 않았는데 mbti는 비과학적이야

이런 식으로 얘기한 사람이 되게 많습니다.

저는 신기한 테라.

왜 그러시는지 모르겠는데 mbti의 문제는 비과학적이라서 문제가 아니에요.

과학적인 것도 아니지만 딱히 비과학적이지 않고 딱히 과학적인 것도 아니지만 예를 들면 우리가 길이로 잴 때 한 뼘 두 뼘 재면 그거 자체가 비과학적인 건 아닙니다.

그냥 부정확한 거지.

두 가지를 구별하셔야 돼요.

부정확한 거랑 비과학적인 건 다른 얘기예요.

mbti의 문제는 그게 부정확한 건 문제지 비과학적이어서 문제는 아닙니다.

과학적이어도 정확하지 않을 수 있거든요.

여러분 1기가 과학적이니까 비과학적이고 과학적으로 하죠.

그래도 안 맞잖아요.

과학적으로 한다고 해도 꼭 맞는 건 아니고 비과학적으로 해서 꼭 안 맞고 아이고 허리야 아이고 무릎에 내일 비 오려나 그렇게 과학적이긴 하지만 대충 맞잖아요.

그쵸?

그러니까 좀 다른 차원이에요.

mbti 같은 경우도 잘 모르는 분들이 사람 성격을 어떻게 내가지고 이게 뭐 못할 거 없니까?

다 정확히 나를 해요.

실제로 보통 심리학에서 쓰는 거는 빅파이브라고 있는데 이름에 5가 들어가죠.

5차원이에요.

mbti랑 한 차원 밖에 차이 없어요.

과학적으로 하는 방법이나 mbti에서 하는 거나.

사실 차원이 4차원이라서 문제는 심리학에서 하는 빅파이브도 문제죠.

4나 5나 5차이 있습니다.

그러니까 이걸 4나 5나 했다는 게 문제는 아니에요.

그래서 어쨌든 우리가 사람 성격을 4차원 아니면 빅파이브는 5차원으로 나타내는데 4차원이나 5차원으로 나타낼 수 있습니다.

그리고 나타내는 게 편해요.

왜냐하면 예를 들면 어떤 사람이 있는데 이 사람이 어떤 사람이냐

그럼 당연히 그 사람은 굉장히 복잡합니다.

여러 가지 복잡한 점을 가지고 있는데 예를 들면 우리가 어떤 사람에 대해서 진단을 해야 된다.

아니면 어떤 사람한테 진로를 지도를 해야 된다.

예를 들면 어떤 직업을 추천을 해줘야 되는데 이 사람 성격에 맞는 직업을 추천을 해줘야 될 거 아니에요.

사람의 성격은 아주 복잡다단하니까 추천을 못 해줘.

복잡다단한 게 무슨 의미가 있습니다.

그런데 예를 들면 너는 외향적이고 사람 만나는 걸 좋아하니까 너는 영업이나 이런 쪽을 해보고 좋을 것 같다.

이렇게 추천할 때 그러면은 외향적이라든가 사람 만나는 걸 좋아한다든가.

이거는 어떤 특정 차원을 얘기를 하는 거에요.

우리가 차원을 줄이는 게 일정적으로 도움이 되는 사운드가 많이 있다는 거에요.

그래서 이제 성격 같은 걸 할 때 우리가 차원을 줄여서 하듯이 우리가 텍스트에도 예를 들면 특허가 있는데 수백 개의 특허가 있어요.

특허는 다 다릅니다.

다 다르지만 우리가 유산 특허를 찾는다든가 특허를 분류한다든가 이럴 때 좀 차원을 줄여가지고 간단하게 얘기하면은 이해하기 쉬울 때가 있겠죠.

아니면은 사람 성격도 이렇게 하고 아니면은 이제 예전에 스티브 잡스가 살아있을 때 아이폰 처음 내릴 때 스티브 잡스가 어떤 차트를 들고 왔냐면 현재 시장에서 핸드폰들이 있는데 핸드폰들이 스마트하냐 쓰기가 쉬우냐

이렇게 놔뒀을 때 기존의 핸드폰들은 쓰기는 쉽지만 스마트하지 않냐

쓰기는 쉽지만 스마트하지 않던가 스마트하지만 쓰기가 어려웠는데 우리 아이폰은 쓰기도 쉽고 스마트합니다.

이러면서 짜잔하고 들고 나왔거든요.

그러면 이 사람은 지금 뭘 한거에요?

수많은 핸드폰들을 2차원으로 나타낼거죠.

쉬우냐 아니냐 스마트하냐 아니요.

그렇게만 나타낼 수 있겠어요.

키보드가 있냐

없냐를 할 수도 있고 모니터가 스크린이 크냐 아니냐를 할 수 있고 여러가지 차원으로 얘기할 수 있는데 2차원으로 보는게 쓸모가 있다고 생각하는거죠.

이해되십니까?

2차원으로 해야하는 법이 있는 건 아니에요.

제가 계속 얘기해드리지만 저학이 나름입니다.

여러분 마음이에요.

근데 그게 쓸모가 있냐

없냐는 별개의 얘기다.

근데 아이폰은 그렇게 구별하는게 쓸모가 있었던거죠.

이해되시죠?

그래서 우리가 차원을 줄이는 것이 그렇게 해도 되냐?

마음이니까 근데 그게 쓸모가 있냐는 별개의 얘기다.

차원 축소를 하면 왜 쓸모가 있느냐 하면 문서를 결국 구성하는 건 단어들인데 단어라는 거는 예를 들어 아까 보면 추출 추출 물 추출 액 이런 단어가 있으면 단어는 다르지만 결국 의미는 거의 비슷하잖아요.

이런걸 어떻게 좀 잘 축소를 하면은 얘네를 다시 재배치를 해서 이런 개별적인 단어의 차이를 무시할 수가 있어요.

그 다음에 뭐 오타자라든가 이런 것들도 어느 정도 처리가 되고 그 다음에 문서에 어떤 노이즈가 존재합니다.

예를 들면 추출이라는 단어가 어떤 문서에는 예를 들어서 12번 나오고 어떤 문서에는 11번 나왔어요.

한 번의 차이가 있는데 그렇게 큰 차이냐 하면은 그렇지 않을 수 있거든요.

12번 나온 거랑 11번 나온 거랑 뭔 차이인 거죠?

그러니까 이런 어떤 사소한 차이, 여기서 노이즈라고 말하는 거는 큰 의미 없는 차이 이런 거 있잖아요.

샴푸 특헌데 갑자기 되게 뜬금없이 로봇 이런 단어가 한 번에 쓱 지나갔다가 나왔다가 사라졌어요.

근데 이 특허 전체에 단어가 막 수만 개가 있는데 로봇 한 번 나온 게 큰 의미가 있겠습니까?

그냥 뭐 딴 얘기 하나가 요즘의 현대 사회에서는 로봇도 있고 뭐 도 있는데 이런 식으로 얘기했으면 그 단어는 의미 없는 거잖아요.

그런 어떤 사소한 의미 차이 이런 거를 무시할 수가 있어요.

사원을 자유시키는 거.

그래서 그런 이유 때문에 대체로 쓸모가 있다.

물론 실제로 쓸모가 있는지는 해봐야 합니다.

항상 쓸모가 있는 건 아니에요.

어떤 데이터에서는 쓸모가 있는데 또 다른 데이터에는 쓸모가 없을 수도 있습니다.

개박해거든요.

그래서 본은 해봐야 한다.

그래서 이거는 그림으로 좀 나타내면 사원 축소의 개념을 그림으로 나타내면 우리가 이렇게 2차원이 있고 2차원의 데이터가 이렇게 있어요.

그러면 이 데이터에다가 주황색으로 새로운 축을 잡을 수가 있어요.

그렇겠죠?

이거는 우리 마음이에요.

왜 막 그렇게 잡습니까?

하면 안 됩니다.

내 마음이.

그래서 이렇게 축을 새로 잡고 생각을 해보니까 이쪽 방향의 축은 없어도 그만인 거 같잖아요.

그쵸?

왜냐하면 대부분 데이터가 대부분 이쪽 방향으로 퍼져있지

이쪽 방향으로 거의 안 퍼져있단 말이에요.

그러면은 이쪽 방향은 없어도 되겠네.

이렇게 생각할 수 있겠죠.

그럼 날려버리면 됩니다.

날려버리면 이쪽 방향만 생기는 거죠.

이쪽이냐?

이쪽이냐.

그러니까 우리가 예를 들면은 요즘에 뭐 삼겹철인데 예를 들면은 좌냐 우냐

이렇게 나누잖아요.

그런데 보면은 사람의 생각이라는 게 다 다양하잖아요.

예를 들면 현안이 여러 가지가 있으면 수백 가지 현안이 있으면 사람마다 의견이 다 다르단 말이에요.

예를 들면 최저임금을 올려야 되냐 낮춰야 되냐

그다음에 북한이랑 어떻게 해야 되냐

주택정책을 어떻게 해야 되냐

공무원 정책을 어떻게 해야 되냐 정책이 수백 가지 정책이 있으면 수백 가지 정책을 받은 사람들의 의견이 다 다르다는 거 아니에요.

그러면은 그걸 어떻게 좌냐 우냐 우연히 나누냐.

생각해보면은 실제로 해보면 여러 가지로 사람들의 의견이 다 똑같지 않아요.

다 다르거든요.

5천만 국민의 의견이 같은 사람이 아무도 없습니다.

의견이 다 달라요.

그런데 보통 정치에서 좌우로 나누는 이유가 없니까?

그러니까 다 정책이 다른데 대체로 그래도 예를 들면 최저임금 정책이랑 주택정책이랑 이쪽에서 의견이 같으면 전쪽에서도 의견이 대체로 보통 같은 경우가 많아요.

대체로 이제 정부 개입을 많이 하는 쪽이 좋다는 쪽이 있고 적게 하는 쪽이 좋다는 쪽도 있고 그렇죠.

그래서 이제 보면 좌우로 가면 정부 개입을 하는 쪽을 선호하고 우는 쪽을 선호하는데 사실 이것도 보면은 또 안 돼?

이렇지도 않거든요.

경제적인 측면은 그런데 또 사회문화적인 측면으로 가면 보통 우쪽으로 가면은 또 정부가 개입하는 걸 대체로 선호합니다.

사회문화적인 측면에서.

좌로 가면은 덜 개입하는 거를 선호해요.

그래서 어떤 사람은 좌저예요.

정치..

정치적으로는 사회문화적으로는 정부가 개입하는 거를 별로 선호하지 않지만 예를 들면 영화, 거멸 이런 거는 반대하는데

그런데 최저임금 이런 거는 정부가 개입해야 해요.

이렇게 생각하는 사람이 있고.

어떤 사람은 좌우 성형이 있어요.

그래서 둘 다 개입해야 해.

이렇게 생각하는 사람이 있고.

우, 좌 성형도 있고.

그러면 또 복잡해지는 거죠.

실제로는 이게 어떤 게 쓸모가 있냐.

이거는 상황에 따라 다릅니다.

어떤 나라에서는 좌우로만 나누는 게 도움이 될 때가 있고 어떤 나라에서는 네 가지로 나눠서 보는 게 쓸모가 있을 수도 있고.

이게 차원을 몇 차원으로 보는 게 맞느냐 라는 거는 딱 정해진 게 아니에요.

그때그때 쓸모 있는 걸로 하는 겁니다.

그런데 우리가 상황에 따라서는 예를 들면 이런 데이터가 있다.

그러면은 우리가 그냥 이쪽 방향이냐, 이쪽 방향이냐

이렇게 생각할 수도 있다는 거죠.

이해되십니까?

근데 바꿔나라면 그렇게 생각을 안 할 수도 있다는 거죠.

그걸 항상 염두에 두셔야 합니다.

우리가 통계를 배우면 그런 거를 자유롭게 바꿀 수 있는 그게 필요해요.

예를 들면 아까도 얘기했지만 좌우 이렇게 하면 아 꼭 그렇게 생각할 수도 있겠구나.

이런 게 무슨 법에 헌법에 우리나라는 정치 성향을 좌와 으로만 구별하기로 한다는 게 정해져 있는 게 아니잖아요.

그냥 그렇게 생각하는 것 뿐이라는 걸 다르게 생각할 수도 있는 거죠.

이렇게 생각할 수도 있고 저렇게 생각할 수도 있고 중요한 거는 이렇게 생각할 수도 있고 저렇게 생각할 수도 있다 해서 끝나면 안 되고 그 중에 더 쓸모 있는 게 뭐냐

그거를까지 할 수 있어야 합니다.

그럼 이것까지만 하고 실습은 왜 실습은 가무중 일이죠?

그래서 주성분 분석이라고 PCA라고 하는 방법인데 이 PCA는 아주 통계에서 많이 사용하는 방법입니다.

그래서 이 PCA에서 하는 것이 LSA에서 하는 거랑 거의 똑같고 실제로 수학적으로도 둘 다 똑같은 방법을 씁니다.

Truncated SVD라는 방법을 써요.

그럼 PCA랑 LSA 차이는 뭐냐 PCA는 센터링이 더 들어가요.

센터링이란 게 뭐냐 하면 데이터가 있으면 여기다가 평균을 빼주는 거를 센터링을 한다고 합니다.

예를 들면은 내가 어떤 시험을 받는데 시험에 80점을 받았어요.

근데 이 시험의 평균 점수가 60점입니다.

60점을 빼주면 내 점수를 20점으로 만드는 거를 센터링이라고 해요.

왜 이렇게 하냐?

그러면은 40점 받은 사람이 있어요.

그러면 내가 그냥 40점이라는 점수는 보면 이 사람이 시험을 잘 받는지 못 받는지 모르잖아요.

이상 평균을 빼준 거 어떻게 됩니까?

마이너스 20점이 되죠.

아, 이 점수를 보면은 얘가 시험을 잘 받는지 못 받는지 바로 알 수 있습니다.

평균보다 높으면 플러스, 평균보다 낮으면 마이너스 이렇게 됩니다.

그리고 이 사람은 정확하게 평균이랑 똑같은 점을 맞으면 되요.

0이 되죠.

센터링을 0으로 딱 맞추기 때문에 센터링, 이런 가운데가 맞추기 때문에 가운데는 0으로 맞추기 때문에 그래서 이제 원자료를 센터링을 해주고 그 다음에 Truncated SVD를 하면은 LSA가 되고요.

센터링을 안 하고 바로 Truncated SVD를 하면은 LSA가 됩니다.

그 차이에요.

그러면 왜 일반적인 통계에서는 PCA를 쓰는데 텍스트 분석에서는 LSA를 쓰느냐.

여러분 일반적으로 시험을 보면 사람들 점수가 어디에 가있습니까?

평균 근처에 가있죠.

보통 평균 점수를 받는답니다.

평균보다 조금 더 잘 보거나 덜 보지

평균에서 멀리 떨어져 있는 경우는 잘 없단 말이에요.

일반적인 자료는 평균 근처에 많이 분포를 하기 때문에 LSA를 만들 수 있는 센터링을 하는 PCA를 맞습니다.

근데 텍스트 분석이 일반 데이터랑 좀 다른 거는 텍스트 분석은 아까도 보시면은 어떻게 되어있어요?

좌안부 0 근처에 있어요

데이터가.

평균 근처에 있는 게 아니에요.

그래서 데이터 분포가 다른거든요.

그러니까 일반적인 데이터는 여기가 평균이면은 평균 주변에 이렇게 분포하고 있어요.

근데 텍스트 분석은 끝에 0이 있으면 0 근처에 이렇게 있어요.

그래서 데이터가 나타나는 양상이 다르기 때문에 분석하는 방법도 조금 달라지게 됩니다.

그래서 일반적인 통계에서 쓰는 PCA와 유사점과 차이점은 이렇게 정리를 했습니다.

자 이렇게 해서 이번 주 내용은 여기까지고요.

질문 있으신가요?

점심 먹으러 가야 되니까 질문을 드리겠습니다.

다음 이제 갬투 퀴즈에는 뭐가 나오느냐 갬투 퀴즈에는 실습은 별로 안 나오고 이번 주에 한 게 별로 없죠.

PTMI 정도 있는데 코스하인 유사도랑 거기서 이제 몇 가지 T라든가 A라든가 이런 거 물어볼 수 있겠죠.

그 다음에 LSA가 뭐 하는 거냐 왜 이렇게 하는 거냐

이런 거 물어볼 수 있겠죠.

그거를 한 번 금요일쯤에 강의자들 다시 한 번 보시면 안 되면 토요일 아침에 이제 오셔서 빨리 그걸 다시 보시면 퀴즈를 잘 푸실 수 있을 것 같습니다.

이렇게 준비를 하시면 됩니다.

질문 없으시면 오늘 수업 여기까지 하겠습니다.

감사합니다.