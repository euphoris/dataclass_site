그래서 우리가 이제 4주차 슬라이드 13, 14쪽에 있는 거 하시고 그 다음에 5주차 슬라이드로 가서 22조 부터 하면 되는데 그래서 이제 LSA라는 게 여러분들 퀴즈에서도 푸셨지만은 결국에는 여기 문서 단어 행렬 왼쪽에 있는 거를 이렇게 3개로 쪼개요 가운데 있는 거는 일단 중요하지 않고 왼쪽 U하고 V가 중요한데 그러니까 왼쪽이 문서 토픽 행렬 U가 문서 토픽 행렬이고 오른쪽 V가 토픽 단어 행렬이 됩니다

그리고 회색으로 이렇게 조그맣게 회색으로 나타나는데 U는 가로폭이 넓은데 회색으로 칠한 부분이 이제 좁아져 있죠

그 얘기는 뭐냐면 토픽 수를 꽉 줄여놨기 때문에 그래요

그래서 우리가 문서가 굉장히 많은데 그 문서를 아주 작은 개수의 토픽들로 표현을 하겠다 이쪽은 단어들이 N개로 굉장히 많은데 그것도 아주 소수의 토픽들로 설명을 하겠다

이런 얘기입니다

그래서 이렇게 분해하는 수학적인 기법을 SVD라고 하고 여기서 이제 네모가 있는데 네모에서 많이 잘라냈잖아요

그래서 Truncated, 잘라낸 SVD다

그래서 이런 수학 기법을 써서 실제로 LSA 분석을 하게 됩니다

그래서 LSA 함수가 따로 있는 게 아니고 Truncated, SVD를 하면 LSA가 된 거예요

이런 기법을 씁니다

물론 이제 다른 계산 방법으로 LSA를 할 수도 있겠죠

근데 이게 제일 많이 사용하는 계산 방법이고 그래서 이것도 이제 SK런이 있습니다

여기 이제 Dcomposition 이렇게 되는데 Dcomposition은 쪼갠다

이런 거예요

쪼개는데 끝에 D가 이제 Dcomposition입니다

뭘 쪼개냐면 행렬 하나를 여러 개로 쪼개기 때문에 Dcomposition이라고 해요

그래서 이제 Truncated, SVD 한 다음에 설정을 해주면 되는데 Ncomponent는 100 이렇게 했는데 100은 뭐냐면 일단 여기 K를 정해줘야 되거든요

K를 얼마로 할 거냐 그러면은 이걸 이제 여러 가지 방법이 있는데 일단 잘 모를 때는 적당히 일단 쪼개 보고 그 다음에 이제 결과를 본 다음에 다시 한 번 더 돌립니다

그래서 우리가 일단은 그냥 넉넉하게 100개 쯤으로 줄일 거예요

그 다음에 이 SVD의 계산에는 우리가 학교 다닐 때 배우는 거는 1 더 1 하면 이 답이 딱 정해져 있어요

SVD의 답은 물론 그것도 정해져 있습니다

답은 정해져 있는데 이게 그냥 1 더 1 이렇게 공식에 넣으면 답이 딱 나오는 게 아니라 일단 대충 숫자를 때려 넣은 다음에 그 숫자를 조금 조금 조절을 해가지고 답을 맞춰요

그래서 초익 값을 일단 랜덤하게 설정을 합니다

약간 좀 이상하지만 사실 머신러닝의 대부분의 계산이 이런 식으로 돼요

약간 때려 맞추는 시기거든요

그래서 숫자를 조금 조금 바꿔가면서 맞는 답이 나올 때까지 계속 고치는 시기인데 굉장히 어떻게 보면 무식한 방법이죠

그래서 이렇게 하는데 그래서 이제 초익 값을 어떻게 설정하느냐에 따라서 최종 결과가 조금 조금 다르거든요

거의 비슷하긴 하지만 약간 다른데 그래서 우리가 이제 초익 값을 고정하기 위해서 랜덤 스테이트라는 걸 설정을 해 줍니다

그래서 이걸 해 주면 초익 값이 랜덤이지만 똑같은 값이 나와요

왜냐하면 뒤에서 다시 얘기해 드리겠지만 컴퓨터는 랜덤 넘버를 못 만들거든요

어떻게 하냐면 보통 현재 시간을 가지고 어떤 계산을 해서 난수 비슷한 숫자를 만듭니다

그러면 시간을 기준으로 하니까 돌릴 때마다 결과값이 달라지겠죠

그래서 돌릴 때마다 뭐가 달라지니까 랜덤에 보이는 거예요

그러면 시간이 흘러가면 계속 숫자가 바뀌니까 우리가 그거를 시간 대신에 강제로 어떤 값 1, 2, 3, 4가 아니어도 됩니다

하나의 값으로 딱 해 주면 언제 돌려도 랜덤 넘버처럼 난수처럼 보이지만 똑같은 숫자가 만들어집니다

여러분하고 저하고 똑같은 결과가 나오게 하기 위해서 이거는 고정을 해 주는 거예요

그다음에는 문서단행렬 만들 때처럼 핏 트랜스포면 이거를 변환이 됩니다

이렇게 나오는 결과는 제가 독 EMB를 했는데 이거는 뭐냐면 문서에 대한 문서 토픽 행렬이 됩니다

그래서 지난주 슬라이드 22조 그 다음에 우리가 퀴즈를 먼저 하고 수업을 하니까 1교시가 항상 너무 짧더라고요

그래서 이번 주에는 한번 쭉 하고 쉬는 시간 중간에 한 20분 이렇게 하고 그다음에 쭉 하고 이렇게 한번 해볼게요

뭐 혹시 나는 그렇게 하면 못 하겠다

이런 그런 느낌으로 한번 해볼게요

그러면 다음 주에 또 한번 제가 또 한 번 더 해볼게요

혹시 나는 그렇게 하면 못 하겠다

이런 분 계신가요

어차피 쉬는 시간 야한은 똑같습니다 중간에 너무 끊으니까 수업이 자꾸 너무 끊어져 가지고 실습이 쭉 좀 이어져야 될 것 같아서 일단 한번 그렇게 해볼게요

아니면 다음 주에 다시 원래 대로 돌아가면 되겠죠

그래서 우리가 이제 LSA는 끝났습니다

간단해요

그냥 세 줄로 끝났고 그 다음에 이제 아까 제가 컴퓨터가 난수를 못 만든다고 했는데 그래서 어떻게 하냐면 어떤 그냥 이런 공식을 만든다 되게 복잡한 공식을 만들어 가지고 여기에다가 어떤 숫자를 싹 집어 넣으면 뭐 이렇게 이렇게 계산하면은 되게 이제 여기 넣은 숫자랑 여기 넣은 숫자가 되게 달라 보이는 어떤 숫자가 나와 그거를 다시 여기다 집어 넣으면 또 되게 달라지는 그래서 이걸 하면은 마치 난수처럼 보이는 어떤 숫자들이 7 나왔다가 9846 나왔다가 다시 421 나왔다가 막 이렇게 막 이리저리 막 튀는 값이 나오는데 그래서 겉으로는 난수처럼 보이지만 사실은 난수가 아닙니다

그냥 정해진 고정된 수열이에요

그래서 이제 이런 거를 유사 난수라고 하고 그래서 이제 제일 처음에 넣는 값을 이제 씨앗 값이라고 합니다

여기 넣은 게 이제 씨앗 값이에요

그래서 요거에 똑같으면은 항상 랜덤해 보이지만은 똑같은 결과가 나와

그래서 이제 컴퓨터에서는 이런 식으로 가짜 난수를 생성을 한다

그래서 요거를 이용하는 이제 트릭 같은 것도 있는데 뭐냐면 되게 특이한 트릭 중에 예를 들면 우리가 어떤 파일을 압축을 해야 되는데 그 파일을 어떤 랜덤한 수열로 이렇게 만들 수가 있어요

좀 얘기하면 좀 힘들지만 그러면 그거를 거꾸로 찾는 거예요

어떤 씨앗 값을 넣었을 때 이 어떤 특정 씨앗 값을 넣어서 그걸로 난수를 만들면 이 파일하고 똑같이 된다 그러면 어떻게 하냐면 그 파일 전체를 씨앗 값 하나로 압축을 할 수가 있습니다

약간 이상한 트릭인데 사실 이게 현실적으로 말이 되는 건 아니고 되게 특수한 경우에만 쓰는데 그런 경우에는 심지어 어떤 되게 큰 파일을 숫자 하나로 바꿀 수가 있어요

근데 뭐 실제로 거의 존재하는 경우는 아니고 아주 드물게 쓰이는 분야도 어쨌든 이제 여기서 랜덤 스테이트 1, 2, 3, 4의 의미는 그런 거고 그다음에 이제 우리가 트렁케이드드 SVD를 했는데 중요한 게 이 컴포넌트의 개수거든요

이거를 몇 차원으로 줄일 거냐

그래서 그거를 할 때 여러 가지 방법이 있는데 스크린 플롯이라는 걸 그립니다 스크린 플롯에서 스크린은 뭐냐면 이게 이제 절벽이 이렇게 있는데 절벽 밑에 보면 절벽에서 돌들이 굴러 떨어지거든요

조금씩 자갈이나 모래 같은 게 떨어져 가지고 그게 세월이 지나면 절벽 밑에 이렇게 쌓여요

그래서 이렇게 쌓인 거를 영어로 스크리라고 한다고 합니다

저도 영어를 잘 못해서 뭐 언어미는 제가 평생 통계를 제외하고서는 이 스크리라는 단어를 쓴 걸 본 적이 없거든요

모르겠어요

언어미는 이걸 아는...

그러니까 우리도 한국 사람한테 절벽 밑에 모래 쌓인 걸 뭐라고 불러요?

뭘 뭐라고 하겠는데 절벽 밑에 모래 쌓였다고 이거에 이름이 있다는 게 정말 되게 이상한데

여러분 혹시 갑자기 생각해보니까 여러분 혹시 이거 이름 아십니까?

벽 있으면 벽 밑에 이렇게 있는 산떼기 이거요?

이것도 이름이 있더라고요

까먹은 혹시 아시는 분이 있네요

그러니까 세상에 뭐든지 이름이 있겠죠

건축하시는 분들은 알 거 아니에요

우리는 벽 밑에 있는 산떼기 이렇게 하지는 않는데 그분들이 그렇게 부르지는 않겠죠

저도 까먹었는데 하여간 그런 식으로 세상에 이런 단어도 있습니다

그래서 스크리라고 해서 절벽 밑에 이렇게 모래가 쌓인 모래나 자갈이 쌓인 걸 스크리라고 하는데 그래서 보면 뚝 떨어지다가 이렇게 평평해지는 이런 모양인데 그래서 그거랑 닮았다고 해서 스크리 플롯이라고 해요

보시면 뚝 떨어지다가 평평한 이런 건데 그래서 이제 SVD에 보면 Explained Variance라는 걸 가지고 그리면 이런 모양이 됩니다

이 모양이 의미하는 게 뭐냐

우리가 이제 트렁케이티드 SVD를 하면 이런 식으로 차원을 축소를 하거든요

그럼 차원을 축소할 때 어떻게 축소를 하냐 하면 어떤 데이터가 있을 때 보통 대부분의 데이터는 특정 방향으로 크게 변합니다

그리고 다른 방향으로는 좀 잘 안 변해요

예를 들면 한국 사람이다 하면 보통 한국 사람도 다 달라요

어떤 사람 키가 크고 어떤 사람 키가 작고 어떤 사람은 성격이 급하고 어떤 사람은 성격이 안 급하고 이렇게 좀 차이가 있는데 예를 들면 머리카락 색깔은 별로 차이가 없잖아요

어떤 면에서는 차이가 큰데 어떤 면에서는 차이가 작고 이런 게 있단 말이에요

대부분의 데이터가 모든 방향으로 다 똑같이 변하는 게 아니라 어떤 방향은 크게 변하고 어떤 방향은 작게 변하고 그런 게 있습니다

그래서 우리가 차원 축소의 논리는 뭐냐면 이 작게 변하는 방향은 그렇게 중요하지 않으니까 그 차원은 없어도 그만이다

이거예요

예를 들면 우리가 어떤 사람에 대해서 얘기할 때 한국 사람인데 그 사람이 머리 까맣고 이런 얘기 잘 안 하잖아요

뭐 키 크고 성격 나쁘네

걔 있잖아

뭐 이렇게 얘기하지

왜냐하면 머리 까맣다는 얘기는 별로 정보가 없단 말이에요

왜냐하면 다 머리가 까맣거나 물론 좀 더 밝은 사람도 있고 좀 어두운 사람도 있고 요즘에는 외국 출신들도 이민을 많이 오니까 좀 차이가 있긴 한데 사실 대부분은 별로 언급 안 해도 큰 상관 없단 말이에요

그러니까는 어떤 데이터에서 크게 변하는 방향이 있고 작게 변하는 방향이 있는데 우리가 생각할 때 작게 변하는 방향의 차원은 그냥 없어도 그만 아니냐

데이터에서 그런 거 필요하냐, 날려버리자 이런 거 그래서 이 스크립 플롯이 나타나는 게 뭐냐 하면 차원을 순서대로 0 1 2 3 4 이렇게 쭉 가다 보면은 뒤로 가면은 짜잘한 차원들이 이렇게 있습니다

이쪽 방향에서는 그냥 그 이쪽 방향의 이제 변하는 정도라고 생각하실 수 있는데 변하는 정도가 다 짜잘한 차원들이 있어요

이쪽에는 앞부분에는 변하는 정도가 큰 차원들이 있고 그러면은 이제 크고 크고 크고 크고 하다가 이제 점점 점점 점점 줄어들어서 이제 뒤로 가면은 이제 짜잘한 애들이 이렇게 있는 거예요

이제 이거를 해보면 우리 데이터에다가 해보면 차원이 100개니까 100가지 쭉 있는데 보시면은 보통 이렇게 튈 때도 있고 안 튈 때도 있습니다

이거는 이제 이렇게 나타나는 거는 우리가 지난주에 PCA하고 LSA의 차이를 얘기했는데 PCA는 센터링을 하고 LSA는 센터링을 안 한다

이렇게 얘기했는데 PCA에는 이렇게 튀는 게 없습니다

센터링을 해주면 여기 없어지거든요

근데 이제 센터링을 안 하면 이런 현상이 나타나고 어쨌든 튀든 말든 이렇게 갔다가 쭉 내려가면 될까요

여기는 지금 0번 차원은 여기 이제 배리언스인데 배리언스가 12 조금 넘죠

굉장히 큰 거예요

배리언스가 굉장히 배리언스가 크다는 거는 말 그대로 배리언스가 분산이죠

어떤 건 되게 크고 어떤 건 되게 작고 숫자가 막 들쭉날쭉 하다는 거예요

근데 이제 뒤로 가면은 거의 이제 고만고만 하거든요

이런 애들은 배리언스가 작아요

뭐냐면 예를 들면 우리가 지금 특허 관련된 문서인데 특허에 보면은 예를 들면 상기 이런 단어는 그냥 모든 특허에 다 나오거든요

그런 단어는 그냥 여기도 나오고 밭에도 나오고 그러니까 골고루 나오니까 배리언스가 작아요

그런 단어와 관련된 차원들은 여기 뒤쪽에 이제 고만고만하게 있는 거죠

그럼 우리가 이제 100개의 차원으로 축소를 했는데 어차피 뒤에 나오는 차원들은 그냥 고만고만한 그냥 꼬맹이들이잖아요

짜잔한 애들이란 말이에요

그러면 이게 절벽이 있으면 절벽에 떨어진 모래 같은 애들이랑 쓰죠

얘네는 짜잔한 애들이니까 사실 뭐 고만고만하고 그러면은 이제 앞에 있는 큰 애들 이것만 남기자

이게 중요한 부분인 거죠

왜냐하면 문서마다 확 확 달라지는 부분 그래서 보통 여기 꺾이는 부분에서 자릅니다

대충 보면은 한 여기 좀 더 크게 보면 되게 좋겠지만 대충 보면 한 13 14 이 정도에서 여기가 10 정도 될 거고 13 14 이 정도에서 확 꺾이는 게 보이죠

여기 이쪽 여기서 그래서 이렇게 꺾이는 지점을 이제 우리가 팔꿈치라고 부르는데 팔꿈치가 되는 부분에서 이렇게 잘라 그래서 강의자료에서는 그래서 어디서 잘랐냐면 아 그게 없네

강의자료에 여기 이제 100으로 돼 있는데 대충 한 13 14 13 14 정도 되면 되겠죠

여러분들은 어떻게 보십니까

한 13이라고 합시다

그래서 13에서 자르겠습니다

그래서 여기 우리 분석했던 코드 앞으로 돌아가서 100을 13 이렇게 줄여요

이렇게 다시 한번 돌려주시면 스크립 플로트 그리면 이렇게 되다가 이제 뭐 그렇게 확 꺾이는 부분이 없어졌죠

확 꺾이는 부분에 잘랐으니까 큰 애들만 남기죠

비교를 위해서 그래서 여기서 13 이렇게 잘라 사실 이제 다시 돌리지 않고 횡절해서 그냥 뒤에는 버려도 되거든요

그러면 약간 귀찮기 때문에 그냥 한번 더 돌리는 걸로 그래서 100으로 했던 거를 13으로 다시 돌려주시면 그러면 우리가 이제 이렇게 하면 이게 무슨 얘기냐면 우리가 지금 굉장히 많은 다양한 특허들이 있는데 이 특허를 13개의 어떤 차원으로 우리가 다시 말하면 13개의 토픽으로 구별을 하겠다

이렇게 되니까 그 전에는 우리가 이제 단어들로 각각의 문서들을 구별을 했어요

이 문서에는 이 단어가 나왔고 안 나왔고 저 문서에는 이 단어가 나왔고 안 나왔고 근데 이제 우리는 그게 아니고 이 문서에는 이 토픽이 나왔고 안 나왔고 이렇게 보겠다

그러면 우리가 지금 DOCEMB만 있는데 Word EMB는 어떻게 하면 되냐면 SVD Components에 T, 이걸 전치시키면 단어 임베딩이 나오게 됩니다

그래서 이제 DTM 점 쉐입을 해 보시면 이게 문서란어 형태 행렬의 모양이죠

439행 362열인데 이거는 무슨 뜻이냐면 439개의 문서를 문서와 362개의 단어로 구성된 행렬이에요

그러면 DOCEMB의 쉐입을 보시면 그러면 DOCEMB의 쉐입을 보시면 DOCEMB의 쉐입을 보시면 439개의 문서를 13개의 토픽으로 표현을 한 거죠

그래서 439개의 문서를 13개의 토픽으로 표현을 한 거고 그 다음에 Word EMB의 쉐입을 보시면 이 362개의 단어를 또 13개의 토픽으로 표현을 하게 됩니다

그래서 우리가 369개의 문서에 대해서 다 일일이 얘기를 하는 게 아니라 이 단어는 어떻고 저 단어는 어떻고 이렇게 얘기하는 게 아니라 그냥 이 토픽은 어떻고 저 토픽은 어떻고 이런 식으로 얘기를 하는 거예요

지금 이제 특허를 가지고 얘기하는데 예를 들어 우리가 주식을 가지고 이런 식으로 분석을 한다

그럼 각 주식별로 설명이 있을 거 아니에요

예를 들어 테슬라면 전기차하고 친환경이고 일론 머스크고 이런 식으로 어떤 게 있는데 그러면 우리가 주식 같은 것도 토픽이라든가 테마 이런 말 쓰잖아요

예를 들어 친환경 주식 하면 전기차도 있겠지만 풍력 발전이나 이런 것도 있을 거고 그럼 그런 것들이 다 어떤 동일한 유사한 토픽들을 가질 거 아니에요

그럼 이제 이런 식으로 분석해서 우리가 투자 같은 걸 할 때도 써 먹을 수 있어요

같은 토픽인 게 뭐냐 그러면은 우리가 친환경 토픽을 가지고 있는 주식들에서 친환경 요즘에 트렌드니까 이런 주식들 이런 토픽이 있는 거에다가 투자를 하자던가 이런 식의 실제로 미국에서는 그렇게 하는 회사도 있습니다

그래서 테스트 분석을 해가지고 회사마다 어떤 설명은 다 테스트로 있으니까 분석을 해서 할 수도 있겠죠

그래서 그게 이제 돈이 잘 벌리지 않고 이제 또 별개의 문제인데 할 수는 있다

설명이 좀 길게 다 나오는데

네 단어를 몇 세계로 하시나요?

그러면은 이제 말이 좀 어렵죠

그럼 이제 우리가 WordEMB에 0번 이렇게 해서 보면은 이거 근데 이렇게 하고 안 쓰는 거래요?

네 아 문서...

단어 고맙습니다

그래서 이제 이거를 그냥 한번 보면은 WordEMB 0번 하면은 숫자가 13개가 나와요

뭐 플러스 마이너스 이렇게 암호 같은 숫자가 나오는데 이 숫자가 이제 뭐냐 하면은 0번 단어가 어떤 토픽이 얼마나 있는지를 나타내는 그런 숫자가 됩니다

그래서 우리가 이제 0번 단어 이러면 좀 재미가 없으니까 이제 우리가 본 이 데이터가 이제 샴푸 관련 데이터거든요

그래서 샴푸가 모발과 관련된 거니까 모발이라는 단어를 하나 뽑아가지고 모발이 몇 번째 단어인지 하면 이게 I에 들어가거든요

그래서 이 모발이라는 단어의 토픽학 그래서 0을 기준으로 이렇게 선을 그어보시면 플러스로 솟아 있는 부분도 있고 마이너스로 되어 있는 부분도 있는데 사실 여기서 플러스 마이너스는 별로 의미가 없습니다

왜냐하면 플러스 마이너스는 그냥 뒤집으면 되는 거라서 사실 이거는 그냥 개선 과정에서 만들어지는 거라 큰 의미가 없고 중요한 건 이제 0으로부터 얼마나 이제 벗어나 있느냐 이거 그러면은 이 모발이라는 단어는 지금 이걸 기준으로 가로로 쭉 그어보시면 6번 토픽이 굉장히 뭔가 세게 나오고 그죠

그 다음에 4번 토픽이 되게 세게 나오고 또 3번 토픽도 이제 세게 나온 방향은 다르지만 어쨌든 그 다음에 이제 7번 도 좀 강하게 나오는 그런 단어라고 할 수 있겠죠

그게 뭐 의미가 뭐냐 사실 아직까지는 별로 없습니다

그냥 뭔가 이렇게 뭐가 세게 나오는 부분이 있는 거에요

4번이 뭐고 6번이 뭐고 이런 거 우리가 나중에 해석을 해야 됩니다

일단은 그렇구나

그래서 만약에 우리가 다른 단어가 있는데 다른 단어가 이거랑 뭔가 비슷한 패턴을 보인다 그러면 그 단어는 뭔가 모발하고 의미적으로 유사하거나 이렇게 얘기를 할 수 있습니다

예를 들면 모발 대신에 두피 같은 걸로 해볼까요

그러면은 좀 패턴이 다르죠

그러면 아 모발하고 두피는 뭔가 의미가 좀 다르구나

이렇게 생각을 할 수 있습니다

그래서 이제 우리가 앞으로는 어떤 단어 두 단어가 비슷하다

아니다를 이제 이런 단어들이 가지는 토픽 패턴 가지고 이제 얘기를 할 수가 있게 되는 거죠

그래서 이제 그러면 그 패턴이 비슷하다라는 걸 할 때 코사인 유사도를 이제 사용을 하면 됩니다

코사인 유사도를 사용하면 됩니다

코사인 유사도를 가지고 우리가 이제 지난번에 했던 것처럼 한번 해보죠

그래서 모발하고 어떤 패턴이 비슷한 단어들이 뭐가 있느냐 하면은 손상 머리카락이 손상이 되니까 모발 손상 그다음에 염모제 이거 뭐 염색약 같은 건가 봐요

그다음에 염색 최소 트리트먼트 트리트먼트도 머리 하는 거죠

그다음에 모발이 회복이 되어야겠죠

모발 자극 모발 윤기 모발 클렌징 모발 도포 보면은 다 뭔가 모발이랑 관련된 단어들이 주르륵 나오게 됩니다

이 단어들이 우리가 샴푸 관련된 특허 내에서 분석이 된 거기 때문에 어떤 샴푸 특허라는 맥락 내에서는 모발이라는 단어는 윤기라든가 손상이라든가 염색이라든가 이런 거와 관련이 있는 단어다

이렇게 얘기를 할 수가 있겠죠

다른 것도 해볼 수가 있는데 예를 들면은 우리가 이제 추출 같은 걸로 해보죠

추출 그러면은 추출은 감초 발효 부작용 항산화 효능 숙성 염증 예방 추출은 이런 단어들하고 뭔가 관련이 있다

이렇게 볼 수 있고 추출물도 해볼까 추출물 하면은 잎, 판테놀, 녹차, 뿌리, 천공, 나무, 편백, 로즈마리, 제용, 임산물 이런 단어, 여기에서 추출물을 뽑으니까 아마 그렇겠죠

그래서 이제 이런 식으로 우리가 토픽 패턴이 비슷한 단어들이 이런 식으로 표현할 수가 있습니다

그래서 우리가 이제 한 단어씩 지금 해보고 있는데 그렇게 하지 말고 여러 단어를 한번 쭉 찾아보도록 할게요

모발, 손상, 부피, 모공, 용기, 내용물 6단어를 번호를 한꺼번에 쭈르륵 찾아보겠습니다

그러면은 이제 모발은 79번, 손상은 157번, 두피는 61번, 모공은 77번, 용기는 201번, 내용물은 45번 이런 식으로 이제 번호가 나오게 되는데 우리가 이거를 가지고 한 가지 이제 시각화를 해보려고 해요

어떻게 시각화를 할 거냐면 미리 보여드리면 의미가 유사한 단어들끼리는 이렇게 가깝게 잘 안 보이실 수도 있는데 키워보면 용기랑 내용물은 관련이 돼 있죠

왜냐하면 용기 안에 내용물이 들어가니까 여기서 이제 용기는 할 수 있어 이런 용기 말고 상추 용기를 받았는데요 별로 재미없나요?

그 다음에 이제 모공하고 두피는 당연히 두피에 모공이 들어가서 상추 용기를 받았고요 상추 용기는 상추 용기, 상추 용기, 상추 용기, 상추 용기, 상추 용기, 상추 용기 두피에 모공이 있으니까 관련이 있겠죠

그 다음에 이제 모발하고 손상하고 당연히 아까도 좀 관련이 있겠죠

그래서 이렇게 좀 관련된 것들끼리 이렇게 가깝게 시각화를 하고 싶어요

근데 문제가 뭐냐면 우리가 토픽이 지금 13차원이라서 시각화를 하려면 13차원을 그려야 되는데 13차원을 그리기는 어렵잖아요

우리가 보통 해봐야 한 3차원으로 그릴 수 있단 말이에요

그래서 이제 이럴 때 시각화 하는 방법 중에 다차원 척도법이라는 시각화 방법이 있습니다

이 다차원 척도법은 뭐냐면 우리가 이제 이렇게 13차원, 이렇게 다차원에 있는 거를 가까운 거는 가깝게, 먼 거는 멀게 이렇게 어거지로 이렇게 최대한 끼어 맞춰주는 그런 시각화 기법이에요

그래서 이제 이걸 이용을 해서 우리가 이제 13차원에 있는 애들을 최대한 2차원에다가 잘 배치를 시켜봐라 이렇게 좌표를 계산을 해줍니다

그래서 이 다차원 척도법 같은 거는 뭐 시각화 할 때 예를 들면 우리가 제품이 있어요

여러 개가 있는데 그런 걸 많이 하잖아요

회사 같은 데서 보면은 포지션이 이렇게 해가지고 뭐 이런 식으로 예를 들면은 스타벅스가 있고 투썸이 있고, 뭐 메가커피가 있고, 백다방이 있고 하면은 이렇게 축 두 개 그어가지고 여기는 뭐 여기 있고 여기는 여기 있고 하니까 아 이쪽이 비었네, 디저트 중심에 저렴한 가격이 비었으니까 우리는 이 시장을 공략하겠습니다

뭐 이런 거 이제 회사에서 전략 같은 거 수립할 때 많이 하는데 이거를 이제 배치를 어떻게 하면 사람 마음대로 하거든요

그냥 내 마음대로 이렇게 하는데 우리가 이제 다차원 척도법 같은 걸 쓰면은 저걸 데이터를 가지고 할 수 있어요

데이터에 보면 수치가 여러 개가 있잖아요

그러면은 뭐 스타벅스에 어떤 수치들이 여러 개 있고 그러면 뭐 폴바셋에 수치가 여러 개 있고 그러면 이제 그 수치가 여러 개가 있으니까 이걸 2차원에서 시각화하기 힘들단 말이에요

그럼 아까 이제 mds 같은 걸 쓰면은 어쨌든 비슷한 것끼리 2차원에 잘 배치를 해줍니다

그러면은 이제 여기 비어있는 여기가 비었으니까 우리는 여기를 공략하겠다

이런 식으로 할 때 데이터분석을 통해서 할 수도 있겠죠

제가 그렇게 하시는 경우를 실제로 본 적은 없어요

왜냐면 그냥 대충 어차피 이제 우리가 데이터분석하는 사람들의 애환인데 대국에는 이게 데이터분석을 하면 보통 크게 보면 두 가지로 나눌 수 있는데 하나는 뭔가 분석을 해가지고 이게 사람이 중간에 이제 끼어들지 않고 바로 뭔가 액션을 하는 경우가 있고 예를 들면 추천시스템 같은 경우가 그렇죠

추천시스템은 중간에 사람이 끼어들지 않고 그냥 고객한테 바로 뭔가 추천해 나가잖아요

그런 경우가 있고 아니면 의사결정권자한테 데이터를 보여주면서 분석해 보니까 이런 결과가 나와서 우리가 이런 거 해야 됩니다

이런 얘기를 하는 경우가 있는데 보통 이제 의사결정권자들이 통계에 사실 관심이 잘 없기 때문에 그냥 적당히 그려서 보여줘도 되는 경우가 많이 있습니다

그래서 사실 의사결정권자가 이런 거 여러분이 그려가셨을 때 이거 데이터 가지고 그린 거야

이런 거 안 물어보기 때문에 대충 그림이 그릴 듯하면 넘어가니까 사실 굳이 MDS까지 안 써도 되지만 쓸 수는 있다

어쨌든 MDS로 우리가 좌표를 계산하는데 이 좌표는 최대한 13차원 공간에서 가까이 있는 애들끼리 최대한 붙어서 나올 수 있게 어거지로 이제 어거지라기보다는 최대한 어떤 그런 2차원으로 시각화할 수 있게 좌표를 잡아주는 거예요

이 좌표를 그리면 되겠죠

그래서 이제 약간 짜잘한 게 들어가는데 Adjusted Text라고 해서 이거는 뭐 할 때 쓰는 거냐면 해도 그만, 아��도 그만이긴 한데 글자끼리 겹치면 보기가 싫으니까 지금은 사실 점이 6개밖에 없어서 글자끼리 겹칠 일이 없는데 점을 많이 찍으시면 글자들끼리 겹쳐가지고 보기가 되게 나쁘거든요

그래서 이 Adjusted Text를 깔아주면 글자들이 겹치지 않게 최대한 글자들을 서로 피해 주면 됩니다

이건 Text 분석이랑은 상관없는데 알아두시면...

그래서 이거 PIP 앞에는 느낌표 하나 붙여주시고요

그 다음에 이제 콜랩에서 한글 글꼬를 쓰려면 아, 이거 먼저 해줄 거 이제 글꼬를 설치를 해주시면 되는데 이게 이제 될 때도 있더라고요,

일단 한번 해봅시다 이걸 먼저 해줄 걸 이게 이제 콜랩 같은 경우에 윈도우에서도 마찬가지지만 그래프 같은 거 그릴 때 기본적으로 영문 글꼴로 돼 있어서 한글을 이렇게 그리면 한글이 다 깨지거든요

그래서 한글 글꼴을 그리는 걸 설정을 해주셔야 되는데 이 font 나눔 글꼴을 설치를 해서 이걸 기본 글꼴로 지정해주는 겁니다

근데 이게 이렇게 지정을 해줘도 안 될 때가 있거든요

일단 되는지 봅시다, 이게 됐다가 안 됐다가 해가지고 깨지네요

그래서 이렇게 글꼴이 깨지면 어떻게 하시는 거냐면 일단은 여러분은 하지 말고 그냥 넘어가세요

그냥 다시 여기 런타임메뉴에 보면 어...

세션 다시 시작이라고 있거든요

그래서 여기 메뉴에 보시면 여기 세션 다시 시작이라고 있는데 한번 껐다 켜주는 겁니다

이렇게 하면 이렇게 되죠

이렇게 되면 이렇게 되죠

이렇게 되면 이렇게 되죠

세션 다시 시작이라고 있는데 한번 껐다 켜주는 겁니다

세션 다시 시작하고 처음부터 다시 해야 되기 때문에 여러분들은 하지 마시라고 하는 거예요

벌써 하신 분들은 없죠

제가 하지 말라고 해도 꼭 하시는 분들이 있더라고요

다시 하면 됩니다

그냥 모두 실행하면 되겠죠

다시 설명드리면 사실은 그래서 글꼴 설정을 맨 먼저 하셔야 돼요

이거는 제가 슬라이드 순서를 헷갈려가지고 그래서 이렇게 해서 다시 설명드리면 다시 설명드리면 사실은 그래서 글꼴 설정을 맨 먼저 하셔야 돼요

제가 슬라이드 순서를 헷갈려가지고 그래서 글꼴 설정 먼저 해주시고 세션 다시 시작해서 한번 콜랩을 껐다 켜주시면 됩니다

그래서 다시 해보면 어?

여전히 깨지네

어?

나눔 고데기 설치가 안됐다고?

에헤이 뭐지

어?

어디 나눔 고데기 설자가 틀리네

나눔 고데기 왜 설치가 안됐지?

나눔 고데기 폰트는 설치가 됐고 아!

아!

나눔 고데기 설치가 됐고 나눔 고데기 폰트가 인식이 안되네

음..

이건 제가 확인해보고 다음주에 다시 한번 얘기 드리겠습니다

어쨌든 지금 글자가 깨져서 잘 안보이는데 한글 글꼴이 지금 깨져가지고 마음의 눈으로 보시면 됩니다

일단은 글꼴을 제가 해결해야죠

그래서 여기 이제 두개씩 붙어있죠

그래서 이제 아마도 이제 여기가 강의자로랑 비슷하게 나올거에요

이렇게 나올거에요

두개 두개 두개 이렇게 비슷하게 붙어서 나오게 됩니다

그래서 우리가 이제 단어 인베딩을 이용을 해서 시각화도 할 수 있다

근데 이제 문제가 뭐냐면 어..

우리 아까 앞에 그래프로 돌아가서 여기에 있는 이제 요 패턴에서 지금 예를 들면 모발이란 단어는 지금 6번 4번 3번 7번이 이제 되게 강한 그런 단어다

이런 얘기 했는데 그럼 도대체 뭐 3번 뭐 4번 6번 7번이 뭔데 하면은 이제 할 말이 없어요

왜냐면은 요거는 뭐 어떤 고정된 해석이 있는게 아니라 그냥 수학적으로 계산 하다보면 이렇게 튀어나오는 거거든요

그래서 얘들의 의미가 뭐냐 하면은 그 의미를 얘기하기가 되게 곤란합니다.

그럼 또 하는 문제는 모발이라는게 여기에도 걸려있고 저기에도 걸려있으면 해석하기가 되게 힘들어요.

그러니까 우리가 뭐 모발이란건 뭐냐 하면은 아 모발은 3번 토픽이랑 관련있지

이러면은 아 뭐 하여간 3번이 모발이랑 관련있구나

이렇게 얘기하기가 쉬운데 여기도 관련있고 저기도 관련있고 하면은 쟤는 뭐야?

약간 이렇게 되거든요

그래서 좀 해석하기가 까다롭습니다.

그래서 우리가 해석을 하기 좋게 하기 좋게 하기 위해서 회전이라는 걸 해줘요 회전은 뭐냐 하면 일단 이제 LSA가 가지는 특징 중에 하나가 얘가 어떤 행렬 하나를 우리가 이제 두개로 쪼개는데 문서 토픽 행렬이랑 토픽 단어 행렬로 쪼개는데 우리가 이제 곱셈 같은거를 생각을 해보면 곱셈 같은거를 생각을 해보면 예를 들어서 우리가 이제 12 같은거를 곱셈으로 나타낸다

그럼 뭐 예를 들면 3 곱하기 4 이렇게 나타낼 수 있겠죠

근데 잘 생각을 해보면 여기에다가 3 곱하기 4인데 이 사이에다가 2 곱하기 2분의 1을 끼어넣을 수가 있어요.

어차피 얘는 2 곱하기 2분의 1 하면 1이니까 끼어넣어도 어차피 차이가 없잖아요

그죠

우리가 곱셈에다가 1을 아무리 곱해도 곱셈 결과가 똑같죠

그러면 어떻게 됩니까

우리가 곱셈이라는거 뭘 먼저 계산해도 상관없잖아요.

이렇게 묶을 수가 있으니까 그러면 3 곱하기 하면 6이고 2분의 1 곱하기 4 하면 2하고 6 곱하기 2가 되는거죠

6 곱하기 2만 될까요

우리가 생각을 해보면 곱하기에서 1이 되는 조합은 무수히 많죠

지금 우리가 2 곱하기 2분의 1을 했는데 3 곱하기 3분의 1을 해도 될거고 4 곱하기 4분의 1을 해도 될거고 5 곱하기 5분의 1을 해도 될거고 이런게 무한히 많잖아요

그죠 행렬에도 마찬가지로 역행렬이라는게 있어서 어떤 행렬하고 역행렬하고 곱하면 우리가 산수에서 1처럼 항등행렬이라는게 생겨서 이 항등행렬은 아무 데나 곱해도 그냥 안한거랑 똑같습니다

그럼

이런게 또 무한히 많거든요

이런 조합이 그러면 이거 어디든 이런 무한히 많은 조합을 이 사이에다 끼어넣을 수가 있는데 그러면 이제 행렬의 곱셈에도 순서가 상관없어요

이렇게 먼저 곱해도 됩니다

그러면은 이거랑 이거랑 곱하면 뭔가 다른 행렬이 되겠죠

그래서 이 LSA는 해가 무수히 많아요

이 쪼갤 수 있는 조합이 무수히 많아요

우리가 12도 3 곱하기 4로 쪼갤 수도 있고 6 곱하기 2로 쪼갤 수도 있고 3.5 곱하기 얼마 이렇게 쪼갤 수도 있고 무수히 많은 해가 나오듯이 LSA에도 해가 무수히 많이 나오게 됩니다

그래서 이렇게 해가 무수히 많이 나오게 하는 이거를 회전이라고 부릅니다

회전이라고 부르는데 왜 회전이라고 부르냐면 이게 이제 공간적으로 어떤 물체를 회전시킬 때 이거를 행렬로 표현할 수 있는데 그거를 회전 행렬이라고 합니다

회전 행렬이랑 똑같은 특성을 가지고 있어서 회전 이렇게 불러요

그래서 우리가 달리 말하면 LSA의 결과는 회전을 시킬 수가 있는데 회전이라는 게 뭡니까

뺑글뺑글 돌리는 거잖아요

왜 회전 행렬이라고 부르냐면 이렇게 돌렸다가 반대로 돌리면 제자리잖아요

앞에 있는 R이 한쪽 방향으로 돌리는 거면 역행렬은 반대로 돌리는 겁니다 돌렸다가 반대로 돌리면 제자리니까 어차피 내가 이만큼 돌렸다가 이렇게 다시 돌아가도 어차피 제자리고 이만큼 돌렸다가 이만큼 다시 돌아가도 제자리고 회전이라는 건 무수히 많이 할 수 있는 거죠

그래서 이걸 회전이라고 부르는데 회전을 네던데로 시키는 게 회전을 네던데로 시킬 수가 있어요

회전을 시키면 어떻게 되냐면 일단 유사도는 달라지지 않습니다

왜냐하면 우리가 회전이라는 걸 생각해보면 예를 들어서 어떤 공간상에 이 점하고 이 점이 있는데 A하고 B하고 거리가 이만큼 떨어져 있겠죠

그럼 얘를 회전을 시켜요

예를 들면 90도로 돌렸습니다

A가 여기로 가고 B가 여기로 가겠죠

근데 거리는 똑같을 거 아니에요

내가 예를 들면 이렇게 한 바퀴 돈다고 해서 여러분들 간에 거리가 바뀌는 건 아니잖아요

제가 이쪽을 보고 있다가 이렇게 보면 여러분들 저를 기준으로 할 때 위치가 바뀌죠

이쪽에 있는 분들이 여러분이 제 왼쪽에 있다가 제가 이렇게 돌면은 이쪽에 있는 분들이 제 오른쪽으로 위치가 바뀌는데 여러분들끼리의 거리는 똑같잖아요

그냥 제가 보는 방향이 바뀐 거죠

이해되시죠?

유사도는 똑같습니다 유사도는 똑같은데 뭐가 달라지느냐 하면 해석이 달라져요

왜 해석이 달라지느냐 하면 여기 있는 분들이 다 제 왼쪽에 있으시잖아요

만약에 제가 제가 여기 있어야 하는데 여기에서 이렇게 여러분을 보면 여기 쪽에 있다는 거네요

제가 이렇게 돌면 어떻게 되니까 이쪽 분들은 제 윗쪽에 있고 이쪽 분들은 오른쪽에 있어요

예를 들면은 지금 실제로 볼 때가 있지만 남자분들도 이쪽이 앉아있고 여자분들도 이쪽이 앉아있죠

남자분들이나 여자분들은 한 day 왼쪽에 있는 거예요

남자분들은 제 왼쪽에 있고 여자분들은 제가 오른쪽에 있으니깐 해석하기가 쉬운데요 왼쪽에 있는 분들 하면 다 남자분들이 되는 거고 오른쪽에 있는 분들 하면 다 여자분들이에요

제가 획득을 이렇게 하나 드리기 그러니까 지금 다 오른쪽에 있으니까 다 똑같은 거거든요

그럼

제가 획득을 여기서 이렇게 할 수도 있고 이렇게 할 수도 있는데 어떻게 해석을 하느냐에 따라서 해석하기가 쉬울 수도 있고 그래서 무슨 많은 회전 중에 내가 해석하기 좋은 회전을 하나 고를 수가 있는 거예요

예를 들어서 그러면 유사도는 변하지 않으면서도 내 마음대로 하기 좋은 해석을 할 수가 있어요

그러니까 우리가 어떤 세상을 볼 때 그 세상을 보는 우리가 관점을 바꾼다고 해서 세상이 실제로 바뀌는 건 아니에요

하지만은 어떤 식으로 보면 세상이 좀 더 이해하기 쉽고 뭔가 아 이렇게 보니까 좀 세상이 맑게 잘 보이는데 어떻게 보면 세상 너무 복잡하고 약간 일대가 있잖아요

유사도를 그대로 놔두면서도 내가 하기 좋은 형태로 해석을 바꿀 수 있기 때문에 이게 해가 무수히 많다는 게 어떤 단점이랄 보다는 장점이 될 수도 있습니다

그래서 이제 그런 게 회전이고 그래서 우리가 이렇게 밑에 보시면 회전을 똑같은 거 내에서도 이렇게 이렇게 세 가지로 다르게 할 수 있는데 예를 들면은 우리가 이제 A하고 B하고 이렇게 있다

그러면 이제 초록색 입장에서 봤을 때는 A는 이쪽에 있고 B는 이쪽에 있는데 주황색 입장에서 봤을 때는 A는 이쪽에 있고 B는 이쪽에 있고 뭔가

이게 이제 보는 관점은 아니지만 얘네끼리의 유사도 자체는 계속 똑같은 거죠

어떤 관점으로 볼 거냐

그게 이제 달라진다

그런 얘기고 그래서 이제 회전 방법이 여러 가지가 있는데 우리가 근데 이제 해석하기 좋은 거를 무한히 많은 거를 일일이 돌려가면서 찾을 수는 없잖아요

그죠?

그래서 그 회전 방법 중에 우리가 이제 좀 해석하기 좋은 거를 바로 찾아줄 수 있는 또 알고리즘들이 여러 가지가 있습니다

그중에 대표적으로 많이 쓰는 게 베리막스라는 회전 방법이 있어요

베리막스는 분산을 최대한다고 뭐 이런 말이 있는데 결국 얘가 하는 건 뭐냐면 우리가 이제 해석하기 쉬우려면 그 토픽 하나가 특정 단어로 쫙 몰려야 해석하기 쉬워요

반대로 말하면 특정 단어는 하나의 토픽으로 딱 몰려서 있어야 해석하기가 쉽습니다

예를 들면은 우리가 삼성전자는 어떤 회사입니까?

라고 물었는데 삼성전자는 AI도 하고요, 반도체도 하고 핸드폰도 만들고요, 그걸 뭐라는 거야?

도대체 그 회사는 뭐 하는 회사야?

머리가 복잡하잖아요

삼성전자는 전자회사인가요?

간단하잖아요, 그죠?

이렇게 볼 수도 있고 저렇게 볼 수도 있는데 최대한 하나로 딱 몰아줘야 얘기가 간단해진단 말이에요

그래서 베리막스는 회전을 시키는데 최대한 하나로 몰아주는 특성을 가진 알고리즘입니다.

요 알고리즘을 쓰면은 다른 알고리즘도 있지만 제일 많이 쓰는 이유는 요걸 하면 보통 간단해지기 때문에 실제로 간단해진지 한번 봅시다

여기 PIP 앞에는 꼭 느낌표를 붙여주세요

회전 자체는 간단해요 특별하게 설정을 안해줬는데 설정을 안하면 베리막스가 기본 알고리즘이라서 기본적으로 베리막스로 회전이 됩니다

그래서 요거는 단어 토픽 행렬 Word EMB를 베리막스 알고리즘으로 회전을 시키는 거예요

그리고 이 회전은 단어 토픽 행렬 Word EMB를 베리막스 알고리즘으로 회전을 시키는 거예요

그래서 회전을 하면은 어떻게 되냐면 지금 보시면은 나머지 아까는 지금 이게 똑같이 모발이라는 단어거든요

똑같이 모발이라는 단어인데 아까는 보면은 모발이라는 단어가 0에서 많이 벗어나 있으면 그게 이제 토픽이 센 거라고 얘기를 드렸어요

여기도 세고 여기도 세고 여기가 더 세고 여기가 더 세고 여기가 더 세고 여기가 더 세고 여기도 세고 여기도 세고 여기도 세고 여기도 세고 삼성전자인데 삼성전자는 반도체도 하고 뭐 이거 이거 저거 막 이렇게 얘기가 복잡했는데 이제 어떻게 되었냐면 회전을 시키고 나니까 그냥 7번 토픽만 가하고 나머지는 다 0 근처로 싹 정리가 됐어요

뭐냐면 얘가 원래 이쪽 토픽도 있고 저쪽 토픽도 있는데 그냥 다 7번으로 몰아준 거예요

그래서 쉽게 생각하시면 이제 모발이랑 관련된 다른 단어도 찍어보면 어떻게 되냐면 다 7번 토픽으로만 나옵니다

7번 토픽에는 모발하고 또 관련된 손상 염색 이런 단어들이 있잖아요

4번에도 있고 6번에도 있고 막 흩어져 있었는데 그냥 다 7번으로 몰아주는 거예요

그래서 모발을 보면은 7번만 이렇게 딱 나오고 이렇게 해야 됩니다.

그러면은 우리가 이제 7번 토픽이 뭐냐 해석하고 싶죠

머리카락이랑 관련된 샴푸 특허 중에 머리카락의 손상이나 윤기나 염색이나 이런 거에 관련된 거는 다 7번 토픽이에요 바꿔 말하면 어떤 특허를 봤는데 7번 토픽이 세게 나오는 특허가 있어요

아 그럼 이건 머리카락 관련된 거구나

특허라는 게 샴푸가 머리카락에 작용하든지 두피에 작용하든지 이런 식으로 종류가 나뉠 거 아니에요

우리가 그냥 딱 그 토픽 번호만 보면은 바로 해석을 할 수 있게 됩니다

그래서 우리가 이제 보통 이렇게 로테이션을 시켜가지고 회전을 시켜서 해석하기가 좋게 만들어주게 됩니다

그래서 이제 반대로 할 수도 있는데 이거를 그럼 이게 이제 마이너스로 되어 있어서 이 코드로 하면 안 되고 그냥 손으로 해줄게요

T는 7 이렇게 해가지고 아 이게 지금 마이너스지

그러면은 이렇게 해가지고 마이너스로 되야 되니까 0부터 10까지 이렇게 응?

잠깐만요

이렇게 해가지고 마이너스는 이렇게 이렇게 이렇게 이렇게 이렇게 이렇게 이렇게 이렇게 이렇게 이렇게 Τ 취 응?

이럴 수 있겠다 이럴 수 있네

헐 정말achel justcer 그 ㅣ overuğRY raj 으 으 으 I 으 00 2340ool 으 으 으 으 응 으 으 이상하네

뭐 네 이거는 지금 제가 생각한 대로 잘 안나오는데 일단은 요거는 됐고 그러면은 아 으 일단 지금 이제 그 모발이 지금 7번이 이렇게 세게 나오죠

그러면은 아까 모발하고 관련이 있었던 단어 중에 두피가 관련 모발 손상이 관련 있잖아요

157번 그러면은 우리가 이제 손상도 한번 플러스를 그려보면 아니 그렇게 손상은 또 6번에서 세게 나오지 모발 손상 말고 아까 모발 음 모발 염색 염색이 191번 이것도 6번이네

왜 다 6번에서 나오지

아 이상하네

어쨌든 잠깐만요 6번이랑 관련된 단어를 봅시다

아 네 어쨌든 음 약간 제가 생각하고 좀 다르게 나오는데 어쨌든 어 지금 보시면은 이제 6번 그냥 원래 강의자로 대로 해야 되나

네 네 아닌데 일단 이제 6번 토픽에서 6번 토픽 값이 크게 나오는 단어들을 이제 쭉 보면은 어 지금 이제 모발 두피 도포 단계 손상 염색 세척 뭐 이런 단어들이 이제 6번 토픽이랑 쭉 관련돼서 이렇게 나오는 거를 볼 수 있지

그래서 음 이렇게 이제 회전을 시키면 특정 토픽의 해석이 좀 쉬워집니다

그래서 지금 6번 토픽은 이제 이런 걸 봤는데 뭐 예를 들어서 지금 좀 약간 결과 이상한데 7번 토픽을 보면은 7번 토픽은 이제 혼합 뭐 호스 회전 샤워 천연 기능 등받이 그러니까 보시면 뭐냐면은 뭔가 샤워기랑 관련된 단어들이 나오는 걸 볼 수가 있어요

다른 단어도 보면은 5번 토픽을 보면은 5번 토픽은 뭐 추출물 단계 사용자 탈모 뭐 두피나 탈모 이런 거와 관련된 뭔가 그런 토픽들이 이제 나오는 거를 볼 수 있어요

그래서 이제 이런 식으로 우리가 이제 회전을 시키면 좀 특정 토픽에 특정 단어들이 이렇게 딱 몰려가지고 해석하기가 좀 쉬워진다

이렇게 이제 되구요

근데 그래도 딱 뭐가 깔끔하게 좀 떨어지진 않거든요

보시면은 알겠지만 두 가지 문제가 있는데 하나는 숫자가 지금 보시면은 7번 토픽 같은 경우도 아까 보시면은 이제 이렇게 마이너스로 나오니까 어떤 거는 마이너스로 나오고 어떤 거 플러스로 나오는데 사실 이게 마이너스라든가 플러스라든가 이런 방향성이 특별하게 의미가 있는 게 아니라서 좀 헷갈리고 그 다음에 결과가 그렇게 썩 깔끔하게 나오지를 않습니다

보시면 이런 데도 딱 0이 아니라 자글자글 하거든요

그래서 이제 이런 문제 때문에 LSA도 많이 쓰는데 결과 해석이 좀 애매하다

이런 문제가 있어서 우리가 이제 이렇게 해서 이런 문제가 있어서 우리가 이제 여기까지가 지난주 강의 자료고 이번 주 강의 자료의 내용으로 넘어오게 됩니다

그래서 이제 NMF라는 기법이 LSA 다음에 후속으로 나오게 돼요

그래서 이 NMF라는 기법은 회전이 일단 필요가 없습니다

무조건 해석이 좀 간편한 형태로 딱 나오게 되거든요

그래서 이 NMF라는 기법을 잠깐 쉬었다가 알아보도록 하겠습니다

이거는 이제 결과가 좀 깔끔하게 잘 나올 거예요

그래서 일단은 잠깐 쉬었다가 하고요 지금 10시 22분이니까 20분 쉬고 10시 42분에 다시 시작하도록 하겠습니다

빨리 말씀해 주시고요

녹화를 보실 분들을 위해서 빠르게 다시 설명을 드리면 어디부터 해야 되니 그래서 이제 NMF는 비음수 행렬 분해다 분해를 하는데 LSA랑 다르게 음수가 안 되게 0이나 플러스가 되게 하는 거고 그래서 분해를 하면 이렇게 깔끔하게 나오는 특성이 있습니다

기본적으로 LSA는 SVD를 이용하면 플러스 마이너스로 튀는데 그런 게 없고 깔끔하게 플러스로만 나오고 그 다음에 0 근처에서도 거의 대부분 0에 납작하게 붙어 있어요

그래서 이렇게 짜잘한 그런 것들이 안 생기기 때문에 해석이 훨씬 쉽게 됩니다 해석이 훨씬 쉽고 그 외에는 NMF랑 거의 똑같아요

유사도 높은 단어보기 이런 코드도 SVD랑 비슷하고 토피별 단어보기 이런 것도 비슷하고 그래서 우리가 지금까지는 다 단어보기 같은 거를 다 했고 문서별로도 볼 수가 있습니다

문서별로도 보면 우리가 0번 문서, 예를 들어 개운 죽 잎 추출물을 이용해서 모발의 윤기와 부드럼의 지속력을 향상시키는 컨디셔닝 샴푸 이런 특허가 있는데 이 특허의 토픽을 뽑아보면 이렇게 패턴이 나온다

이거죠

4번 6번 6번 6번 7번 8번 9번 10번 11번 12번 12번 12번 12번 12번 12번 12번 12번 12번 12번 12번 12번 12번 이렇게 패턴이 나온다

이거죠

4번 6번 8번 13번 이런 데서 이제 세게 나오는데 그러면은 지금 7번 4번 6번 7번이죠

4번 6번 7번을 보면은 7번이 추출물, 천연물 이런 거고 6번이 모발 두피 이런 거고 그 다음에 13번이 성분 컨디셔닝 이런 거 그래서 이 특허는 이런 것들이 어떤 핵심 내용이 되는 이런 특허다

이런 거를 우리가 볼 수 있어요

그래서 이거랑 패턴이 비슷한 거를 또 코스아임 유사도로 찾을 수 있겠죠

그래서 이렇게 찾으면 유사한 특허들을 찾을 수 있다

그래서 가끔 보면은 추천 시스템 같은 경우에 결국 추천을 할 때도 상당히 많은 경우가 어떤 제품의 설명을 가지고 추천을 하거든요

내가 산 제품이랑 설명이 비슷한 거를 추천을 해주게 되는데 가끔 보면은 예를 들어서 내가 이번에 여름에 베트남을 갔다 왔어요

그랬더니 베트남 잘 놀러 왔다 이랬더니 가을부터 계속 베트남 여행 가세요

광고가 오는 경우가 있습니다

이런 건 추천 시스템을 잘못 만든 거죠

왜냐하면 내가 베트남을 맨날 가진 않을 거 아니에요

한번 베트남 갔으면 그 다음에는 뭐 필리핀을 가든지 일본을 가든지 아니면 제주도를 가든지 한번 해외여행 갔다 왔으면 그러니까 내가 이 사람은 해외여행을 좋아하는 사람이구나

그럼 결은 갔더라도 좀 다른 내용으로 추천을 해줘야 될 거 아니에요

근데 이제 추천 시스템을 잘못 만들면 내가 베트남 갔다 오면 추가나 베트남만 추천해주고 내가 이번에 책상을 샀어요

그럼 계속 책상 사라고 광고를 하는 경우가 있는데 책상 이미 샀는데 뭘 또 책상을 사래 책상을 샀으면 그 다음에는 방향은 비슷하면서도 좀 다른 걸 추천해줘야 돼요

의자를 사라든가 책상 사셨어요

그럼 이번에 의자 어떠세요

아니면 뭐 책상 바꾸셨으니까 이번에 좀 등도 한번 바꿔보시죠

이러든가 이렇게 방향은 좀 비슷하면서도 약간 다른 걸 추천해줘야지 계속 똑같은 그냥 정말 글자 그대로 똑같은 것만 추천해주면 사실 좀 의미가 없단 말이죠

여러분들도 그런 거 가끔 하시다 보면은 그런 거 좀 느낄 때가 있잖아요

그러니까 이런 거를 추천 시스템 만들 때도 응용을 할 수가 있습니다

우리가 결이 좀 비슷하면서도 좀 세부적인 내용이 다른 거를 찾으려면 이런 식으로 모델링을 해가지고 하실 수가 있겠죠

질문 있으시면 또 질문해주세요

다음 주에 퀴즈를 보면은 뭘 볼까요

lsa 랑 nf의 차이를 얘기해 보세요

그 다음에 이제 우리 이번 거는 nf가 또 더 잘 나오는데 무조건 nf가 더 좋냐 하면은 꼭 그렇지는 않습니다

왜냐하면 기본적으로 bg도 학습 이런 bg도 학습은 정답이 없는 습이기 때문에 어떨 때는 좋든 나쁘든 우리가 이제 결과를 보고 주관적으로 판단할 때도 있고 아니면은 이제 예를 들어서 추천 시스템을 만든다

그럼 추천 시스템은 결국 뭘로 평가를 됩니까

고객들이 좋아하고 많이 그걸로 구매를 하면 그게 좋은 거죠

정답이 있는 건 아닌데 우리가 nf로 추천 시스템을 만들 수도 있고 svd로 추천 시스템을 만들 수도 있는데 둘러 돌려 봤더니 고객들이 svd로 뭔가 추천한 거를 내가 볼 땐 별로인데 고객들이 그걸 더 좋아해요

그럼 그게 맞는 거죠

그래서 무조건 nf가 좋다기보다 nf가 장점이 확실히 더 나중에 나온 방법이라서 더 해석하기 좋다든가 이런 장점은 있는데 해석을 기준으로 하면 이제 그렇다는 거죠

근데 이제 뭐 우리가 꼭 lsa나 nf의 목적이 해석에만 있는 건 아니기 때문에 다른 거에서는 또 장점이 있으면 svd를 또 쓸 수도 있고 기본적으로는 거의 똑같은 방법이니까 둘 다 알아두시면 되겠죠

그래서 둘 다 해보시고 더 결과적으로 더 좋은 걸 쓰시면 되요

질문 없으신가요

네 그 다음에 우리가 이제 지금까지는 유사도가 높은 거를 내가 0번 특허랑 유사도가 높은 거 이런 걸 이제 찾아봤는데 그거를 하나씩 이렇게 찾는 게 아니라 매번 0번이랑 1번이랑 2번이랑 비슷한 거 이렇게 하는 게 아니라 미리 이렇게 좀 묶어줄 수가 있겠죠

그런 거를 클러스터링이라고 합니다

클러스터링은 클러스터라는 게 뭔가 이렇게 모여있는 거를 클러스터라고 합니다

그래서 가끔 보면은 뭐 산업 클러스터 이런 말을 쓰는 경우가 있거든요

산업 클러스터는 뭐냐면 쉽게 말하면 산업 단지 이런 거예요

옛날에는 단지 이랬는데 단지 이러면 왠지 좀 요즘에는 영어를 안 쓰면 사람들이 멋이 없다고 생각하니까 아파트 이름도 계속 영어를 넣잖아요

그래서 킹갓 제너럴 이런 식으로 영어 단어를 넣는데 결국 우리가 흔히 말하는 거는 단지하고 거의 비슷한 뜻이죠

클러스터의 단지가 뭡니까

아파트가 모여있으면 아파트 단지고 산업시설이 모여있으면 산업 단지듯이 산업 클러스터 하면은 산업 뭔가 시설이 모여있는 거예요

그리고 아파트 클러스터라는 말은 잘 안 쓰네요

나중에 쓸지도 사람들 영어 단어를 쓰다 쓰다 이제 포레 이런 거 쓰다 질리면 클러스터 이런 거 쓸지도 혹시 그런 시대가 오면은 제 생각을 한번 해보세요

어쨌든 그 그래서 이렇게 뭔가 모여있는 거를 클러스터 우리 말로는 군집이라고 하는데 이것도 이제 비지도학습의 일종입니다

우리가 뭘 이렇게 모아놓는 것도 딱히 정답이 있는 건 아니잖아요

이렇게 모일 수도 있고 저렇게 모일 수도 있는데 여러 가지 모으는 방법이 있을 수 있습니다

그래서 클러스터도 알고리즘이 여러 가지가 있습니다

이런 알고리즘 저런 알고리즘이 있는데 크게 나누면은 두 종류로 나눌 수가 있습니다

그래서 이제 파티션 알고리즘은 데이터가 이렇게 있어요

그러면은 이거를 이렇게 선을 그어가지고 경계를 그어가지고 이쪽에는 빨간 애들 이쪽에는 초록 애들 이쪽에는 파란 애들 이렇게 나누는 겁니다

사실 이제 이런 거는 선을 이렇게 그어야 될까

생각을 해보면 뭐 좀 약간 이런 데다 그어도 될 것 같아요

이제 이렇게 나눠도 되지 않나?

여기가 좀 애매하잖아요

뭐 어쨌든 그거는 이제 알고리즘이 따라 달라지는데 이제 이런 식으로 수평적으로 나누는 거를 파티션 알고리즘이라고 하고 그다음에 이제 다단계식으로 나누는 거를 위계적 알고리즘이라고 합니다

그러니까 처음에 예를 들면은 데이터가 A, B, C, D, E, F 이렇게 있다 그러면 B하고 C가 좀 비슷하니까 얘네를 하나로 묶어주고 D하고 E가 비슷하니까 얘네를 또 하나로 묶어줘 그다음에 D, E하고는 보니까 F가 좀 비슷해요

D, E, F를 하나로 묶어줘 그다음에 B, C랑 D, E, F가 또 비슷해요

그럼 B, C, D, E, F를 또 하나로 묶어줘

이렇게 다단계로 비슷한 것끼리 모으고 또 모으고 또 모으고 또 이렇게 단계를 나눠서 하는 거를 위계적 알고리즘이라고 합니다

보통은 이제 위계적 알고리즘보다는 파티션 알고리즘을 더 많이 사용합니다

그런데 이제 위계적 알고리즘도 예를 들어서 이렇게 자르면 어떻게 됩니까?

A가 한 묶음, B, C가 한 묶음, D, F가 한 묶음 이렇게 되겠죠

그래서 위계적 알고리즘도 어느 단계에서 스탑을 하면 파티션 알고리즘하고 똑같이 되긴 해요

그래서 이제 우리가 이렇게 클러스터로 묶는데 이번 시간에 알아볼 거는 K-Mins 라는 방법을 알아보도록 하겠습니다

K-Mins는 클러스터링에서 가장 대표적인 알고리즘입니다

그래서 보통 클러스터링 하면 특별하게 뾰족한 게 없으면 일단 K-Mins를 많이 해봐요

K-Mins는 K는 코리아의 K가 아니고 K-게 이런 뜻입니다

보통 통계에서 몇 개 이렇게 할 때 우리가 이제 수학 기호들이 있는데 우리가 잘 모르면 보통 X라고 하잖아요

마찬가지로 그냥 몇 개라고 하고 싶으면 보통 K를 많이 씁니다

어디서 나왔는지는 잘 모르겠어요

보통 이제 K를 많이 쓰는 것 같아요

어디서 나왔는지는 잘 모르겠어요

보통 이제 기호를 몇 개 해보면 I하고 J는 보통 행은 I라고 하고 예를 들면 세 번째 행이다 그러면 I는 3 이런 식으로 얘기를 하고 J는 보통 10을 가르치는 때 많이 쓰고요 K는 몇 개 할 때 쓰고 N도 몇 개를 할 때 쓰는데 보통 N은 전체 개수를 할 때는 N을 많이 쓰고 그 중에 몇 개 할 때는 K를 많이 쓰고 그래요

그러니까 데이터가 몇 개냐 하면 그냥 N이라고 많이 하는데 데이터를 내가 세 가지 그룹으로 나누고 세 개의 그룹으로 나누고 싶다

보통 이제 K를 써서 K는 3 이런 식으로 하는 거예요

이거는 뭐 법으로 정해진 건 아니에요

그냥 관습적으로 그렇게 많이 합니다

기호를 쓸 때 우리가 방정식 풀 때도 꼭 미지수를 X라고 표기할 방법은 이유는 없는데 내가 그걸 X라고 해도 Z라고 하든 내 맘이잖아요

근데 그냥 보통 X라고 하듯이 K도 보통 몇 개 이렇게 할 때 K입니다

그래서 가끔 그걸 헷갈리시는 분들이 있는데 KNN이라는 머신 러닝 알고리즘이 또 하나 있거든요

이거는 지도학습에서 쓰는 건데 얘네 둘은 무슨 관계가 있을까요?

아무 관계도 없습니다

앞에 K가 붙으니까 K 뭐 하면 다 우리나라 정부에서 무조건 앞에 K 붙이잖아요

근데 K팝 이런 것처럼 이거 아무 상관 없습니다

그냥 K민즈는 평균을 K게 구한다고 해서 K민즈고 KNN은 NN이 제일 비슷한 거 이런 거거든요

비슷한 거를 K게 찾는다고 해서 KNN이 그냥 두 개는 서로 아무 관련도 없는데 그냥 몇 개를 하기 때문에 그냥 K가 붙은 거예요

아무 관련이 없습니다

근데 이거 가끔 관련이 있다고 생각할 수 있는 분들이 있더라고요

근데 그냥 둘 다 그냥 몇 개를 가지고 뭘 하기 때문에 그냥 그 정도의 그래서 그냥 K가 들어가는 거지

얘네 자체는 아무 관련도 없는데 또 이렇게 하면은 수업을 듣다가 잠깐 다른 생각할 때 KNN 얘기가 잠깐 나왔다고 하시는데 그러면 시간 지나면 헷갈리거든요

그래서 가끔 이 생각을 해요

이 얘기를 꺼내지 마

왜냐하면 기존에 헷갈리셨던 분들은 아 저게 둘이 관련이 없구나 알게 되는데 괜히 안 헷갈리셨던 분들이 이거 들으면 점심 먹다가 그래서 관련이 있다고?

없다고?

있었나?

없었나?

또 헷갈려요

그래서 관련이 없습니다

그래서 K는 사실 굳이 K라고 쓸 필요도 사실 없는 거예요

그냥 몇 개의 평균들 이런 얘기입니다

그래서 이거는 무슨 얘기냐면 우리가 군집을 만드는데 군집의 평균점을 찾아요

그러니까 우리가 군집을 여러 가지로 만들 수가 있는데 군집의 어떤 바운더리를 이렇게 구일 수도 있고 아니면은 바운더리가 아니라 중심점, 평균점을 찾아서 이 평균에서 가까우면은 그거를 하나의 클러스터로 잡을 수가 있습니다

그러면은 K민즈는 그 중심을 평균으로 잡아가지고 이 중심에 가까운 것들을 하나의 클러스터로 묶는 방법이죠

그러면은 어떤 특성이 생기냐면 예를 들면은 바운더리를 가지고 클러스터를 만들면 우리가 바운더리를 예를 들면 이런 식으로 이렇게 긁을 수도 있거든요

근데 K민즈는 이런 바운더리가 안 만들어집니다

왜냐하면 항상 중심을 기준으로 중심에서 가까운 것들을 모으니까 기본적으로 경계가 동그랗게 만들어지겠죠

왜냐하면 예를 들면은 평균이 여기다 그러면은 이렇게 움푹 들어갈 수가 없잖아요

왜냐하면 여기도 얘한테 가까우니까 그래서 기본적으로는 뭐 이렇게 동그랗게 좀 만들어지거나 아니면 이제 두 개가 이렇게 붙어 있으면 여기는 이렇게 직선이 되고 이렇게 이렇게 이런 식으로 만들어지게 돼 기본적으로 이렇게 좀 볼록한 형태를 가지게 돼

그래서 만약에 나는 클러스터가 그렇게 볼록하게 되는 게 싫다 그러면은 K민즈를 쓰시면 안 되겠죠

보통은 약간 그런 거에 딱히 좋고 싫고 할 이유가 별로 없거든요

특별히 선호가 있거나 이런 경우는 아닌데 그래서 일단은 보통은 제일 많이 쓰는 방법이기도 합니다

예를 들면 난 볼록한 게 싫어

이럴 일은 잘 없잖아요

여러분 그죠 좋고 싫고 자체가 없죠

그냥 볼록하면 뭐 어떤데요

특별한 건 없는데 그냥 볼록한 거죠

그래서 이게 어떤 식으로 돌아가는지를 보면은 좀 웃기게 돌아가는데 예를 들어서 이런 데이터가 있어요 초록색 점으로 그린 데이터가 있는데 여러분 눈으로 보시기 어떻습니까

이거 두 개로 나눠라고 하면 그냥 이 뒤에 거 보지 마시고 그냥 보면 어때요

대충 한 요거 한 덩어리 요거 한 덩어리 이렇게 나누면 될 것 같죠

아닌가요

나는 저는 이렇게 한 덩어리 하고 이렇게 한 덩어리가 맞는 것 같습니다

이런 분 있나요

뭐 그렇게 나눠도 됩니다

이건 비지도학습이기 때문에 클러스터링은 답이 없어요

여러분 생각에는 나는 이게 너무 맞는 것 같은데 이러면 그건 여러분 마음이에요

비지도학습은 정답이 없습니다

그러나 보통은 대부분의 사람들은 이렇게 한 덩어리 이렇게 한 덩어리 나누고 싶죠

볼록하다는 게 이런 거예요

이렇게 하면 볼록 이렇게 되잖아요

근데 만약에 아까처럼 이렇게 하면 이거는 볼록하지만 얘는 볼록하지가 않잖아요

삐뚤 뭐 약간 이런 거죠

삐뚤이라고 하면 좀 이런 데가 오목하게 이렇게 패 있는데 보통 그런 걸 별로 좋아하지 않는단 말이에요

이렇게 한 덩어리 이렇게 한 덩어리 우리가 뭐 묶어보세요 이렇게 하면 동그라미 쳐서 묶지

이렇게 안 한단 말이에요

이건 녹화가 안 되겠네요

그래서 이제 K-Mins로 하면은 실제로 제일 오른쪽 끝이 K-Mins로 나눈 건데 이렇게 한 덩어리 이렇게 한 덩어리 이렇게 나눠집니다

그래서 어떻게 하냐면 일단 랜덤하게 중심점을 잡아요 두 개를 우리가 두 개로 나누고 싶어요

중심점을 두 개로 잡는데 그럼 여기에 하나가 잡히고 여기 하나 잡히겠죠

그러면은 이 데이터들을 빨간색 X에 가까운 거는 빨간색으로 파란색 X에 가까운 거는 파란색으로 구별을 합니다

그러면은 여기 있는 애들은 이쪽에 가깝기 때문에 빨간색으로 색칠이 되고 여기에 있는 애들은 파란색에 가깝기 때문에 파란색으로 색칠이 됩니다

경계선이 이렇게 경계선이 이렇게 그어지게 됩니다

어?

여기 왜 이래?

그 다음에 그러면 이제 여기 파란색 애들을 가지고 평균을 내면 평균이 여기에 찍혀요

그 다음에 빨간색 애들을 가지고 평균을 내면 새로운 평균이 여기에 찍히게 됩니다

평균은 그냥 다 더해서 N으로 나누면 되니까 그럼 어떻게 되냐면 중심점이 바뀌었죠

아까는 중심점이 여기 있었는데 여기 있던 중심점이 여기로 이동하고 여기 있던 중심점은 여기로 이동을 하죠

그럼 이제 바뀐 중심을 가지고 새로 다시 칠을 해보는 거예요

그럼 여기에 가까운 애들은 빨간색으로 칠하고 여기에 가까운 애들은 파란색으로 칠하고 그러면은 색칠이 바뀌었죠?

아까는 이렇게 색칠이 됐는데 색칠이 이렇게 바뀌면서 경계선도 이렇게 다시 부어지게 됩니다

그러면 이제 색칠이 바뀌었으니까 평균을 다시 내보는 거예요

그러면은 또 여기 평균 내고 여기 평균 내고 이걸 계속 반복하면은 언젠가는 이제 더 이상 평균이 변하지 않는 순간이 오게 됩니다

반드시 그렇게 돼있어요 반드시 오게 돼있고 그러면은 우리가 이제 더 이상 바뀌지 않으면 이렇게 한 덩어리 이렇게 한 덩어리 이제 딱 깔끔하게 나누고 됩니다

그래서 이거는 우리가 이제 K-Mins는 우리가 이제 시작점을 어디다 잡느냐에 따라서 결과가 좀 달라질 수 있습니다

시작점 처음에 잡을 때 랜덤하게 잡는데 만약에 이제 빨간색 X를 처음에 이쪽에 긋고 파란색 X를 이쪽에 긋었다

그럼

최종 결과가 어떻게 됐습니까?

얘네가 파란색이 되고 얘네가 빨간색이 됐겠죠

그래서 뭐가 빨간색이 되고 뭐가 파란색이 되는지는 처음에 랜덤하게 중심점을 어디 잡느냐에 따라서 다를 수 있다

그러니까 여기서 지금 얘네를 빨간색, 얘네를 파란색으로 하는 거는 그냥 아무 의미가 없습니다

그 색깔은 바뀔 수가 있어요

컴퓨터에서 돌리면 번호로 붙는데 1번 2번 3번 4번 이렇게 번호로 붙는데 번호는 아무 의미가 없습니다

얘가 1번이어도 되고 얘가 2번이어도 되고 그냥 번호는 바뀔 수가 있어요

대체로 이런 결과는 비슷한데 만약에 데이터가 뭐 이런 식으로 있고 이런 식으로 있고 이렇게 3덩어리로 있었다

근데 이거를 우리가 두 덩어리로 쪼개려고 해요

그러면 처음에 중심점을 여기 하나 잡고 여기 하나 잡고 이렇게 하면 어떻게 되겠습니까?

경계선이 이렇게 그어지겠죠?

얘네가 한 덩어리로 묶이고 얘네가 한 덩어리입니다

근데 만약에 중심점을 여기 하나 여기 하나 이렇게 처음에 랜덤하게 이렇게 그어졌다 경계선이 이렇게 그어지겠죠?

그러니까 경계선이 이렇게 그어지면 얘네 둘이 한 덩어리고 경계선이 이렇게 그어지면 얘네 둘이 한 덩어리인데

그러니까 이거는 랜덤하게 결과 자체가 달라질 수가 있어요

어떻게 묶이느냐 자체가 랜덤하게 달라질 수가 있어요

그래서 그럼 어떻게 랜덤하게 달라지면 어떡합니까?

그냥 어쩔 수 없어요

그냥 이거 자체가 K-Mins라는 알고리즈 더 나아가서는 클러스터링이라는 건 애초에 정답이 없는 거니까 얘가 답일 수도 있고 제가 답일 수도 있어요

애초에 정답이 없습니다

그냥 다양한 답들이 존재해요

그래서 비지도학습은 이 알고리즘 자체가 정답을 주는 게 아니고 우리가 이제 그걸로 나온 결과를 가지고 뭐 어딘가에 써먹겠죠?

그래서 써먹었을 때 그게 좋으면 잘 좋은 거고 아니면 맞는 겁니다

예를 들면은 뭐 우리가 쇼핑몰을 만드는데 쇼핑몰에 제품들을 이렇게 카테고리로 만들어가지고 묶어서 사용자들한테 보통 보여주잖아요

쇼핑몰들이 근데 이제 보통 카테고리 묶을 때 그냥 사람이 분류를 정할 수도 있고 아니면은 이제 이렇게 알고리즘으로 돌려서 정할 수도 있는데 만약에 카테고리를 이렇게 나눠봤더니 사용자들이 불편하다고 그런가?

다시 알고리즘을 돌려서 이렇게 나눠져서 이렇게 바꿔봤더니 사용자들이 좋아요

그럼 그게 맞는 거죠

그냥 정답이 있는 건 아닌데 그냥 뭐 사용자들이 좋아하면 그걸로 하면 됩니다

비지도학습은 정답은 없지만 누가 좋아한다든가 써봤더니 더 돈이 잘 벌이느라든가 그럼 그게 정답은 아니고 그걸 일종의 정답이라고 생각할 수도 있겠죠

근데 그거는 이제 계산 과정에서 나오는 건 아니고 나중에 그 계산을 바탕으로 써먹었을 때 나오는 거니까 일단 돌리는 단계에서는 딱히 정답이 있는 건 아닙니다

그 다음에 이제 우리가 이 알고리즘을 설명을 지금까지 잘 들어보면 중심에 가까운 거라는 얘기를 하는데 그때 거리는 뭘 쓰고 있을까요?

제가 말은 안 했지만은 유클리드 거리를 쓰고 있죠

직선 거리면 근데 우리가 문서나 단어 유사도 계산할 때는 유클리드 거리를 쓴다 안 쓴다?

잘 안 쓴다

그죠?

그러면은 이 케민즈 알고리즘은 그냥 문서나 단어에다 쓰면 좀 안 맞아요

왜냐면 우리가 문서나 단어 유사도는 코스아잉 유사도를 쓰는데 케민즈 알고리즘은 유클리드 거리를 쓰니까 서로 이 거리 유사도의 기준이 안 맞는 겁니다

그래서 이제 스피리컬 케민즈라는 방법이 있는데 이거는 케민즈에서 거리 대신에 유클리드 거리대신에 코스아잉 유사도를 쓰도록 바꾼 거를 스피리컬 케민즈라고 합니다

근데 이 스피리컬 케민즈는 불행하게도 사이킨런이 없어요

우리가 못쓰는 일입니다

그럼 어떻게 하느냐?

대충 비슷하게 하는 방법이 있어요

우리는 그 방법으로 할 겁니다

자 그래서 유사문서 검색 이거는 아까 했는데 왜 있지?

이건 넘어가고 그래서 이제 아까 스피리컬 케민즈가 안 된다고 했는데 대충 비슷하게 하려면 어떻게 할 수 있냐

면 롤라이즈 기억나십니까?

롤라이즈에이션이 뭐냐면 우리가 데이터가 있을 때 멀리 있을 수도 있고 가깝게 있을 수도 있는데 얘를 다 일정거리로 바꿔주는 그래서 여기 있는 것도 이렇게 오겨주고 가까이 있는 거는 멀리 밀고 멀리 있는 거는 가까이 당겨주고 그래서 L2 노멜라이즈를 하면 어떻게 되냐면 이렇게 구면상에다가 멀리 있는 거는 이 원에 이쪽으로 우리가 지부에 있는데 예를 들면 누구는 지하실에 있고 국민대학교 미경동 국민대학교가 지금 부기 37.6도에 있고 공경 126.99도에 있거든요

이게 우리 좌표잖아요

가로, 세로 좌표인데 예를 들면 국민대 5층에 있건 국민대 지하실에 있건 가로, 세로 좌표는 똑같은 거잖아요

지구라는 어떤 구면이 있으냐

이 사람이 5층에 있으면 지구에서 약간 떠있는 거 그죠 지하실에 있으면 그거 파고 들어가서 있는 건데 그냥 같은 좌표에 있는 거잖아요

너 어디있어?

국민대에 있어?

이러면은 같은 좌표에 있는 거잖아요

마찬가지로 위에 있으면 당겨가지고 여기 표면으로 내리고 밑에 있으면 밀어가지고 표면을 올리고 이렇게 해주는 게 L2 노멜라인에 있는 거에요

그래서 이거를 해주면은 어떻게 되냐면 우리가 이제 이거랑 이거랑 코스앰 유사도는 똑같잖아요

크잖아요

코스앰 유사도가

왜냐하면 코스앰 유사도는 각도만 보니까 각도는 똑같죠

근데 이렇게 같은 표면으로 밀면은 어떻게 됩니까?

코스앰 유사도는 어깨가 크고 유클리드 거리가 0으로 되죠

그래서 코스앰 유사도랑 유클리드 거리가 굉장히 비슷해지는 유사도가 크면은 유클리드 거리가 아주 작아지는 그런게 됩니다

그래서 완전히 똑같진 않은데 L2 노멜라이전을 해준 다음에 그 다음에 유클리드 거리로 케임 인즈를 하면 코스앰 유사도로 케임

인즈를 하는 거랑 얼추 비슷한 결과가 나와요

완전 똑같지는 않지만 그래서 이거를 하는 방법이 여러가지가 있는데 노멜라이즈 함수가 있습니다

사이킬러네

그래서 여기가 12가 아니고 이거는 L2에요

L을 소문자로 쓴 겁니다

소문자 L을 써서 L2 이러면은 L2 노멜라이전을 해주게 됩니다

그래서 이제 독 EMB 문서 토픽 행렬을 노멜라이전을 해가지고 노멜라이전 된 걸로 바꿔주겠습니다

그래서 이제 독 EMB 문서 토픽 행렬을 노멜라이전을 해가지고 그 다음에 이제 문서 클러스터링 케임

인즈를 해주는데 N 클러스터는 9 그래서 9개의 덩어리로 나눌거에요

그래서 특허들이 지금 400 몇 개가 있는데 400 몇 개의 특허를 토픽 패턴이 비슷한 것들끼리 묶어가지고 미리 9개의 특허를 묶어가지고 왜 하필 9개냐

그냥 이건 제 마음대로 한 겁니다

이것도 나중에 이제 9개로 할 건지 10개로 할 건지 정하는 건 좀 이따 해볼거고 그 다음에 이제 이게 랜덤하게 하니까 결과가 이제 그때그때 다를 수가 있거든요

여러분들이랑 저랑 결과를 통일하기 위해서 랜덤 스테이트를 정하는 거고 이건 뭐 1234로 하던지 5678로 하던지 마음인데 그냥 뭐 아무거나 하시면 됩니다

근데 최소한 저랑 여러분들이랑 통일을 해야 똑같이 나오겠죠

그래서 이제 KM fit 하시면 되구요

그래서 요렇게 돌리면 여기 클러스터라고 나오는데 그리고 이렇게 클러스터 이berger 이 여기 보시면은 이게 순서대로 나온거에요

0번 특허는 5번 클러스터에 속합니다

5번 그룹에 들어가있습니다

그 다음에 1번하고 2번 특허는 8번 클러스터에 들어가있습니다

1일이 비슷한거를 찾았잖아요

지금은 미리 9개의 덩어리로 다 묶어 놓은거에요

거기 가면은 비슷한 애들끼리 모여 있겠죠

미리 모아 둔겁니다

그래서 이제 우리가 0번 문서는 클러스터가 몇번이냐면 5번 클러스터겠죠

그래서 클러스터 idx는 5번이 될거구요

5번에 해당되는 문서들을 보면은 0번 5번 25번 54번 이런 특허들인데 개운죽 줄기 추출물을 함유하는 샴푸 조성물 어쩌고 이건 샴푸통이 들어갔는데 이런건 좀 잘못들어가있는데 발효홍삼을 유효성분으로 하는 샴푸제조 소일성분을 함유하는 발효조성 횡년, 팅크처 방식, 탈모방지, 청국장 삼키 별개 다 있죠

그래서 보면은 대체로 뭔가 추출물을 써가지고 뭔가 모발이나 두피에 개선효과가 있는 특허들이 대체로 5번 클러스터에 이렇게 이렇게 옹기종기 들어가있는걸 볼 수 있습니다

그 다음에 아까 보니까 우리가 지금 1번 2번 다 8번 클러스터에 들어가있죠

그러면은 1번으로 해서 보면은 i가 1번이면은 클러스터 idx가 8번이 될겁니다

8번에는 어떤 애들이 있냐면 뭘까요?

1번 2번 1번은 탈모케어 헤어 샴푸고 2번은 천연인데 이것도 비슷하지 않나?

비슷한것 같아요 1번은 잘 모르겠네

다른걸 한번 봅시다

0 1 2 3 그래서 이거는 뭐 드라이 샴푸, 샴푸캡, 샴푸용 방수코트, 탈모하나형 샴푸 클러스터링이 좀 이상하게 된것 같긴 해요

이거는 반려동물용 샴푸 이런것들이 모여있는 이런것들이 클러스터가 되어있습니다

이런식으로 어쨌든 그룹화가 되어있습니다

그래서 방금 보면은 어떤 클러스터는 좀 이상하게 될 때도 있구요

왜냐하면은 우리가 클러스터링을 할 때 아까도 살짝 비슷한 얘기를 하긴 했는데 예를 들면은 실제로는 그룹이 이렇게 3덩어리가 있어요 특허가 예를 들면 이렇게 3덩어리가 있는데 얘네를 2덩어리로 쪼개면 이렇게 쪼개기 때문에 사실 별로 관련없는 애들끼리 1덩어리가 되거든요

그러면은 좀 결과가 이상하게 되고 또 4덩어리로 쪼개면 어떻게 될까요?

4덩어리로 얘네를 쪼개면 억지로 얘네를 쪼개야 되니까 이렇게 쪼개질 수가 있다는거죠 억지로 쪼개다 보면은 이런식으로 쪼개집니다

그러면은 얘네는 괜찮은데 요런 데에서 억지로 사실 똑같은 애들인데 2덩어리 쪼개져버린거에요

이거를 클러스터링에서는 개수가 되게 중요합니다

개수를 너무 많거나 너무 적거나 하면은 결과가 이상하게 나올 수가 있어요

근데 이 9개를 지금 대충 제가 나눠놨는데 그러면은 좀 안 맞을 때가 있거든요

그래서 요거를 정하려면 어떻게 되냐

근데 이건 사실 되게 어려운 문제입니다

왜냐면은 실제로 데이터가 이렇게 깔끔하게 나눠져 있는 경우가 잘 없고 이렇게 다 조금씩 이렇게 이어져 있어요

실제로 시각화를 해보면은 데이터라는 게 깔끔하고 왜냐면은 뭐 추출물을 이용해서 두피에 좋은 샴푸 근데 추출물을 안 넣고 두피에 좋은 샴푸 근데 뭐 두피에 좋지만 그 성분을 잘 보존하는 샴푸통 성분이랑 상관없는 샴푸통 뭐 약간 이런 식으로 이렇게 딱 끊어지는 게 아니고 이렇게 연결이 되는 경우가 좀 있거든요

그래서 이거를 그루핑을 할 때 이게 딱 몇 개다

이런 게 그 정확한 답이 잘 나오지 않습니다

그렇지만은 그래도 여러 가지 우리가 기준으로 그런 거를 볼 수 있는데 그 중에 하나가 이제 지표가 우리가 두 개를 소개해 드릴 텐데 하나는 이제 관성이라는 게 있고 또 하나는 실루엣 개수라는 게 있어요

그래서 이제 관성은 우리가 기본적으로 케이민제를 할 때는 중심점까지의 거리가 가까운 걸 기준으로 이렇게 묶어주잖아요

그러면 이렇게 중심점이 있는데 이렇게 바글바글 중심점 기준으로 몰려 있어야 이게 클러스터링이 잘 된 거지 중심점이 여기 있는데 어떤 거는 여기 있고 어떤 거는 여기 있고 어떤 거는 여기 있고 어떤 거는 여기 있으면 이건 사실 뭐 클러스터라고 볼 수가 없잖아요

그죠

예를 들면은 똑같이 뭐 예를 들어서 대전 우리 우리 가족은 우리 가족은 다 대전에 몰려 살아

아 그래 아버지 어디 계시는데 아버지 서울에 계시지 어머니는?

어머니 부산에 계셔

너는 어디 산데?

나 광주 살아 대전을 중심으로 몰려 산다고?

대전 중심으로 이렇게 있잖아

그죠?

아빠 서울 살고 엄마 부산 살고 나 광주 살면 다 대전을 중심으로 이렇게 몰려 사는 거잖아요

좀 멀리 몰려 사는 거일 뿐이지 그죠?

이해되시죠?

그러니까 그러면 말이 틀린 건 아닌데 좀 말이 너 이상하게 한다 약간 이런 느낌이 있잖아요

그죠?

이렇게 되면은 이게 클러스터링이 잘 이것도 뭐 클러스터링이 되긴 된 건데 별로란 말이에요

그죠?

이렇게 되는 게 좋은 거죠

그래서 이 관성이라는 거는 이 거리를 재가지고 이렇게 되면은 관성이 작은 겁니다

이렇게 되면은 관성이 큰 거고 그래서 관성이 작아야 좋다

이렇게 생각을 하는 거죠

그러면은 관성도 결국에는 이 클러스터가 많아지면은 무조건 작아지거든요

왜냐하면은 예를 들면은 이렇게 3개 눈으로 볼 때 이렇게 3개가 있는데 뭐 2개로 나누면은 이렇게 이렇게 될 테니까 여기는 이제 거리가 멀잖아요

관성이 크겠죠

3개면은 관성이 줄어들고 4개로 해도 관성이 늘어나진 않습니다

그냥 이렇게 될 뿐이지

그러니까는 많이 하면 관성이 줄어들 건데 3개로 할 때나 3개로 하면 여기에 중심이 있고 이렇게 붙어 있을 텐데 4개로 하면은 뭐 여기 하나 이렇게 하나 이렇게 되겠죠

그러면은 줄긴 줄겠지만 별로 크게 많이 줄진 않겠죠

그래서 그림으로 그리면 똑같이 스크립 플로우시 만들어지게 됩니다

LSA 할 때랑 비슷하게 그래서 클러스터를 이렇게 늘리면은 관성이 줄다가 어느 순간부터는 잘 안 줄어요

그러면은 보통 이것도 이제 이렇게 꺾이는 지점을 찾습니다

근데 지금 이거는 제가 미리 그려본 건데 그렇게 이렇게 뚜렷하게 꺾이지가 않거든요

이렇게 딱 이렇게 딱 이렇게 딱 이렇게 딱 잡으면 되는데 이렇게 하면은 이제 약간 생각하기 나름 저는 여기서 꺾이는 것 같은데 나는 여기 같은데 약간 애매하죠

아니면 그 다음 이 정도는 되야 되지 않나 애매한데 사실 이거는 뭐 어디서 꺾인다고 좀 보기가 어려울 것 같고 약간 대충 뭐 이 어둠 어딘가 정도로 생각하시면 됩니다

저는 보니까 한 9 정도에서 꺾이는 것 같더라고요

그래서 이제 9에서 잡았는데 이거는 여러분들이 보시고 마음에 드는 걸로 합니다

그래서 이제 이건 어떻게 하냐면 이 K-Mins를 지금 20번을 돌린 거예요

20번을 돌려가지고 K가 1, 2, 3, 4, 5, 6 이렇게 다 바꿔가면서 K-Mins를 돌려가지고 그거에 이제 관성 값, 이너샷 값을 구해서 이거를 플러스로 그린 겁니다

되게 많이 그려야겠다 그래서 보면은 이렇게 꺾이는 걸 볼 수 있고 그래서 이제 이거는 약간 마음에 눈을 줘봐야 됩니다

되게 애매하죠?

이게 어디서 꺾이는 거야?

이게 보통 주식 투세 같은 거 하면은 아 뭐 추세가 꺾였어

이런 얘기 그거 다 사람마다 의견이 다르잖아요

가끔 보면은 막 이렇게 하는 거잖아요

그래서 이게 약간 이런 거였고 그래서 이제 이렇게 하면은 이렇게 하면은 이렇게 하면은 이렇게 하면은 이렇게 하면은 이렇게 하면은 의견이 다르잖아요

가끔 보면은 그 막 이렇게 차트 같은 거 그려놓고 여기에서 꺾여서 올라갈 거 이러면은 아니야

이렇게 꺾여서 내려가 이게 사람 보는 사람 마음이거든요

내가 샀으면 이렇게 올라갈 거 같고 안 샀으면 이렇게 막 떨어질 거 같고 이런데 이것도 약간 좀 그렇습니다

이게 뭐 어디서 딱 꺾인다고 보기는 힘들어요

이게 이제 여러분 마음대로 하시면 됩니다

근데 이제 한 대충 한 이 정도 어디라고 하면 되겠죠

이런 데는 아직 너무 다파르고 이런 데 완만하니까 중간은 어딘데 제 눈에는 한 9쯤인 거 같아요

네 여러분이 저는 10 같은데요

그럼 여러분 10에서 하시면 됩니다

그 정도는 분석가의 재량이라고 볼 수 있어요

그래서 이제 고렇게 보시면 될 거 같고 그래서 이렇게 관성으로 이제 이게 잘 됐냐

안됐냐

이렇게 볼 수도 있고요 그 다음에 이제 실루엣 개수라는 게 있습니다

실루엣 개수는 지금 관성은 중심을 기준으로 하는 거고 실루엣 개수는 예를 들어서 우리가 이제 클러스터를 이렇게 하고 지금 이제 동그라미랑 더학이랑 이렇게 두 가지로 나눴는데

그럼 여기 있는 얘는 이쪽에서 제일 가까운 거는 거리가 이만큼이고 이쪽에서 제일 가까운 거하고는 거리가 이만큼이죠

그러면은 요거에 비하면은 그래도 이제 이쪽이 좀 더 가까운 편이라고 할 수 있겠죠

그 다음에 예를 들어서 얘는 어떻습니까

얘는 이쪽에서 제일 가까운 거는 이만큼이고 이쪽에서 제일 가까운 거는 이만큼이고 이쪽에서 제일 가까운 거는 이만큼이고 이쪽에서 제일 가까운 거는 이만큼이죠

그러면은 요거에 비하면은 이쪽이 훨씬 가까운 거죠

그래서 이 실루엣 개수는 중심까지의 거리가 아니고 군집 내에서 나랑 같은 그룹에 속하는 애들 중에 제일 가까운 애의 거리하고 다른 군집에 속하는 그러니까 남하고 제일 가까운 거하고 중심은 상관없고 내가 예를 들면은 우리가 반을 1반, 2반 이렇게 나눴어요

내가 어쨌든 1반이야 1반에서 나랑 어쨌든 제일 비슷한 애가 있을 거고 2반에도 나랑 제일 비슷한 애가 있을 거 아니에요

그러면은 그거하고의 두 개의 거리 비율만 보는 겁니다 중심은 됐고 그래서 이거는 경계선에 있는 애들이 가운데 안쪽에 있는 애들은 상관없어요

안쪽에 있는 애들은 상관없는데 경계에 뭐 예를 들면은 이렇게 가다가 뭐 이런 데 있는 애들이 있을 수 있거든요

경계선에 바짝 붙어 있는 애들 그럼 얘는 중심을 기준으로 하면 어쨌든 이쪽 중심에 왼쪽 중심에 가깝긴 하지만 그림을 이렇게 그려봅시다

어떤 경우가 생길 수 있냐

면 중심이 여기 하나, 여기 하나 있는데 이쪽에는 이렇게 있고 이렇게 있어요

이쪽에는 다 빨간색이니까 세모로 그립시다

이렇게 있어요

그럼 이 동그라미는 이 중심에 가깝냐, 이 중심에 가깝냐 하면 이쪽 중심이 더 가까운데 자기 친구들하고는 되게 멀리 떨어져 있습니다

어차피 중심이라는 건 평균이니까 왼쪽에 이렇게 바글바글 모여있으면 오른쪽에 있어도 여기가 중심이면 중심이겠죠

근데 보면은 이쪽에서는 차라리 얘하고 더 가깝답니다

이런 경우가 생길 수 있겠죠

그러면 실루엣 개수가 어떻게 되냐면 굉장히 크게 튀어서 나오게 됩니다

그래서 이 실루엣 개수는 이렇게 경계선에 있는 애들이 어떤 특성을 가지냐

이런 것을 보려고 이런 식으로 하시면 빨리 끝내야겠네

만든 개수에요

그래서 이거를 하나하나의 실루엣 개수를 다 구합니다

전마다 다 구해가지고 이거의 분포를 볼 수가 있는데 시각화를 해서 보면은 이런 식으로 나와요

그래서 이게 첫번째 0번 클러스터인데 이게 실루엣 개수가 자그마한 개수가 작은 애들은 중심에 가까이 있는 애들이고 실루엣 개수가 큰 애들은 경계선 쪽에 있는 애들 어떤 거는 경계선 쪽에 있고 어떤 거는 중심에 있으니까 보통 이렇게 떨어지는 순서로 나와요

이렇게 떨어지는 순서로 여기서는 보시면은 두 가지를 볼 수 있는데 바닥은 당연히 비슷할 거고 고점인데 경계에 있는 애들이 얼마나 실루엣 개수가 높으냐인데 사실 다 고만고만하죠 크게 뭔가 한쪽으로 치우쳐있는 경우는 잘 없다는 거예요

또 하나는 이 폭인데 이 폭은 뭘 의미하냐면 한 클러스터에 얼마나 많은 데이터가 있느냐

이거를 나타냅니다

이게 이제 여기서 여기까지가 0번 클러스터고 여기서 여기까지가 1번 클러스터 여기까지가 2번, 3번, 4번, 5번 6번, 7번, 8번 왜 8개지?

하나, 둘, 셋, 넷, 다섯 일곱, 여덟, 아홉 개 제가 숫자를 잘못 샜네요

어쨌든, 근데 이거는 좀 많고 얘네 둘은 좀 많고 나머지는 고만고만하고 그래도 큰 차이는 안 나죠

이건 뭐냐면 클러스터가 대체로 고르게 형성이 됐다는 거죠

만약에 여러분이 클러스터를 이상하게 잡으면 예를 들어 아까처럼 세 덩어리로 있는데 이걸 두 덩어리로 쪼겠어

이거는 많고 이거는 적고 이렇게 되겠죠

비대칭적으로 이 폭이 너무 한쪽이 크거나 한쪽이 좁거나 그러면 그거는 잘못 잡힌 클러스터의 가능성이 있다

그다음에 이게 하나가 이렇게 툭 튀어 올라와있어요

그러면 그건 뭐예요?

너무 높다는 거는 자기편보다 남의 편이 더 가까운 애들이 많다는 거잖아요

그것도 좀 이상한 겁니다

기본적으로 높이도 좀 고르고 이 폭도 고르게 골고루 나와야 잘 잡혔을 가능성이 높다

이렇게 볼 수 있습니다

그래서 이거 빠르게 한번 해보고 여기서 왜 시부를 했지?

여기 강의자료가 또 잘못되어 있는데 여기 샘플 실루엣이라고 돼있는데 이거를 앞에 샘플을 떼주세요

여기 실루엣 땡땡하고 샘플 실루엣이라고 돼있는데 여기 슬리퍼를 떼고 여기 슬리퍼를 떼고 여기 슬리퍼를 떼고 여기 슬리퍼를 떼고 여기 슬리퍼를 떼고 땡땡하고 샘플 실루엣이라고 돼있는데 여기 슬라이드에 여기 샘플 실루엣이라고 돼있는데 여기 앞에 샘플 언더바이오건 떼주세요

그래서 이렇게 나오고 이거를 클러스터 갯수를 지금 9개인데 8개 이렇게 바꿔봅시다 여기 엔 클러스터를 8로 바꿔서 다시 돌려보면 이렇게 하면 지금 보시면 아까보다 훨씬 더 높이가 들쭉날쭉하죠

그 다음에 이 뒤에가 얘네는 두껍고 얘네는 좁고 좀 이렇잖아요

그러면 이걸 보니까 8개보다는 그래도 9개가 골고루 잘 묶이는 것 같다

우리가 이렇게 생각을 해볼 수가 있습니다

높이가 지금 얘네는 되게 높은데 얘는 낮고 이렇잖아요

그래서 모양이 고르지 않다

이렇게 볼 수가 있고 그 다음에 10개로 그럼 늘리면 어떨까

10개로 늘려보니까 이번에는 고른 것도 문제지만 여기 있는 이 클러스터가 너무 작죠

지금 보면 되게 뾰족한 애가 있잖아요

얘네는 클러스터를 만들었는데 자 여기 여러분들 여기 조별 토론하겠습니다

조별로 뭉치세요

했는데 다 10명씩 모여있는데 어디 2명만 모였으면 선생님들은 뭐하십니까

이렇게 나올 거 아니에요

이상한 거잖아요

그쵸?

남들은 다 이만큼씩 모여있는데 여기만 이만큼 모였어요

얘네는 좀 이상한데 이런 거죠

그래서 8개나 10개보다는 그래도 9개가 좀 9개로 해보면 모양이 고르게 잡힌다

실루엣 개수로 모양을 그려보면 물론 여기가 좀 크긴 한데 고르게 보인다

우리가 아까 이너시아로 그린 스크립 플롯이랑 이 실루엣 개수로 그린 플롯이랑 이걸 종합적으로 고려해서 또 다른 것도 고려해야겠지만 9개 정도면 괜찮은 것 같다

이런 식으로 판단을 할 수 있게 됩니다

드디어 6주만에 슬라이드를 꽉 채워서 다 했군요 밀리는 슬라이드는 없고 자 여기까지 했고요 그 다음에 질문 있으시면 질문해주세요

다음 주 퀴즈는 관성이 뭐냐 실루엣 개수가 뭐냐

이런 거 물어볼 수 있겠죠

그래서 다음 주에 퀴즈가 약간 연결이 가리키고 또 그 다음 주에 퀴즈가 어떤 거 배워볼 수 있겠죠

그런 것도 한번 다시 복습을 해보시면 되겠습니다

그럼 여기까지 할까요?