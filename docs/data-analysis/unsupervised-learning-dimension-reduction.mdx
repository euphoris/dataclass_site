# 비지도 학습: 차원 축소



## 비지도 학습 unsupervised learning
- 라벨이 없는 데이터로부터 학습
- 데이터의 구조나 패턴 발견
- 종류
    - 클러스터링(clustering): 비슷한 특성을 가지는 군집으로 묶음
    - 차원 축소(dimensionality reduction): 데이터를 더 낮은 차원으로 표현
- 예시
    - 고객 세분화: 비슷한 특성을 가진 고객 그룹으로 분류
    - 문서 군집화: 유사한 내용의 문서를 자동으로 묶음
    - 차원 축소: 데이터의 중요 정보를 유지하면서 차원 축소
    - 장바구니 분석: 고객의 구매 패턴을 분석하여 연관 상품 추천


## 차원 축소 Dimensionality Reduction
- 고차원(많은 특성) 데이터를 중요한 정보는 최대한 유지하면서 더 낮은 차원으로 변환하는 기법
- 목표 및 장점:
    - 과적합 방지: 모델 복잡도를 줄여 일반화 성능 향상.
    - 계산 효율성 증대: 학습 속도 향상, 메모리 및 저장 공간 절약.
    - 시각화 용이: 2D 또는 3D로 데이터를 시각화하여 패턴 파악에 도움.
    - '차원의 저주' 완화: 고차원 공간에서 발생하는 데이터 희소성 및 거리 계산 문제 완화.
    - (경우에 따라) 성능 향상: 불필요하거나 노이즈가 많은 특성 제거.
- 고려사항:
    - 차원 축소 과정에서 일부 정보 손실이 발생할 수 있음.
    - 특성 추출로 생성된 새로운 특성은 원래 특성의 의미를 직관적으로 해석하기 어려울 수 있음.
- 성격: 사람의 다양한 특성을 외향적/내향적 등 몇 가지 차원으로 표현하는 일상에서 차원 축소의 예




## 여러 가지 차원 축소 기법
- 주성분 분석(Principal Component Analysis, PCA)  
  고차원 데이터를 저차원으로 변환하여 데이터의 분산을 최대한 보존하는 기법
- 독립성분 분석(Independent Component Analysis, ICA)  
  관측된 데이터가 여러 독립적인 신호의 혼합물로 구성되어 있다고 가정
- 비음행렬 분해(Non-Negative Matrix Factorization, NMF)  
  행렬을 두 개의 비음수 행렬의 곱으로 분해하는 기법


## t-SNE t-Distributed Stochastic Neighbor Embedding
- 고차원 데이터를 저차원 공간(일반적으로 2차원 또는 3차원)으로 변환하여 데이터의 구조적 특징을 시각적으로 이해할 수 있게 하는 차원 축소 기법
- 방법
    - 고차원 공간에서 다변량 정규분포를 기반으로 유사성을 확률로 계산
    - 저차원 공간에서의 다변량 t 분포를 기반으로 유사성을 확률로 계산
    - 고차원 공간에서의 확률 분포와 저차원 공간에서의 확률 분포의 차이를 KL 발산으로 측정
    - KL 발산을 최소화하도록 저차원 공간의 좌표를 결정
- 단점 및 해석상 주의점
    - 계산 비용이 높음
    - 같은 데이터셋에서도 실행할 때마다 다른 결과
    - 저차원 공간에서 거리가 반드시 고차원 공간에서 거리와 비례하지 않음
    - 비선형 차원 축소 기법


## PCA 실습
- 데이터
  ```python
  cc = pd.read_excel('cancer.xlsx')
  X = cc.iloc[:, 1:]
  ```
- PCA
  ```python
  from sklearn.decomposition import PCA
  pca = PCA(n_components=2)
  X_pca = pca.fit_transform(X)
  ```


## 시각화
```python
import matplotlib.pyplot as plt
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cc.iloc[:, 0].map({'M': 1, 'B': 0}))
```


## 결정 경계의 시각화
- 랜덤 포레스트
  ```python
  from sklearn.ensemble import RandomForestClassifier
  y = cc.iloc[:, 0].map({'M': 1, 'B': 0})
  rf = RandomForestClassifier(n_estimators=100, random_state=42)
  rf.fit(X, y)
  y_pred = rf.predict(X)
  ```
- 시각화
  ```python
  pc0_min, pc0_max = X_pca[:, 0].min() - 1, X_pca[:, 0].max() + 1 # 가로축 범위
  pc1_min, pc1_max = X_pca[:, 1].min() - 1, X_pca[:, 1].max() + 1 # 세로축 범위
  pc0, pc1 = np.meshgrid(np.linspace(pc0_min, pc0_max, 200), np.linspace(pc1_min, pc1_max, 200)) 
  X_range = pca.inverse_transform(np.c_[pc0.ravel(), pc1.ravel()]) # 역변환
  y_pred = rf.predict_proba(X_range)[:, 1]
  y_pred = y_pred.reshape(pc0.shape)
  plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, edgecolor='k')
  plt.contourf(pc0, pc1, y_pred, alpha=0.3, levels=np.linspace(y_pred.min(), y_pred.max(), 3))
  ```

## 퀴즈

<iframe src="https://tally.so/embed/mRjE9j?alignLeft=1&hideTitle=1&transparentBackground=1&dynamicHeight=1" loading="lazy" width="100%" height="1422" frameborder="0" marginheight="0" marginwidth="0" title="[통계] 비지도학습 - 차원 축소"></iframe>