# 강화 학습과 딥러닝




## 강화 학습 Reinforcement Learning
- 행위자는 환경과 상호작용
- 행위자의 행동(A)에 따라 보상이 주어짐
- 수익(G): 보상을 장기간에 걸쳐 누적한 것
- 행위자는 현재 상태(S)에서 앞으로 수익이 가장 큰 행동을 내는 정책(π)을 찾아야 함

$$
A = \max_G \pi(S)
$$


## 강화 학습 예시
- 게임 인공지능: 게임 환경에서 스스로 학습하여 최적의 전략을 수행(예: AlphaGo)
- 로봇 제어: 로봇이 다양한 환경에서 최적의 행동을 학습
- 자율 주행: 자율 주행 차량이 주행 상황에 맞춰 최적의 경로를 선택



## 지도 학습과 강화 학습의 차이

| 지도학습 | 강화학습 |
| :--- | :--- |
| X에서 Y를 예측하는 문제 | X에서 가장 보상이 큰 행동을 찾는 문제 (Y는 예측하지 않을 수도 있음) |
| Y에 대한 예측 오차를 줄이는 것이 목표 | 행동으로 인한 보상을 최대화하는 것이 목표 |
| X와 Y가 모두 있는 데이터가 필요 | 데이터 대신 직접 시행 착오 |
| 바둑) 현재 상황(X)에서 프로 기사들의 다음 수(Y)를 학습 | 바둑) 현재 판세(X)에서 다음 수(A)를 시행착오를 통해 학습 |
| 투자) 기업의 정보(X)에서 주가(Y)를 학습 | 투자) 기업의 정보(X)에서 매수/매도/보유(A)를 시행착오를 통해 학습 |


## 강화학습의 현업 적용에서 이슈
- 강화학습은 지도학습과 달리 레이블이 있는 과거 데이터를 사용하기 어려움
- 행위자가 직접 시행착오를 통해 보상을 최대화하는 정책을 발견해야 함
- 현실에서 시행착오를 할 경우 시간이 오래 걸리고 비용이 많이 들기 때문에 시뮬레이션이 필요
    - 바둑이 빨리 정복된 이유: 가상의 환경 내에서 100% 실행 가능하기 때문
- 보상 함수를 잘못 정의할 경우 의도하지 않은 방향의 행동을 할 수 있음
    - 예: 테트리스를 "오래" 플레이하는 것에 따라 보상을 할 경우 게임을 일시정지 시키고 플레이하지 않는 방법을 발견


## 뉴런 neuron
- 동물의 신경계를 구성하는 신경 세포
- 흥분하면 전기 신호가 발생
- 신경 전달 물질을 사용하여 세포 간 통신


## 신경망 Neural Network
- 뇌는 신경 세포의 네트워크가 다층 구조를 이루고 있음


## 인공신경망 artificial neural network
- 생물학적 신경망에서 영감을 받은 머신러닝 모형
- 인공 뉴런은 입력값을 가중합하고, 그 결과에 활성화 함수를 적용하여 출력
- 인공 뉴런은 로지스틱 회귀분석과 (거의) 같음
- 다층 신경망: 뉴런들로 이뤄진 층(layer)을 만들고, 이것을 여러 층으로 쌓은 것
    → 딥러닝으로 발전
- 보편근사정리: 다층신경망은 어떤 함수도 모방할 수 있음


## 인공신경망의 학습
- 파라미터를 조정하여 모형의 예측과 실제 값이 가장 잘 맞게 조정
- 예측과 실제 값의 차이는 손실 함수로 계산
- 인공신경망은 매우 복잡한 함수이므로, 최적 파라미터를 한 번에 찾을 수 있는 공식은 없음
- 경사하강법: 손실이 줄어드는 방향(경사)으로 파라미터를 점진적으로 조정하여 손실을 줄여 나감(하강)
- 역전파 backpropagation: 출력층 → 은닉층 → 입력층 방향으로 오차를 역방향으로 전파하여 파라미터 조정

## 퀴즈

<iframe src="https://tally.so/embed/3yryyx?alignLeft=1&hideTitle=1&transparentBackground=1&dynamicHeight=1" loading="lazy" width="100%" height="3555" frameborder="0" marginheight="0" marginwidth="0" title="[통계] 강화학습과 딥러닝"></iframe>