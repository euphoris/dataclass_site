# 딥러닝

## 뉴런 (Neuron)

*   동물 **신경계 구성** 기본 단위 신경 세포.
*   **기능:** 외부 자극 받아 **흥분 시 전기 신호(활동 전위)** 발생.
*   **통신:** **신경 전달 물질** 사용하여 다른 뉴런/세포와 정보 전달.

## 신경망 (Neural Network)

*   **뇌 구조:** 수많은 뉴런들이 **네트워크** 이루고, **여러 층(layer)** 구조로 정보 처리.

## 인공신경망 (Artificial Neural Network, ANN)

*   생물학적 신경망 구조/기능 **모방/영감** 받은 머신러닝 모형.
*   **인공 뉴런 (Perceptron):**
    *   여러 입력($x_i$) 받아 각각 **가중치($w_i$)** 곱하여 합산 (가중합, $\sum w_i x_i$).
    *   가중합에 **편향(bias, b)** 더함.
    *   결과 값을 **활성화 함수(activation function)** 통과시켜 최종 출력($y$) 생성.
    *   (수학적으로 로지스틱 회귀와 매우 유사한 형태 가짐)
*   **다층 신경망 (Multi-Layer ANN / MLP):**
    *   인공 뉴런으로 구성된 **층(layer)** 여러 개 쌓아 만든 구조.
    *   입력층(Input) - **은닉층(Hidden Layer)** 여러 개 - 출력층(Output).
    *   층 깊어질수록(많아질수록) 더 복잡한 패턴 학습 가능 → **딥러닝(Deep Learning)**으로 발전.
*   **보편 근사 정리 (Universal Approximation Theorem):** 충분한 크기(뉴런 수, 층 수)의 다층신경망은 **어떤 연속 함수든** 원하는 정확도로 근사(모방) 가능.

## 인공신경망의 학습

*   **목표:** 모형 예측값과 실제 값 차이(**손실 함수 값**) 최소화하도록 **파라미터(가중치 w, 편향 b)** 조정.
*   **최적화 어려움:** 인공신경망은 매우 복잡한 비선형 함수. 최적 파라미터 **한 번에 찾는 공식 없음**.
*   **경사 하강법 (Gradient Descent):**
    *   손실 함수 **기울기(gradient)** 계산하여, 손실 값 **감소하는 방향**으로 파라미터 **점진적 업데이트**. 언덕 내려가듯 최저점 찾아가는 방식.
*   **역전파 (Backpropagation):**
    *   다층 신경망에서 **효율적으로 기울기 계산**하는 알고리즘.
    *   출력층 오차(손실) 계산 → **역방향(출력층 → 은닉층 → 입력층)**으로 오차 정보(기울기) 전파하며 각 층 파라미터 업데이트.


## 퀴즈

<iframe src="https://tally.so/embed/w4OebY?alignLeft=1&hideTitle=1&transparentBackground=1&dynamicHeight=1" loading="lazy" width="100%" height="1198" frameborder="0" marginheight="0" marginwidth="0" title="[통계] 딥러닝"></iframe>
