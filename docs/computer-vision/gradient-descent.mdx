# 경사하강법

## 경사하강법 gradient descent
*   기존의 파라미터를 바탕으로 예측 ŷ을 출력
*   실제 y와 비교하여 손실을 계산 L(y,ŷ)
*   손실을 줄이는 방향으로 파라미터를 수정
*   위의 과정을 반복

## 학습률 learning rate
*   파라미터는 한 번에 경사 × 학습률만큼 수정
*   경사가 0에 가까울 수록 오차는 최소에 가까워짐
*   경사에 비례하여 수정할 크기를 조정
*   학습률이 크면 학습이 빠르지만 최소점 근처에서 수렴하지 않을 수 있음

## 경사하강법 예시
*   데이터

| y | x |
| :--: | :-: |
| 3 | 1 |
| 6 | 2 |
| 9 | 3 |
| 12 | 4 |

*   모형
    `y = wx`

## 경사하강법 (step 1)
*   w = 0

| | y | x | ŷ | y - ŷ | L | ∂L/∂w |
| :-- | :-: | :-: | :-: | :---: | :--: | :---: |
| | 3 | 1 | 0 | 3 | 9 | -6 |
| | 6 | 2 | 0 | 6 | 36 | -24 |
| | 9 | 3 | 0 | 9 | 81 | -54 |
| | 12 | 4 | 0 | 12 | 144 | -96 |
| **평균** | | | | | 67.5 | -45 |

*   경사가 -45이므로 w를 높여야 오차를 줄일 수 있음
*   현재 w = 0
*   경사 -45
*   학습률 0.1
    `w ← 0 - 0.1 × (-45) = 4.5`

## 경사하강법 (step 2)
*   w = 4.5

| | y | x | ŷ | y - ŷ | L | ∂L/∂w |
| :-- | :-: | :-: | :--: | :---: | :----: | :---: |
| | 3 | 1 | 4.5 | -1.5 | 2.25 | 3 |
| | 6 | 2 | 9 | -3 | 9 | 12 |
| | 9 | 3 | 13.5| -4.5 | 20.25 | 27 |
| | 12 | 4 | 18 | -6 | 36 | 48 |
| **평균** | | | | | 16.88 | 22.5 |

*   경사가 +22.5이므로 w를 낮춰야 오차를 줄일 수 있음
    `w ← 4.5 - 0.1 × (22.5) = 2.25`

## 경사하강법 (step 3)
*   w = 2.25

| | y | x | ŷ | y - ŷ | L | ∂L/∂w |
| :-- | :-: | :-: | :--: | :---: | :----: | :----: |
| | 3 | 1 | 2.25 | 0.75 | 0.56 | -1.5 |
| | 6 | 2 | 4.5 | 1.5 | 2.25 | -6 |
| | 9 | 3 | 6.75 | 2.25 | 5.06 | -13.5 |
| | 12 | 4 | 9 | 3 | 9 | -24 |
| **평균** | | | | | 4.22 | -11.25 |

*   경사가 -11.25이므로 w를 높여야 오차를 줄일 수 있음
    `w ← 2.25 - 0.1 × (-11.25) = 3.375`

## 경사하강법의 문제점
*   한 단계의 가중치 수정을 위해 전체 데이터셋에 대해 계산
*   국소최소점(local minima: 근방에서만 최소인 점)에 수렴
*   안장점(saddle point: 경사가 0이지만 최소나 최대가 아닌 점)에서 업데이트 불가

## 확률적 경사하강법 Stochastic GD
*   하나의 사례를 바탕으로 경사를 계산
*   극소점을 향해 바로 가지 않고 지그재그로 이동하게 됨
*   한 번의 업데이트를 위한 계산량이 적음
*   국소최적화를 피할 가능성이 있음
*   업데이트 방향이 불안정하므로 더 많은 업데이트가 필요할 수 있음

## 미니배치(Mini-batch) 경사하강법
*   전체 데이터의 일부(=미니배치)만을 사용하여 경사하강법
*   경사하강법과 확률적 경사하강법을 장점을 융합
*   일반적으로 확률적 경사하강법이라고 하면 실제로는 미니배치 경사하강법을 가리킴

## 모멘텀 momentum
*   파라미터에 경사를 직접 더하는 대신, "속도"에 경사를 반영
*   파라미터는 속도에 따라 변화
*   장점:
    *   국소 최적에서 벗어나는데 도움이 됨
    *   진동을 억제하는 효과

## Adam
*   학습률을 가변적으로 조정하는 방법
*   Adagrad: 많이 가본 방향의 학습률은 낮추고, 새로운 방향의 학습률은 높임
    *   경사를 제곱해서 부호를 없애고, 그 크기를 G에 누적
    *   경사의 누적된 크기로 학습률을 나눔 → 많이 이동한 방향으로는 학습률이 낮아져 이동 X
    *   분모가 0이 되는 것을 막기 위해 아주 작은 수 ε을 더함
*   RMSProp: 경사의 크기를 누적시킬 때, 최근 경사가 항상 일정 비율 반영되도록 수정
    *   최근의 경사를 항상 (1 − β)만큼 반영 (0 < β < 1)
*   Adam: RMSProp + 모멘텀
*   RAdam(Rectified Adam): 학습 초기의 Adam의 불안정성을 해결한 알고리즘

## Lookahead
*   느린 가중치를 빠른 가중치로 복사
*   빠른 가중치에 k번 경사하강법 실시
*   빠른 가중치를 느린 가중치에 일정 비율 반영
*   최적점 주변에서 빠른 가중치가 수렴하지 못할 경우, 손실함수의 형태가 대체로 볼록하므로 위와 같이 하면 더 빨리 수렴할 수 있음
*   국소최적에 빠질 때도 빠져나오기 쉬움
*   Ranger: Lookahead에서 RAdam을 빠른 가중치에 적용하는 알고리즘
*   Lookahead는 PyTorch에서 지원X

## 경사하강법 알고리즘 설정
```python
optimizer = torch.optim.Adam(self.parameters(), lr=0.001)
```
*   학습 알고리즘은 Adam, 학습률은 0.001
*   PyTorch에서 지원하는 알고리즘:

- Adadelta 
- Adafactor 
- Adagrad 
- Adam 
- AdamW 
- SparseAdam 
- Adamax 
- ASGD 
- LBFGS 
- NAdam 
- RAdam 
- RMSprop 
- Rprop 
- SGD

## 퀴즈
<iframe src="https://tally.so/embed/mVVDDa?alignLeft=1&hideTitle=1&transparentBackground=1&dynamicHeight=1" loading="lazy" width="100%" height="1200" frameborder="0" marginheight="0" marginwidth="0" title="[CV] 경사하강법"></iframe>