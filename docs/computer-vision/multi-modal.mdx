# 멀티 모달

그래서 우리가 인베딩 얘기를 했었는데 앞에서 했던 인베딩은 이미지랑 이미지를 비교하는 인베딩이죠.

그런데 멀티모달 인베딩이라고 해서 멀티모달이라는 것은 모델리티가 두 개다, 여러 개다.

모델리티라는 것은 어떤 형태를 말합니다.

텍스트면 텍스트, 이미지면 이미지, 오디오는 오디오, 비디오는 비디오.

이런 걸 모델리티라고 하는데, 모델리티가 두 개 이상이면 멀티모달이라고 해요.

주로 많이 하는 게 텍스트랑 이미지랑 동시에 처리하는 건데, 그래서 우리가 멀티모달 인베딩이다

이러면 무슨 얘기냐면 텍스트하고 이미지를 비교할 수 있게 된다는 거예요.

예를 들면 우리가 사과라는 텍스트가 있고, 사과에 어떤 이미지가 있다.

그럼 얘네 두 개의 인베딩 값이 비슷해서 내가 사과라는 텍스트로 검색을 해도 이미지인 사과가 나오고, 이미지 사과를 검색을 해도 사과라는 텍스트가 나오고, 이렇게 할 수 있는 겁니다.

이게 되면 어마어마한 그런 게 되는데, 예를 들면 아까 이미지 검색하면 사람 얼굴을 집어넣으면 사람 얼굴이 나오지만, 예를 들어 머리 짧고 나이 많은 연극배우, 이렇게 하면 거기에 해당되는 얼굴을 다 찾아줄 수 있다.

그러면 어마어마한 그런 시대가 열리겠죠.

제가 요즘에 생각하고 있는 것 중에 하나가 유튜버들 보면 들고 다니는 카메라 같은 것들, 들고 다니면서 이렇게 영상을 찍잖아요.

여행 유튜버들을 보면.

그래서 그런 걸 하나 사가지고 어디 갈 때마다 그냥 다 찍어놓을까

이런 생각을 하거든요.

왜냐하면 요즘에 이런 검색 기술이 점점 발전해가지고 제 생각에 몇 년 기다리면, 그렇게 영상을 개인적으로 다 찍어놓으면, 그때 지나가면서 빨간색 차 본 거 있었는데, 외제차 뭐였지?

빨간 외제차 지나가면서 본 거, 이렇게 검색하면 검색이 되는 게 그렇게 오래 남았다고 생각이 되지 않습니다.

그래서 다 영상으로 찍어놓을까?

그러면 기억이라는 걸 할 필요가 없잖아요.

그래서 다 찍어놓을까?

이런 생각을 하고 있습니다.

근데 이제 보면은 그거 찍는 거 자체가 사실 되게 불편하기 때문에, 맨날 유튜버도 아니고 맨날 들고 다니는 거 찍는 것도 좀 웃기잖아요.

그래서 고민만 하고 있는데.

그래서 이제 멀티모델 임베딩은 이미지랑 텍스트를 비교할 수 있는 그런 임베딩을 멀티모델 임베딩이라고 하고, 그래서 이제 멀티모델 임베딩은 이미지랑 텍스트를 비교할 수 있는 그런 임베딩을 멀티모델 임베딩이라고 하고, 그러면은 우리가 이제 설명의 텍스트로 쓰지 않아도 이제 검색을 할 수 있다.

이런 장점이 생기는 거죠.

그래서 대표적인 게 이제 클립이라고 하는 모델인데요.

Contrastive, 이거는 대조라는 뜻입니다.

그래서 우리 아까 대조학습 얘기를 했는데, 비슷한 것끼리는 비슷하게, 다른 거는 다르게 하는 거가 이제 대조학습이고요.

그다음에 Language랑 Image랑 Pre-training, 사전학습을 시켜 놓는다.

이런 거.

그래서 이제 데이터를 어떻게 모으냐면, 주로 SNS에서 긁어모읍니다.

SNS에서 이제 사진을 긁어모으는데, 보통 이제 SNS에 사진을 올릴 때 사진만 올리는 게 아니라 사진 밑에 사진 설명을 같이 올리거든요.

그럼 이 사진하고 사진 설명이 딱 맞지는 않아요.

왜냐하면 예를 들면 바다 사진을 딱 찍어놓고 그리움..

이렇게 쓸 수 있거든요.

그러면은 이게 바다라고 써 놔야지, 그리움..

이러면.

근데 이제 그렇다고 하더라도 그런 데이터는 그냥 이렇게 쓰지 않거든요.

근데 그렇다고 하더라도 그런 데이터를 굉장히 많이 모으면, 우리가 바다를 볼 때 떠오르는 정서가 있단 말이에요.

뭐 그립다든지, 뭐 마음이 고요하다든지 뭐 이럴지.

뭐 바다를 보면서 웃김, 이렇게 쓰는 사람은 잘 없단 말이에요.

그러면은 어떤, 정확하게 바다라고는 돼 있지 않지만, 그런 바다와 연합된 어떤 감정을 표현하는 단어들.

그래서 간접적으로 우리가 어떤 바다라는 그 이미지에 포함되어 있는 여러 가지 뜻들을 같이 학습할 수 있는 거죠.

그래서 우리가 예를 들면 뭐 32개씩 한 배치에 집어넣어서 모델에 학습을 시키면, 예를 들면 여기 강아지 사진이 있고, 그 강아지 밑에 붙어있는 말이 있다.

그럼 이제 I1하고 T1하고 한 짝이고, I2하고 T2하고 한 짝이고, I3하고 T3하고 한 짝이고, 이렇게 되면은 I1과 T1이 이제 각각 인코더에 들어가서 인베딩이 나오는데, 그 두 개 인베딩은 최대한 비슷하도록 모델을 튜닝을 해주고요.

그 다음에 I1하고 T2는 같으면 안 되죠.

그래서 얘네 둘이 만약에 비슷하면은 최대한 멀리 떨어지게 그렇게 튜닝을 해줍니다.

그래서 배치마다 이걸 계속 반복을 해주면, 나중에는 이제 새로운 이미지를 넣어도 그 이미지랑 가장 비슷한 텍스트를 찾을 수가 있게 됩니다.

그래서 이제 이 클립을 이용을 해서 제로샷 분류를 할 수 있는데, 이때까지 우리가 분류를 하려면 굉장히 많은 이미지를 넣어서 학습을 시켰어야 되고, 우리 앞 시간에 해봤지만 미세 조정을 하려고 해도 그 미세 조정하는 데이터가 필요했는데, 이 클립 같은 모델이 발전하면은 아예 그런 과정 자체가 필요가 없게 됩니다.

왜냐하면 예를 들면 우리가 강아지랑 고양이랑 분류를 하고 싶어요.

그럼 어떻게 하면 되냐면, 이미지를 넣고 이거 강아지라는 텍스트와 가까운지, 우리가 강아지 이렇게 써놓고, 고양이 이렇게 텍스트로 써놓고, 이미지가 들어오면 이 이미지의 인베딩 값을 추출을 해요.

그 다음에 강아지의 인베딩도 있을 거고, 고양이의 인베딩도 있겠죠.

그럼 얘가 이쪽에 가까운지, 이쪽에 가까운지 비교해보면, 이 이미지가 들어갑니다.

이 이미지가 강아지인지 고양이인지 바로 분류할 수 있습니다.

그러니까 아예 미세 조정이라는 과정 자체가 필요가 없는 거죠.

우리가 그걸 말로 설명할 수 있다면 미세 조정을 할 필요가 없습니다.

예를 들면 강아지인데, 강아지를, 우리가 강아지를 귀여운 강아지와 못생긴 강아지로 분류하고 싶다.

그럼 원래는 귀여운 강아지의 이미지를 여러 개 모으고, 못생긴 강아지의 예를 여러 개 모으고, 이 경우에도 그냥 귀여운 강아지라는 표현이 있는 거죠.

그 텍스트의 인베딩, 못생긴 강아지라는 텍스트의 인베딩.

그래서 인베딩끼리 비교하면 이 강아지가 못생겼는지, 귀여운지를 우리가 분류할 수 있습니다.

이 과정에서 데이터가 필요 없는 거죠.

그냥 내가 분류하고 싶은 이미지를 말로만 설명할 수 있으면 됩니다.

심지어는 요즘에 어떤 것도 시도되냐면, 말로 설명하는 것도 어려울 때가 있잖아요.

대충 이렇게 생긴 건데, 말로 설명하려면 되게 어렵다.

그래서 손그림하고 사진하고 인베딩을 멀티모달로 만드는 그런 것들도 있어요.

그러니까 내가 강아지 본 게 있는데, 대충 귀가 이렇게 생겼고, 코가 약간 이렇게 생겼는데, 이렇게만 해도 이거를 인베딩으로 만들어서 그거와 가장 비슷한 사진을 찾을 수 있게, 이런 식으로도 발전하고 있습니다.

그러면 멀티모달이니까 사실 꼭 두 개만 되어야 될 필요도 없고, 세 개도 되겠죠.

내가 말도 하면서 손으로 대충 그리면 그거랑 제일 비슷한 사진을 만들 수 있어요.

그래서 그런 것들이 제로샷 분류고, 이렇게 발전하면 우리가 이번 수업에서 어제 오늘 배운 것은 다 필요 없습니다.

실제로 여기 허깅페이스에 가시면 모델 중에 여기 보면 제로샷 이미지 클래시피케이션이라고 해서 모델들이 몇 개 올라와 있거든요.

다 보시면 클립이라는 말이 붙어있는데, 클립 기반으로 해서 이렇게 제로샷으로 분류하는 모델들이 좀 있습니다.

그래서 클립 기반으로 해서 이렇게 제로샷으로 분류하는 모델들이 좀 있습니다.

실제로 452쪽으로 넘어가서 데이터set를 만드는데 들어가는 공을 보면 우리가 앞에서도 1번 소개드렸지만 이미지넷이라는 데이터 set인데 그래서 실제로 452쪽으로 넘어가서 데이터 set을 만드는데 들어가는 공을 보면 우리가 앞에서도 1번 소개드렸지만 이미지넷이라는 데이터 set인데 이 데이터 set은 2만 2.000 종의 사물에 대하여 1,400만 장의 데이터로 학습을 다 될 수 있습니다.

데이터로 학습을 우리가 데이터를 만든 건데 이 데이터를 만들기 위해서 참여한 누적 인원이 2만 5천 명이거든요

1,400만 장의 사진을 일일이 카테고리를 2만개 중에 하나로 분류를 한 거예요

여러분 생각해보시면 사진을 딱 보고 아 이거는 8769번 카테고리다

이게 할 수 있겠어요?

그러니까 이제 보통 대분류로 나누는 팀이 있고 거의 공장인거죠

이미지 분류하는 공장 대분류로 이거는 생물인지 무생물인지 무생물 팀은 이거는 가구인지 교통기관인지 교통기관 팀은 아 이거는 자동차인지 비행기인지 이렇게 하면은 나중에 아 이거는 3698번 카테고리 이런 식으로 어마어마하게 대규모 작업입니다

그래서 이거 이미지는 만든 사람이 페이페일리라고 중국계 미국인 스탠포드 교수님인데 이 데이터셋 하나 만든 걸로 그래서 이 데이터셋 하나 만든 걸로 상도 타고 그랬어요

연구는 일단 두 채 치고 데이터 만드는 것 자체가 너무 거대한 작업이었기 때문에 엄청 그걸로 공적을 인정받아서 되게 큰 상도 받고 이랬는데 근데 이제 우리가 문제가 예를 들어서 좋아요

이런거 할 수 있는데 예를 들어서 우리가 의료용 이미지를 이런걸 하겠다

그럼 이제 문제가 아주 심각해지죠

왜냐하면 데이터를 이만큼 모으는 것도 굉장히 어려운데 의료용 이미지를 종류별로 이렇게 나눠야 된다 엑스레이 사진을 나눠야 된다 그러면은 이렇게 이렇게 하면은 그러면 이제 알바가 아니라 의사를 2만 5천명을 동원을 해야 되거든요

그러면 여기 돈이 얼마가 들어가 있습니까

그 사람들한테 분류시키는 비용을 주려면 인건비가 어마어마하게 들어가겠죠

근데 이제 클립 같은 경우는 데이터를 늘리기가 되게 쉬워요

왜냐하면은 그냥 SNS에 사진 올라와 있는 거에 밑에 있는 말 가지고 하니까 따로 사람 손이 가는 게 없죠

그냥 인터넷 쫙 긁어보면 됩니다

실제로 이제 클립 지금은 더 커졌겠지만 논문에 보면은 학습에 사용한 데이터가 4억개입니다

그래서 이미지 넷보다 28배가 넘고요

근데 이 4억개의 이미지를 수집하기 위해서 뭐 돈 들인 게 있냐

없죠

그냥 인터넷 쫙 긁는 거는 자동으로 프로그램 짜서 돌리면 되거든요

인스타그램 같은 데 가면 뭐 4억개 정도면 인스타그램에 하루는 올라올걸요

인스타그램 같은 거 안 쓰시는 분들도 많겠지만 인스타그램 이런 데 가보면 뭐 밥 먹으면 사진 하나 찍고 어디 가면은 셀카 하나 찍고 그쵸 그쵸 사람들 쓰시는 분 옷 입으면 옷 입었다고 또 사진 찍고 강아지 귀여우면 강아지 귀엽다고 사진 찍고 애 귀여우면 애 귀엽다고 사진 찍고 뭐 사면 샀다고 사진 찍고 그게 다 이제 AI를 위한 약간 공짜 데이터 같은 거죠

그래서 쉽게 이제 데이터를 굉장히 쉽게 늘릴 수가 있습니다

그리고 이거를 제로샷으로 분류를 할 수 있는데 실제로 이제 클립 논문을 보면은 여러 가지 다양한 데이터셋으로 실험을 해 봤는데 27개의 데이터셋에서 16개의 데이터셋은 그냥 지도학습 우리가 아까 했던 것처럼 미세조정 하는 거보다 클립을 좀 더 잘 사용할 수 있는 것 같아요

클립에 제로샷 분류를 하는 게 오히려 성능이 더 높았다

그래서 이런 거는 이제 벌써 이미 미세조정 하는 거가 좀 필요 없어진 단계로 넘어갔습니다

근데 이제 클립의 문제는 뭐냐면 우리가 이제 보통 회사에서 이걸 쓰려고 하면 클립이 학습한 데이터가 대부분 SNS 이런 데 올라오는 사진이란 말이에요

그거랑 뭔가 좀 비슷한 그런 범주에 속하는 거는 잘 분류가 되는데 예를 들면 굉장히 전문적인 사진 예를 들면 위성 사진이라든지 그런 걸 SNS에 올리는 사람은 잘 없잖아요

위성 사진 막 우리 집 위성 사진 이러면서 올리는 경우는 잘 없단 말이에요

오늘도 우리 집은 멀쩡하고 뭐 약간 이러면서 아니면은 의료 이미지 이런 것도 다 뭐 병원 가서 엑스레이 찍은 다음에 SNS에 올리면서 아 부러진 뼈 이러면서 올리시는 경우는 잘 없잖아요

그죠?

뼈 부러진 거 정도는 올릴 수 있는데 약간 좀 부끄러운 병이라든지 아 나의 암 뭐 이러면서 이렇게 올리시진 않을 거 아니에요

암이 부끄럽진 않겠지만 그런 걸 일부러 굳이 SNS에 올리지 않는단 말이에요

그리고 이게 좀 의미가 있으려면은 올릴 때 의료 영상이랑 자기 차트랑 같이 올려야 쓸모가 있거든요

그래야 그걸 학습을 해서 새로운 영상이 와도 이게 대충 차트가 어떻게 될 거다

이런 걸 이제 예상할 수 있는데 뭐 자발적으로 올리면은 모르겠지만 자발적으로 올리는 사람도 없고 뭐 그러니까는 이런 거는 클립에 반영이 전혀 안 돼 있습니다

그래서 이런 좀 전문적인 데이터 추상적인 데이터는 지도학습이 아직까지는 여전히 더 높은 성능을 보이고 있는데 우리가 이제 어떤 뭐 이런 데이터를 이제 구할 방법을 찾아낸다

그러면은 위성 사진이라든지 뭐 의료 이미지라든지 아니면 이제 산업 영역에서 쓰는 그런 제조나 이런 데서 쓰는 이미지라든지 이런 거를 데이터셋에 이제 확보가 되면 그런 것도 이제 제로샵 분류가 될 수 있겠죠

제가 어디 가서 이제 뭐 강의 같은 거 할 때마다 이제 하는 얘긴데 가능하면은 이미지 같은 거를 최대한 이제 활용은 어차피 나중에 모델이 다 좋은 게 나와서 활용할 수 있으니까 데이터를 미리미리 좀 쌓아두시면 이미지가 뭐가 됐든 간에 찍을 수 있는 사진은 다 찍어서 어디 저장을 해 두시면은 나중에 아마 다 쓸 데가 있을 거다

이런 얘기를 자주 드리는데 그래서 데이터를 확보를 해 두시는 게 중요하다

이런 얘기를 할 수 있고요 그래서 이것도 그 트랜스포머스에 보면은 이제 아까 보시다시피 클립 모델도 다 올라와 있고 그래서 여러분들 해볼 수 있습니다

그래서 이것도 한번 해보시면 될 것 같고요 그래서 요즘에 이제 원래 그 채찌PT 같은 애들을 LLM이라고 하거든요

대형 언어 모델이라고 하는데 요즘에 이게 이제 발전해가지고 대형 멀티모델 모델로 이제 발전을 하고 있습니다

그래서 이미 채찌PT에 보면은 그 텍스트 처리 기능도 있고 이미지 처리하는 기능도 있고 음성 처리하는 기능도 있고 이제 비디오는 안 되는데 뭐 비디오도 조만간 되겠죠

그러니까 이제 뭐 말투 같은 것도 이제 시킬 수 있고 이게 지금 제가 하는 말이랑 언어랑 뭐 이런 거 음성이랑 이런 거 섞을 수도 있고요 그래서 이제 어 노래는 이제 저작권에 혹시 이제 위배될까 봐 못하게 막아놨는데 사실 이제 노래 같은 것도 뭐 이렇게 만들 수 있습니다

사실 점점 이제 이런 쪽으로 가고 있거든요

언어만 처리하는 게 아니라 하나의 모델이 텍스트 이미지 언어 뭐 이런 거 그다음 음성 이런 것들 이제 다 섞여서 학습하고 할 수 있는 이런 형태로 이제 발전을 하고 있고요 그래서 뭐 우리가 이제 비디오도 이제 하게 되면은 나중에 그냥 켜놓고 뭐 우리가 뭐 고쳐야 될 때 지금은 이제 저한테 이렇게 물어보시지만 핸드폰 이렇게 켜가지고 이거 여기 왜 이거 에러나 이렇게 물어보면 오 여기 임포트를 하셔야 된다

뭐 이렇게 얘기해주겠죠

이제 이런 식으로 발전을 하고 있다
