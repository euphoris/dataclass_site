# 이항 분류


## PyTorch와 Lightning
*   PyTorch 설치:
    *   홈페이지 참고 https://pytorch.org
    *   GPU 지원을 위해서는 별도로 CUDA 및 CuDNN 설치 필요
*   Lightning:
    *   PyTorch 프레임워크 위에서 동작하는 경량화된 래퍼 라이브러리
    *   복잡한 연구 코드와 모델 구현을 보다 깔끔하고 일관되게 작성할 수 있도록 도와줌
    *   실험을 쉽게 관리하며, 다양한 하드웨어 환경에서 효율적으로 훈련을 실행할 수 있는 기능을 제공
*   설치:
    ```python
    !pip install torch torchvision torchaudio lightning
    ```

## MNIST
*   1998년 미국 국립표준기술연구소 National Institute of Standards and Technology에서 개발
*   가로 28 × 세로 28 크기의 손으로 쓴 숫자(0~9) 흑백 이미지
*   6만개의 훈련용 이미지와 1만개의 테스트용 이미지로 구성
*   가장 유명한 컴퓨터 비전 연구용 데이터

## 데이터 다운로드
```python
from torchvision import datasets, transforms

train_dataset = datasets.MNIST(
    root='./data', # 데이터 저장 경로
    train=True, # 학습용 데이터셋
    download=True, # 데이터가 없으면 다운로드
    transform=transforms.ToTensor() # 텐서로 변환
)
```

## 배열과 텐서
*   수학적 개념:
    *   스칼라(scalar) → 0차원 값
    *   벡터(vector) → 1차원 값
    *   행렬(matrix) → 2차원 값
    *   텐서(tensor) → n차원 값
*   배열(array): 컴퓨터에서 동일한 크기의 원소들로 구성된 데이터 구조
*   텐서를 배열로 표현할 수 있음
*   Python에서 numpy는 array, pytorch는 tensor라는 표현을 사용

## 데이터 보기
```python
import numpy as np
from PIL import Image

x, y = train_dataset[0] # 0번 데이터
x = x.squeeze() # 텐서 모양 (1, 28, 28) -> (28, 28)
x = x.numpy() # 텐서 -> 배열
x = x * 255 # 값의 범위 [0, 1] -> [0, 255]
x = x.astype(np.uint8) # 자료형 float -> uint8
Image.fromarray(x) # 이미지 보기
```

## 이항 분류와 다항 분류
*   이항 분류: 둘 중에 하나로 분류하는 것
*   다항 분류: 셋 이상 중에 하나로 분류하는 것
*   현재 데이터셋은 10가지로 분류하는 다항 분류 데이터셋
*   이항 분류를 위해 0과 1만 뽑음
    ```python
    import torchutils
    binary_train_dataset = torchutils.BinarySubset(train_dataset, 0, 1)
    ```
*   이항 분류를 할 때는 항상 레이블이 0과 1이어야 함
*   BinarySubset으로 3과 4를 뽑으면 3 → 0, 4 → 1로 레이블이 바뀜

## 모형 정의
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import pytorch_lightning as pl
from torchmetrics.classification import BinaryAccuracy

class BinaryClassifier(pl.LightningModule):
    def __init__(self, input_size=784): # 입력 이미지의 크기: MNIST의 경우 28*28=784
        super().__init__()
        self.linear = nn.Linear(input_size, 1) # 입력값을 결합하여 한 개의 출력을 만듦
        self.accuracy = BinaryAccuracy() # 정확도(맞은 비율) 계산

    def forward(self, x): # 모형에 x가 입력되면
        x = x.view(-1, 784) # 입력 이미지를 1차원으로 평탄화 (N, 28, 28) → (N, 784)
        x = self.linear(x) # x를 linear 레이어에 입력하여 한 개의 출력을 만듦
        x = F.sigmoid(x) # 시그모이드 활성화 함수
        return x

    def training_step(self, batch, batch_idx): # 훈련의 한 단계
        images, labels = batch
        outputs = self(images).squeeze() # 이미지를 모형에 입력하여 출력을 얻음
        loss = F.binary_cross_entropy(outputs, labels.float()) # 손실 계산
        self.log('loss', loss) # 손실 기록
        accuracy = self.accuracy(outputs, labels) # 정확도 계산
        self.log('accuracy', accuracy) # 정확도 기록
        return loss

    def test_step(self, batch, batch_idx): # 테스트도 훈련과 동일하게
        return self.training_step(batch, batch_idx)

    def configure_optimizers(self): # 알고리즘 설정
        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)
        return optimizer
```

## Linear
*   선형 모형 `y = wx + b` 형태의 레이어
*   모든 입력에 대해 가중치 w를 곱하고, 편향 b를 더함
*   Fully Connected라고도 함
*   -∞ ~ +∞ 를 출력
*   이항 분류 문제이므로 0 또는 1로 출력을 해야 함 → 활성화 함수(activation function)

## 평탄화

## 시그모이드 함수 sigmoid function
*   출력층에서 0~1 사이의 출력을 표현하기 위해 사용
*   S자(sigma)를 닮은(oid)이라는 뜻
*   계단 함수는 미분불가능하므로 점진적 학습이 어려움
*   0~1 사이에서 매끄럽게 변하는 함수
*   출력은 확률로 해석 (예: 0.8 → 1일 확률이 80%)
*   통계에서는 로지스틱(logistic) 함수라고 부름

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$    

## 퀴즈

<iframe src="https://tally.so/embed/mVaePv?alignLeft=1&hideTitle=1&transparentBackground=1&dynamicHeight=1" loading="lazy" width="100%" height="1333" frameborder="0" marginheight="0" marginwidth="0" title="[CV] 이항 분류"></iframe>
{/*

## 이항 분류



그래서 우리가 이제 파이토치로 뭐 대단한 건 아니고 간단한 모델을 만들어서 대략 어떤 식으로 돌아가는지 한번 체험을 해 보는 시간을 가져보도록 하겠습니다

그래서 일단 파이토치를 설치를 해야 되는데요 원래는 이제 GPU 지원을 해야 되는데 사실 GPU를 지원을 하려면 GPU만 있으면 되는 게 아니고 CUDA랑 CUDNN이라는 거를 먼저 설치하고 그 다음에 파이토치를 깔아야 돼요

이거를 이제 다 깔려면 굉장히 일이 많기 때문에 이거는 생략을 하도록 하겠습니다

그러니까 파이토치를 쓰긴 하지만 우리는 GPU는 안 쓸 거예요

왜냐면 우리 실습에 GPU까지 사실 필요할 정도로 복잡한 모델을 하지는 않을 거라서 뭐 한 10분 돌리면 되거든요

GPU 지웠면 되면 1분 만에 돌아가겠지만 뭐 돌려놓고 쉬면 되죠

그래서 우리는 이제 CPU 버전으로 설치를 할 겁니다

여기 시작에서 어제 해 봤듯이 아나콘다 프롬프트를 켜시고요

아 지금 그림판에 있었죠

아나콘다 프롬프트 켜시는 방법은 시작에서 모든 앱 그 다음에 아나콘다 3

그 다음에 아나콘다 프롬프트 이 순서대로 켜시면 이제 이런 까만 창이 나오고요 여기다가 여기 설치 명령을 입력을 해주시면 됩니다

아 이미 설치가 되어 있군요

터치 오디오는 없어도 되긴 한데 안 해도 되는 거고요 아닌가 일단 뭐 설치 이렇게 하시면 됩니다 가끔 강사용이랑 여러분들 실습용이랑 세팅이 다른 경우가 있어 가지고 네 일단 이렇게 시작 모든 앱 아나콘다 3 아나콘다 프롬프트 하신 다음에 165쪽에 있는 이거 PIP 인스톨 이걸로 설치하시면 됩니다 .



```python copy
from torchvision import datasets, transforms

BATCH_SIZE = 32
train_dataset = datasets.MNIST(
    root='./data',  # 데이터 저장 경로
    train=True,  # 학습용 데이터셋
    download=True,  # 데이터가 없으면 다운로드
    transform=transforms.ToTensor()  # 텐서로 변환
)
```



```python copy
import numpy as np
from PIL import Image
x, y = train_dataset[0]  # 0번 데이터
```



```python copy
x.shape
```

:::info[output]
```
torch.Size([1, 28, 28])
```

:::



```python copy
x = x.squeeze()  # 텐서 모양 (1, 28, 28) -> (28, 28)
x = x.numpy()  # 텐서 -> 배열
x = x * 255  # 값의 범위 [0, 1] -> [0, 255]
x = x.astype(np.uint8)  # 자료형 float -> uint8
Image.fromarray(x)  # 이미지 보기
```

:::info[output]
```
<PIL.Image.Image image mode=L size=28x28>
```

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA90lEQVR4AWNgGMyAWUhIqK5jvdSy/9/rQe5kgTlWjs3KRiAYxHsyKfDzxYMgFiOIAALDvfwQBsO/pK8Mz97fhPLAlNDtvyBwbNv3j8jCUHbAnOy/f89yM2jPwiLJwMc4628UqgQTnPvp/0eGFAQXLg5lcO/764YuhuArf3y4IAfmfoQwlBX44e/fckkMYaiA7q6/f6dJ45IViP3zdzcuSQaGn39/OkBl4WEL4euFmLIwXDuETav6lKfAIPy1DYucRNFdUPCe9MOUE3e6CpI6FogZSEKrwbFyOIATQ5v5mkcgXV9auVGlwK4NDGRguL75b88HVDla8QBFF16ADQA8sQAAAABJRU5ErkJggg==)

:::



```python copy
y
```

:::info[output]
```
5
```

:::



```python copy
test_dataset = datasets.MNIST(
    root='./data',
    train=False, # 테스트용 데이터셋(이 부분만 달라짐)
    download=True,
    transform=transforms.ToTensor()
)
```


이거는 이제 그 이런 손글씨인데 손글씨를 숫자 0에서 9까지 총 6만 개를 모아 가지고 만드는 데이터고 그래서 훈련용 이미지 6만개 테스트용 이미지 1만개 이렇게 되어있습니다.

그래서 훈련용 이미지 6만개로 모델을 학습을 시켜서 1만개로 이제 테스트 하는 건데 가장 유명한 컴퓨터 비전 연구용 데이터고 수업용으로도 많이 사용합니다.

근데 옛날에는 이 정도도 98년에는 이 정도도 굉장히 어려운 문제여 가지고 이런걸로 연구를 했는데 사실 이제와서는 현재 시점에서는 이건 너무 쉬워 가지고 사실 그 M리스트로 연구하면 좋은 소리를 못 듣습니다.

이거는 학부생들도 한 수업 2시간 정도 들으면 99.9% 정확하게 분류하는 그런 문제거든요.

굉장히 이제는 이정도는 쉽다

그렇지만은 우리가 이제 수업용으로는 또 너무 어려운 데이터를 처음부터 하면 어려우니까 일단 쉬운 거걸로 해보도록 하겠습니다.

그래서 이제 요 데이터셋은 파이토치에 내장이 되어 있어 가지고 파이토치에 요거 코드는 그냥 복사해서 붙여 넣으시면 되는데 이렇게 하면은 우리 작업용 폴더에 .slash 데이터 폴더에 자동으로 다운로드를 받아줍니다.

그래서 우리가 이제 주피터 노트북을 들어가서 이렇게 실행을 해주시면은 지금은 우리가 데이터가 없죠

그래서 조금 기다리시면은 자기가 다운로드를 시도를 합니다.

그래서 여기 첫번째 사이트에서 다운로드가 안되니까 두 번째 사이트에 가서 다운로드를 하죠

이렇게 그래프가 올라가죠

다운로드를 걸어 주시면 되겠습니다.

다운로드 받는 파일이 4개라서 요거 좀 기다리셔야 되요

참고로 이거 메이 SPно호 compared to이 데이터 자 이제 저는 다운로드가 다 끝났고 지금 우리가 하고 있는 데는 167쪽입니다

그래서 이제 데이터를 하나 보면 168쪽으로 넘어가서 여기 이제 트레인 데이터셋에 보면 데이터가 6만개가 들어있는데 그 중에 0번을 꺼내가지고 그러면 X랑 Y가 있겠죠

그래서 X에는 이미지가 들어있고 Y에는 이 이미지의 숫자가 0인지 1인지 2인지 3인지 4인지 뭐 이렇게 들어있어요

그래서 이 X를 일단 여기까지만 한 다음에 X.shape 이렇게 보면 모양이 1, 28, 28 이렇게 돼 있는데 이거는 이제 이미지의 가로, 세로 사이즈고요 그 다음에 맨 앞에 있는 1은 뭐냐면 채널의 개수입니다

채널의...

이제 흑백이라서 채널이 하나밖에 없는 거죠

칼라였으면 여기가 3이었겠죠

그래서 이제 이거를 앞에 있는 1은 필요가 없으니까 스퀴즈를 하면 스퀴즈가 이제 꽉 쥐어짜는 겁니다

그래서 꽉 쥐어짜서 모양을 28 곱하기 28 가로, 세로로 바꿔주고 그 다음에 이제 텐서에서 배열로 이렇게 돼 있는데 텐서랑 배열은 사실 뭐 똑같은 얘기예요

똑같은 얘기인데 보통은 우리가 이제 GPU에서 돌아가는 목적으로 만든 자료구조는 텐서라고 하고 텐서랑 배열은 사실 뭐 똑같은 얘기예요

텐서랑 배열은 사실 뭐 똑같은 얘기예요

텐서랑 배열은 사실 뭐 똑같은 얘기예요

그 다음에 우리가 CPU에서 쓰는 목적으로 만든 자료구조는 배열이라고 합니다

그러니까 뭐 사실 실질적으로는 똑같아요

약간 이제 정확한 의미는 얘기를 드리면 우리가 이제 텐서라는 말도 쓰고 텐서 배열이라는 말도 쓰는데 정확한 의미는 뭐냐면 이제 우리가 수학에서 보면은 스칼라가 있고 벡터가 있고 행렬이 있고 이렇게 되는데 스칼라라는 거는 어떤 1차원수예요 1차원수고 그 다음에 벡터가 있고 벡터가 있고 벡터가 있고 벡터가 있고 벡터가 있고 벡터가 있고 벡터가 있고 벡터가 있고 벡터가 있고 벡터가 있고 벡터가 있고 벡터가 있고 벡터라는 거는 2차원 정확하게 말하면 0차원수입니다

0차원수고 벡터라는 거는 1차원수 벡터라는 건 숫자가 한 줄로 쭉 있으면 벡터라고 하죠

그래서 수들이 한 줄로 있는 게 벡터가 됩니다

그 다음에 우리가 고등학교 때 배우셨겠지만 행렬은 2차원수인데 수들이 이제 그 표현력이 표현력이 표현력이 2차원수 efecto 형태로 이상 텐서는 수들이 텐서라고 해도 별건 아니고 그냥 얘네를 포함해서 더 일반화 시킨 거다

이렇게 생각하시면 됩니다

그래서 어떤 수들이 이렇게 입체로 이렇게 이렇게 돼 있는 거에요

예를 들면 우리가 그림이 있으면 그림은 한 장의 그림은 행렬로 표현할 수 있거든요

근데 그림이 칼라가 되면 사실 3차원이 필요하죠

왜냐면 3장의 RGB 3장이 필요하니까 근데 그림이 여러 장 있다

그럼

RGB가 다시 이제 3차원인데 그거가 여러 장 있으니까 4차원이 필요합니다

수들을 4차원으로 배열을 해야 되거든요

그러면 텐서가 필요하겠죠

그래서 이제 원래는 원래 어원은 그런 얘기고 그다음에 배열이라는 거는 사실 이제 여기까지는 수학용어고 위에는 수학용어고 배열은 컴퓨터용어예요

컴퓨터에서 수들의 배치를 수뿐만 아니라 어떤 데이터를 모은 거 그래서 사실 텐서를 컴퓨터에서 나타내면 그냥 배열이 되겠죠

그리고 배열을 수학적으로 바꿔서 생각하면 텐서가 되고 두 개가 뭐 그렇게 다른 말은 아닙니다

그냥 분야 수학용어 컴퓨터용어 이렇게 생각하시면 돼요

근데 이제 여기서는 현실적으로 무슨 의미냐면 우리가 텐서라는 거를 딥러닝 하려고 파이토치에서 텐서를 만들어 쓰기 때문에 사실은 텐서라고 하지만 사실 그냥 배열입니다

근데 이제 약간 GPU용으로 만들어 놓은 배열이다

이렇게 생각하시면 되고 여기서 이제 배열이라고 하지만 사실 실질은 약간 CPU용으로 만들어 놓은 거다

이렇게 생각하시면 돼요

그래서 실제 뜻하고 여기서 말하는 텐서 배열하고는 다르게 생각하시면 됩니다

그래서 텐서에서 배열로 바꾼다 라고 되어 있는데 X.NUMP 이렇게 하면 GPU용으로 되어 있는 거를 CPU용으로 바꾼다

이렇게 생각하시면 됩니다

그래서 X가 원래 0에서 1 범위로 되어 있는데 여기다 255를 곱하면 0에서 255 범위가 되겠죠

그다음에 이제 자료가 실수형으로 되어 있는데 우리가 이때까지 Uint8을 썼기 때문에 Uint8로 변환을 해줍니다

그래서 이미지 보기 하면 5라는 모양인 거를 볼게요

그래서 뭐 복잡하게 되어 있지만 사실은 그냥 자료형을 바꿔준 거예요 그림으로 그려 보려고 그다음에 Y를 보면 Y가 얼마냐 Y를 보면 Y도 5라고 되어 있습니다

그래서 이 5의 이미지가 들어오면 이게 숫자 5다

이렇게 할 수 있도록 데이터가 만들어진 겁니다 맨 앞에 가시면 여기 0번 데이터 이렇게 되어 있는데 1번으로 바꿔서 보시면 1번 데이터는 숫자 0 모양이고 실제로도 0입니다

이런 식으로 되어 있는 거를 보실 수가 있습니다

그다음에 우리가 데이터를 보면 여기 트레인 데이터셋이 있고 그다음에 우리가 지금 여기 트레인은 True 이렇게 했는데 여기를 False로 하면 테스트 데이터셋을 받을 수 있습니다

그래서 트레인 데이터셋이 6만 개 그래서 트레인 데이터셋이 6만 개 테스트 데이터셋이 10,000개 이렇게 되어 있는데 왜 데이터를 트레인 데이터, 테스트 데이터로 나눠 놓느냐 하면 우리가 모델을 학습을 시킬 때 과소, 과대적합 이렇게 얘기하는데 과소적합은 예를 들면 여기 동그라미랑 X를 구별해야 되는데 우리가 경계선을 그어야 되는데 이런 식으로 그리면 좀 합리적이에요

그래서 적절하게 맞춰서 해야 되는데 너무 단순하게 그리면 이런 데서 삐져나가는 거죠

잘 안 맞습니다

그렇다고 여기 이런 이거는 나의 고장이 처럼 너무 단순한 역할을 하는 것 같아 역시 완전히 다르게 각국 때야 겁이나

괜찮 subscription 원래 좀 5년을 내린 heavy 양계에서 가장까지 근본적인 colours이 그래가지고 이런 것을 연결해서 배경을 바라� libre rains kazpera adulteratedあと 그 connection solve packs 압력 예외적 서류까지 예외적 서류만 보고 업ivities 어떻게 합니까

게임.atswelt 예외적인 사례가 있는데 물론 이것만 보고 예외적인 사례라고ены가 그래서 언더피팅 해도 안되고 오버피팅 해도 안되고 이게 뭔가 통계적인 분야에서는 항상 다 생기는 문제입니다

너무 단순해도 안되고 너무 복잡해도 안되고 그래서 우리가 이제 뭐 예를 들면 주식 이런거 보면 제가 이제 제일 세상에서 좀 별로 안좋아하는 직업이 정치나 경제 사안을 두고 코멘트 하시는 분들을 그렇게 썩 좋아하지 않는데 왜 안좋아하냐면 항상 이분들이 그 뒷북을 치시거든요

무슨 일이 벌어지고 나면 이번에 미국에서 트럼프가 당선된 것은 이런 배경이고 또 경제 코멘트 하시는 분들은 이번에 그 한국 주가가 떨어진 것은 뭐 이런 이런 요인들 뒤에 얘기해 주면 어떻게 해요

미리 얘기해줘요

근데 사실 이분들의 고충도 알 만한게 그 미리 예측해서 만든다는게 쉽지가 않습니다

근데 생각해보면 우리가 인과관계라는 것은 인과관계라는 것은 원인이 있으면 항상 결과가 있는 거라서 우리가 원인을 알면 결과를 알 수가 있거든요

예를 들면 우리가 유리잔을 들고 있다가 놓으면 바닥에 떨어져서 깨지겠죠

그러면은 유리잔을 어떤 유리잔 바닥에 깨져 있어서 이거 누가 떨어트렸어

이렇게 얘기하잖아요

왜냐면 우리가 원인을 알면 결과를 알기 때문에 결과로부터 원인을 거슬러 올라갈 수 있다는 얘기는 반대로 예측도 된다는 얘기에요

누가 잔을 떨어트리려고 하면 어 떨어트리지 마라 깨진다

이렇게 얘기할 거 아니에요

근데 보통 코멘트 하시는 분들이 보면은 정치나 경제 이런 사안에서 결과를 가지고 아 그거는 원인이 이거 이거 때문입니다 라고 말을 한단 말이에요

우리가 유리잔 깰 때와 마찬가지로 이 원인을 봤을 때 이 결과가 예상이 되느냐

안 된다는 거죠

예상이 안 됩니다

그래서 주식 같은 거는 사후적으로 아 이래서 떨어졌다 저래서 떨어졌다

이렇게 말씀하시는 분들은 많은데 그분들이 투자를 잘 하시냐 하면 잘 못해요 바꿔 말하면 그 설명이 우리가 지금 말하는 게 우리가 들으면 말이 그럴 듯하게 들리는 거지 실제로 사실이냐 하면은 아닐 가능성이 크다는 거죠

대부분은 그렇습니다

그래서 이제 약간 사회과학 이런 분야가 어려운 게 뭐냐면 사실 이제 우리가 공학도 어렵지만은 공학은 그래도 변수가 통제가 되거든요

사회현상은 변수 통제가 잘 안 돼요

그러니까는 어떤 하나의 현상도 예를 들면 주가 같은 하나의 현상도 거기에 영향을 미치는 변수가 수백 수천 개가 있고 걔네가 굉장히 복잡하게 얽혀 있는데 우리가 공학이나 자연과학에서처럼 그 변인을 통제할 수가 있는 게 아니기 때문에 사실 인과관계를 알기가 굉장히 어렵습니다

그리고 뭐 전혀 모르는 건 아니지만 인과관계 하나를 안다고 해서 우리가 이제 실질적으로 예측을 하는 데는 그렇게 큰 도움이 안 돼요

왜냐면 변수가 뭐 백 개 천 개 이렇게 있는데 그중에 이제 변수 하나를 알아 가지고 뭐 얼마나 예측에 도움이 되겠어요

예를 들면은 우리가 이제 사람 키 같은 경우는 키에 영향을 미치는 유전자가 한 만 개쯤 있거든요

그러면은 우리 유전자 한 개가 있다면은 그러면은 우리가 유전자 한 개가 있다면은 그 사람 키 하나에 대해서 알아봤자 나머지 9999를 모르면 이 사람 키가 얼마나 클지 모르는 거죠

그래서 우리가 이제 생명성상 보면은 대체로 암기 과목들 있잖아요

생물학 과학에서도 보면은 대체로 물리는 외울 게 별로 없는데 생물학 외울 게 많잖아요

그리고 사회과학으로 가면 외울 거 투성이고 왜 외우냐

단순 원리로 환원이 안 되기 때문에 너무 복잡해서 그래서 이제 보면은 그런 설명들이 대부분 끼워 맞추기란 말이에요

그래서 이번에는 뭐 말로 들으면 그럴싸하지만 당연히 갖다 놓고 결과를 놓고 끼워 맞추니까 당연히 그럴싸한데 그럼 그 틀을 가지고 다음번에 예측을 하면 예측이 되느냐 예측이 안 되는 거죠

그러면은 우리가 이제 사실 뭐 우리가 바꿀 수 있는 건 항상 미래 밖에 없잖아요

과거는 우리가 바꿀 수 없단 말이에요

과거에 대해서 사실 뭐 이렇쿵 저렇쿵 하는 게 무슨 의미가 있겠습니까

별로 의미가 없는데 사실 이제 인간은 항상 설명을 원하기 때문에 우리의 약간 본성이 설명을 원하거든요

예를 들면은 그 사람들한테 소한테 음악을 들려주면 우유가 맛이 좋아진대

이런 얘기를 들려주면 사람들이 뭐라고 질문하냐면 항상 왜 라고 물어보거든요

증거 있어 라고 물어본 사람 잘 없습니다

왜 를 물어봐요

근데 왜 라는 건 설명이죠

근데 설명은 그럴듯한 설명이 있을 수도 있고 없을 수도 있는데 그럴듯한 설명이 있다고 해서 증거가 없는 거를 물어봐요.

증거가 있는 걸로 바꿔주지는 않아요

증거가 없으면 아무리 설명이 그럴듯해도 그냥 설명일 뿐이죠

근데 우리는 약간 본능적으로 설명에 이끌립니다

설명을 좋아해요

사람은 그니까 예측에 도움이 안 돼도 자꾸 어떤 사고 설명을 원하는 그런 게 있는데 어쨌든 머신러닝에서는 우리가 이제 결국 이걸로 하려는 건 뭔가 설명을 하려는 게 아니고 지나간 일을 설명하려는게 아니라 예측을 하거나 아니면 더 낫고 할 수 있는 것들이 있습니다

그니까 이제 뭔가 설명을 하려는 것들만 설명을 하려고 하는 게 아니고 지나간 일을 설명하려는 게 아니라 예측을 하거나 아니면 더 낫는 거를 설명하려고 하는 것도 아니고요 나가서 제어를 하고 싶은 거죠

컨트롤을 하고 싶은 거죠

그래서 설명 예측 제어가 있으면 사실 우리가 이제 공학적인 입장에서 보면 뭐든지 제어가 중요합니다

결과가 좋아야지 뭐 그 결과가 왜 좋은지 알게 뭡니까 예측도 하면 좋지만 사실은 제어만 잘 된다고 하면 예측은 안 돼도 그만이에요

예를 들어 어떤 제품인데 품질이 잘 나오면 그만이고 납기를 잘 맞추면 그 뿐이지

납기를 예측을 한다 예측 못하면 어떻습니까

잘 맞추는데 그러니까 항상 제어가 중요하고 그 다음에 예측이 중요하고 설명은 나중 문제다

이런 얘기를 좀 들을 수 있고요 어쨌든 우리가 이제 과대적합도 안 되고 과소적합도 안 되는데 문제는 우리가 이게 과대적합인지 과소적합인지 알 수 있는 방법이 없어요

알 수 있는 유일한 방법은 예측을 해 보는 방법밖에 없습니다

그래서 어떻게 하냐면 데이터를 미리 분할을 해놔요

우리가 미래를 예측을 하고 싶은데 미래는 이제 돼 봐야 아는 거니까 그냥 데이터 자체를 일부를 유보를 해 놓습니다

그래서 얘는 이제 미래의 역할을 하는 거예요 못 본 데이터인 거죠

그래서 트레이닝 데이터를 가지고 모델을 학습을 시킵니다

그리고 테스트 데이터 이게 이제 미래의 데이터 역할을 하는 거예요

그래서 얘는 본 적이 없는 새로운 데이터입니다

이걸 맞춰야 되는 거예요 본 적이 있는 데이터를 끼워 맞춰서 하는 거는 다 할 수 있으니까 본 적이 없는 거를 맞춰야 되는 거고 그래서 이제 데이터가 항상 트레이닝 데이터와 테스트 데이터로 분리돼 있고 이 두 가지가 섞이면 안 됩니다

미래를 알고 나서 거기에 대해서 이렇쿵저렇쿵 얘기하는 거는 의미가 없어요

그래서 사실 원칙적으로는 테스트 데이터는 딱 한 번만 사용하고 그 다음 버려야 돼요

두 번 사용하시면 안 됩니다

왜냐하면 한 번 사용하고 결과를 본 다음에 성능이 안 좋네

하고 모델을 고쳐 가지고 다시 돌리면 사실 직접적으로 모델에 반영이 된 건 아니지만 간접적으로 모델에 테스트 데이터가 반영이 된 거죠

결과를 다 알고 모델을 튜닝을 했기 때문에 원칙적으로는 안 되는데 사실 뭐 그렇게 썩 잘 느껴지는 원칙은 아니에요

왜냐면 데이터 만드는 게 다 품이 많이 드는 일이라서 별로 그렇게까지 열심히 하진 않는데 원칙적으로는 그렇습니다

사실 이제 앱리스트 데이터 같은 경우에 잘 안 쓰는 이유가 또 이제 뭐냐 하면 이게 한 지난 30년간 연구가 너무 많이 돼 있어서 어떤 문제가 있냐면 논문은 나와요

앱리스트로 해석해 왔더니 컴퓨터 비전을 이렇게 하면 잘하더라

근데 문제는 뭐냐면 우리가 이 테스트 데이터도 30년 동안 어마어마하게 많이 봤단 말이에요

그래서 문제가 뭐냐면 앱리스트 데이터에서는 되게 잘 돌아가는데 다른 데이터셋에서는 안 돌아가는 모델들이 되게 많습니다

왜냐면 그 모델을 만들 때 이미 앱리스트를 여러 번 해봐서 앱리스트에 대한 어떤 경험이 무의식적으로 많이 쌓여 있는 상태에서 모델을 만드니까 자꾸 앱리스트에만 맞는 어떤 데이터 모델을 자꾸 만드는 거야

그래서 사실은 이제 연구조차도 앱리스트로 하면 더 이상 안 되는 거죠

이미 약간 이런 원칙을 좀 유배를 했기 때문에 그래서 데이터를 과적합인지 아닌지를 측정하는 거는 결국 테스트 데이터로 평가를 한다 모델을 학습시킨 적이 없는 그런 데이터로 한다

이렇게 얘기를 할 수 있고요 그래서 이 원칙이 되게 중요한데 여러분들도 일상에서 내가 뭘 정말로 아는지 모르는지 어떤 설명이 맞는지 아닌지를 알고 싶으시면 예측을 해 보시면 돼요

그래서 만약에 예측을 해보면 되죠?

예측을 해보시면 돼요.

그래서 만약에 예측을 해보시면 돼요.

만약에 예측을 해보시면 돼요.

그러면 사실 아는 게 아닙니다

예를 들면 우리가 저출산 이런 걸 두고 저출산의 원인은 이거야 저거야

이렇게 얘기하시는 분들이 있거든요

근데 사실은 우리가 저출산에 대해서 아는 게 별로 없어요

생각보다 그걸 어떻게 아느냐

저출산 예측이 안 됩니다

내년도 출산율이 얼마일까 아무도 몰라요

왠지를 모릅니다

근데 이제 출산율이 나오면 그제서야 사람들이 올해 출산율이 높아진 것은 뭐 이런 것 때문이고 그럼 미리 예측이 됐어야지 예측이 안 돼요 예측이 안 된다는 건 우리가 원인을 모른다는 거죠 원인을 모르니까 다 아는데 예를 들면 우리가 모든 변수를 다 알거든요

알려진 변수들이 소득도 있고 교육 수준도 있고 부동산 가격도 있고 변수 수십 개를 아는데 그 수십 개 변수 우리 다 아는 거거든요

근데도 내년 출산율을 몰라요

그러니까 사실 어느 정도 영향은 있습니다

어느 정도 영향이 있는데 그 영향이 되게 미묘하기 때문에 사실 변수가 수백 개 수천 개가 있는 거죠

그래서 우리가 아는 거를 다 해도 출산율을 정확하게 예측을 못 합니다

잘 모른다는 얘기예요

그래서 잘 모르면 그냥 가만히 있어야죠

근데 이제 사람들은 항상 왜 를 궁금해하기 때문에 이건 이것 때문이고 그래서 이제 그런 관점에서 보시면 된다

이런 거고 그래서 이제 테스트 데이터를 받으시면 되는데 테스트 데이터는 아까랑 받을 때 똑같이 받으시면 되는데요 여기 트레인만 변수를 false로 바꾸시면 됩니다

그래서 이제 true는 맞다

이런 얘기고 false는 아니다

이런 얘기인데 트레인 데이터가 아니라는 건 얘기죠.

그러니까 테스트용 데이터셋을 받으라는 얘기죠

172쪽 그래서 이것도 이제 받아주시면 되는데 이거는 데이터가 좀 개수가 적기 때문에 더 금방 받을 겁니다

같이 받았나?

다운로드는 이때 같이 받았나 보네요

로딩만 그냥 하니까 바로 됩니다

자 그 다음에 이 데이터에는 0부터 9까지 있는데 우리가 처음부터 0부터 9까지를 다 분류하는 거를 하려면 조금 어렵기 때문에 갑자기 약간 코드가 복잡한 코드가 나오는데 우리가 데이터를 2개만 추려 가지고 일단 학습시켜 보고 예측하고 이런 걸 해 볼 겁니다


```python copy
from torch.utils.data import Dataset

class BinaryMNIST(Dataset):
    def __init__(self, dataset, class_1, class_2):
        self.dataset = dataset
        self.indices = [i for i, (_, label) in enumerate(dataset) 
                        if label in [class_1, class_2]] # 원하는 클래스의 인덱스만
        self.class_map = {class_1: 0, class_2: 1} # class_1은 0으로, class_2는 1로
        
    def __len__(self):
        return len(self.indices)
    
    def __getitem__(self, idx):
        image, label = self.dataset[self.indices[idx]] # idx번째 데이터
        return image, torch.tensor(self.class_map[label]) # 레이블 바꾸기
        # label.item() -> label
```



```python copy
binary_train_dataset = BinaryMNIST(train_dataset, 0, 1)
binary_test_dataset = BinaryMNIST(test_dataset, 0, 1)
```




그래서 이제 이 코드는 코드가 좀 복잡한데 원래 데이터를 중에서 2개만 추려내는 그런 코드예요

그래서 이제 어떻게 하는 거냐면 레이블을 봐 가지고 레이블이 클래스 1이나 클래스 2에 포함이 되면 그 데이터만 뽑아내는 겁니다

그래서 예를 들면 내가 3하고 8만 가지고 해 보고 싶다

그러면 3하고 8에 해당되는 것만 뽑아내고 그 다음에 번호를 다시 붙여요

3은 0으로 8은 1로 이렇게 다시 붙입니다

왜 다시 붙이냐면 우리가 이렇게 둘 중에서 둘 중의 하나로 분류하는 것을 이항분류 라고 하는데 이항분류를 할 때는 번호를 무조건 원래 뭐였든지 간에 0하고 1로 붙입니다

왜 그런지는 좀 뒤에 얘기 드리고요 그래서 내가 3하고 8을 구분하고 싶다고 해도 번호는 0하고 1로 붙이셔야 돼요

그래서 3은 0으로 8은 1로 아니면 반대로 하셔도 돼요

8을 0으로 했으면 3을 1로 그래서 이 코드는 이제 전체 데이터에서 두 가지만 추려내는 겁니다

그래서 이거는 이제 그냥 붙여 넣으시면 되고요

여기 들여쓰기만 좀 잘 맞춰 주시면 됩니다

여기 def는 한 칸씩 들여 써 주시고 def 안쪽은 두 칸 들여 써 주시고 이건 제가 미리 짜놨으니까 그냥 하시면 되겠어요

그래서 우리가 이 BinaryMNist 이거는 기존의 데이터셋이 있는 것을 우리가 어떤 특정 그 중에 몇 개를 뽑아 두 개만 뽑아 주는 건데 그래서 이제 174쪽으로 넘어가서 트레인 데이터셋에서 0하고 1 두 개만 뽑아서 우리가 이제 사용을 해 볼 겁니다

그래서 0은 그대로 0이고 1은 그대로 1이고 이렇게 돼요

이거는 사실 뭘 하셔도 상관 없는데 3하고 4로 하고 싶다

그럼 위에랑 아래랑 숫자는 똑같이 맞춰 주셔야 됩니다

왜냐하면 위에 게 트라인 데이터고 밑에 게 테스트 데이터니까 두 개가 종류는 맞아야겠죠

만약에 이렇게 하면 3이 0번이 되고 4가 1번이 되고 이렇게 됩니다

그리고 나머지는 다 빠지고 우리는 일단 0하고 1만 한번 해 보도록 할게요



```python copy
import torch
import torch.nn as nn

class BinaryClassifier(nn.Module):
    def __init__(self, input_size=784):  # MNIST의 경우 28*28=784
        super().__init__()
        self.flatten = nn.Flatten()  # 입력 이미지를 1차원으로 평탄화
        self.linear = nn.Linear(input_size, 1)  # 한 개의 출력 뉴런
        self.sigmoid = nn.Sigmoid()  # 시그모이드 활성화 함수

    def forward(self, x): # 모형에 x가 입력되면
        x = self.flatten(x) # 위에서 정의한 레이어들을 
        x = self.linear(x) # 순서대로
        x = self.sigmoid(x) # 통과하여
        return x # 결과를 반환
model = BinaryClassifier()

```




그래서 이거는 이제 들여쓰기를 조금 주의를 해 주시고 파이토치를 이용해서 모델을 정의를 하는 부분인데 여기서 이제 코드를 좀 설명을 드리면 일단 클래스라는 말이 나오는데요

이거는 이제 프로그램이 파이썬을 잘 모르시는 분들 위해서 얘기를 드리면 일단 클래스라는 건 뭐냐면은 우리 왜 폼은 일시적이지만 클래스는요?

클래스는 뭐죠?

클래스는 영원하다

뭐 이런 말이 있거든요

뭡니까

어떤 선수가 지금은 좀 잘 못 할 수는 있지만 어떤 수준이 높은 선수면 뭐 그거는 계속 간다

이런 얘기죠

여기서 이제 클래스라는 거는 여기서 이제 유형이라는 뜻인데 우리가 프로그래밍에서 다루는 어떤 대상들 그 대상들마다 각각 어떤 특징을 공유를 하고 있습니다

그래서 우리가 이제 배열이다 배열이라고 하면은 숫자들이 가로 세로로 배치가 돼 있다던가

뭐 그런 특징들이 있고요 딥러닝 모델이라는 것도 하나의 어떤 특징을 공유하는 클래스기 때문에 우리가 이제 새로운 클래스를 정의할 수 있는데 Binary Classifier라는 이름의 클래스를 정의를 할 거예요

요 클래스는 n.module에 속하는 그러한 클래스입니다

예를 들면 우리가 사람도 어떤 클래스라고 할 수 있는데 사람은 동물에 속하는 클래스죠

동물이라는 더 큰 클래스가 있고 그 안에 사람이라는 좀 더 제한적인 클래스가 있는데 그 안에 사람이라는 좀 더 제한적인 클래스가 있는데 여기서 우리가 이제 nn.module이라는 클래스에 속하는 Binary Classifier라는 클래스를 정의를 하려는 거예요

nn.module은 뭐냐

딥러닝 모델을 nn.module이라고 얘기합니다

딥러닝 모델의 일종을 한 종류를 정의를 하는 거죠

그래서 이제 이렇게 정의할 때 최소한 두 개의 어떤 함수를 포함을 하는데요

첫 번째로 여기 init이라고 하는 함수는 Initialize 이런 클래스에 있는 함수를 정의하는 거죠

그래서 이제 인풋 사이즈에 해당되는 무언가를 만들어 주는 역할을 합니다

그래서 우리가 이제 값을 하나 봤는데 인풋 사이즈라는 걸 봐둬요

이게 784로 돼 있는데 왜 784냐면 Mlist가 가로 세로가 28이라서 28 곱하기 28 하면 784거든요

그래서 이제 입력 크기는 784 이렇게 했고 셀프는 뭐냐면 Binary Classifier에 해당되는 거를 우리가 나중에 만들어볼 거라고 하면 우리가 나중에 만들 건데 이런 식으로 해서 만들어 줄 거거든요

그러면 모델이라는 거는 Binary Classifier죠

모델이 자기 스스로를 인식을 할 수가 있어야 되거든요

모델이 자기 스스로를 뭐라고 인식하게 할 거냐

그래서 우리가 self라고 스스로를 부르게 해 줄 겁니다

그래서 모델이 자기가 생각할 때 자기는 self예요

그래서 super.init 이거는 뭐냐면 얘가 nn.module의 일종이니까 얘의 init은 이건데 nn.module의 init도 있겠죠

그래서 super.init 이렇게 하면 nn.module의 초기화, initialize를 해 주고 그 다음에 나의 초기화를 해줍니다

그래서 보면 self.platen self는 그냥 나라고 번역하시면 돼요

나.은 의 로 번역하시면 되고 나의 플래튼은 뭐냐면 플래튼인데 플래튼이 하는 건 뭐냐면 입력 이미지를 1차원으로 평탄화 합니다 1차원으로 평탄화 한다는 얘기는 뭐냐면 우리가 이제 기본적으로 이미지라는 것이 이렇게 가로 세로가 있는데 결국에 딥러닝이라는 것은 일종의 행렬 계산이거든요

여기다 어떤 숫자를 곱해 줘야 되는데 이렇게 가로 세로가 있으면 숫자 곱하기가 좀 불편해요

그래서 요거를 한 줄씩 썰어 가지고 이렇게 한 줄로 붙여 줍니다

그러면은 어떤 그림을 벡터로 바꿀 수가 있어요.

이렇게 숫자들이 한 줄로 돼 있으면 벡터니까 그러면은 그냥 벡터 연산을 하면 되기 때문에 그 연산을 하기가 좀 편해요

그래서 여기서 플래튼을 해 가지고 1차원으로 평탄화를 해서 연산하기 쉬운 형태로 바꿔주는 역할을 하는 것이 self.platen이고 그 다음에 self.linear는 뭐냐면 우리가 이제 어떤 이미지가 x로 이렇게 들어오면 얘가 이제 플래튼으로 들어가는데 그러면은 여기서 나온 결과를 x에 다시 덮었으면은 그게 어떤 벡터 연산이 되는 거죠.

벡터가 되겠죠

벡터가 있으면 x라는 벡터가 있으면 여기다가 우리가 이제 가중치를 곱해 주면 되는데 그 가중치를 곱해 주고 뭔가 절편을 더해줍니다

그럼 이건 이제 1차 함수죠

그래서 이런 1차 함수를 다른 말로 선형함수라고도 하거든요

왜냐하면 1차 함수를 그림으로 그리면 이렇게 직선이 되기 때문에 그래서 nn.linear는 그런 함수를 하나 만들어라

이런 얘기입니다

그런 함수를 하나 만들고 그래서 이제 여기 y가 나와야 되는데 y가 이제 기본적으로 벡터 형태이기 때문에 y의 길이가 있습니다

y의 길이를 얼마로 할 거냐

근데 우리는 지금 그냥 숫자 하나만 예측을 하면 돼요

그래서 y의 길이는 1로 하겠다 그러한 형태를 가지는 선형함수를 만들어라

그런 얘기입니다

그러면은 이제 여기 y의 어떤 값이 나오는데 그 값이 여기 다시 나오겠죠

y 값이 나오는데 이거를 이제 시그모이드라는 함수에 다시 통과를 시킵니다

그럼 이 시그모이드라는 함수에 다시 통과를 시킵니다.

시그모이드라는 건 또 뭐냐 뒤로 가면 시그모이드가 이제 179쪽에 나오는데 시그모이드는 이렇게 생긴 함수예요

그래서 이제 선형 모델은 우리가 이제 어떤 x가 있으면 이런 식으로 생긴 게 선형 모델이죠

직선이 플러스로든 마이너스로든 무한히 뻗어 나갑니다

근데 우리는 우리가 원하는 거는 0 아니면 1로 분류하는 거거든요

그럼 얘가 이제 무한히 뻗어 나가면 1 2 3 4 5 6 7 8 9 10 이렇게 올라가니까 우리는 그냥 0 아니면 1이면 1이 되니까요.

그래서 우리가 이렇게 한 번 더 설명을 해볼게요.

10이면 되는데 10은 필요 없단 말이에요.

9도 필요 없고 여러분 배고파서 밥 한 그릇 먹으려고 하는데 갑자기 누가 밥솥을 갖다 주면 이렇게 많이 필요는 없는 거죠

얘가 좀 이상한데 어쨌든 그래서 우리가 필요한 거는 0 아니면 1이니까 이 사이에서만 놀면 되거든요

시그모이드 함수가 해주는 건 뭐냐면 이쪽으로도 무한대 마이너스 쪽으로도 무한대로 있는 어떤 값을 0에서 1 사이로 이렇게 찌그러뜨려 주는 역할을 합니다

그러면 아무리 이쪽으로 뻗어 나가도 결국에는 그냥 1 안쪽에 있고 아무리 이쪽으로 가도 0 안쪽으로 있죠

그래서 0에서 1 사이로 출력 값을 찌그러뜨려서 무조건 0에서 1 사이로만 답이 나오게 해줘요

그래서 출력을 0에서 1 사이로 제한해 주는 역할을 하고요 이름이 왜 시그모이드냐면 시그무가 이제 그리스 문자로 이렇게 쓰는데 이게 이제 우리 현재의 S자의 조상님입니다

그래서 이건 시그무는 S자란 뜻이고 오이드는 뭐뭐를 닮은 이런 뜻이에요

여러분 핸드폰 중에 안드로이드 핸드폰 있죠

안드로이드는 안드로가 사람 오이드는 닮은 사람 닮은 로봇 이런 뜻이에요

그래서 안드로이드 마크가 로봇이죠

아니면 우리 스테로이드 화학물질 중에 스테로이드라고 있는데 스테로이드는 스테롤이라는 화학물질을 닮은 화학물질이다 해서 스테로이드 이렇게 보인다

그래서 이제 과학용어에서 오이드 나오면 다 뭐뭐를 닮은 이런 얘기에요

그래서 시그무는 S자를 닮은 그램을 좀 보시면 약간 S자 모양으로 생겼죠

그런 얘기입니다

그래서 이렇게 이렇게 매끄럽게 변하는데 이렇게 매끄럽게 변하는 이유는 몇 가지가 있는데 우리가 이제 경사항법을 써가지고 이제 튜닝을 하는데 경사항법을 쓰려면 예를 들어서 여기 나오면 조금 이쪽으로 밀어준다던가 정답 1인데 우리가 확률이 결과가 아웃풋이 이렇게 나왔다

그럼

정답이 1이니까 이쪽으로 좀 더 밀어줘야겠죠

이런 식으로 이렇게 조금씩 밀 수가 있거든요

근데 함수 모양이 만약에 예를 들어서 이렇게 꺾여 있다

그러면 0 아니면 1이니까 이게 점진적으로 학습이 안 됩니다

우리가 물을 켜는데 뜨거우면 조금 올리고 차가우면 조금 내리고 이렇게 되는데 물이 버튼식으로 돼 있어서 무조건 100도짜리 물하고 0도짜리 물밖에 안 나온다

우리가 원하는 온도를 맞출 수가 없죠

항상 뜨겁거나 항상 차가운 것 밖에 안 되니까 그래서 이렇게 매끄럽게 변하는 매끄럽게 변하는 함수를 쓰고요 그러면은 이제 우리가 원하는 답은 1 아니면 0인데 예를 들어서 0.8 이렇게 나왔다

그러면은 그거를 해석을 어떻게 하느냐 이거는 1일 확률이 80%라는 얘기다

우리가 세상만사에 확실한 게 없으니까 이미지를 봤는데 이거는 1 같기도 하고 살짝 9 같기도 한데 그래도 1일 확률이 90% 9일 확률이 8% 7일 확률 2% 뭐 이런 식으로 우리가 확률적으로 예측을 할 수 있거든요

근데 이제 시그모이드 함수에서 결과값은 확률로 보통 해석을 합니다

그래서 이닛에서 하는 거는 모델을 구성하는 각 부분을 이렇게 정의를 해 주고요 그 다음에 이제 forward 함수는 x가 입력이 됐을 때 우리가 정의한 각 부분을 어떤 순서대로 통과를 하는지 그래서 최종 결과가 어떤 순서로 나오는지를 정의를 합니다

그래서 이제 이름이 forward죠 앞으로 이런 거죠

그래서 이렇게 모형을 정의를 하시면 됩니다

여기까지 질문 있으시면 질문해 주세요

아까 한 게 출력 기준이라는 얘기예요?

그렇죠

왜냐면 이미지마다 우리가 이미지마다 y가 하나씩 나오거든요

y가 숫자 하나가 나오죠

그래서 이미지 하나에 y 숫자 하나 이렇게 나오기 때문에 출력이 여기 보시면은 리니어에 여기가 하나가 나오거든요

그래서 이제 이항분류인 경우에는 둘 중에 하나로 하니까 숫자 하나만 있으면 그 숫자가 0이면 0으로 분류하면 되고 그 숫자가 1이면 1로 분류하면 되고 물론 실제로는 소수점으로 나오죠

0.8 이렇게 나오는데 0.8이면은 1일 확률이 80%니까 반대로 말하면 0일 확률은 20%가 되는거에요

자동으로 그래서 이제 이 결과는 하나만 있으면 됩니다


```python copy
criterion = nn.BCELoss() # 이항분류이므로 BCELoss
```




자 그러면은 다음으로 넘어가서 손실 함수에 대해서 알아보면 손실 함수는 뭐냐면 손실 함수라고 보면은요.

이럴 때 롯스펑션이 있고 손실이든 비용이든 오차든 다 무조건 작은 게 좋은 거죠

그러니까 이거는 결과적으로 뭐냐면 손실 함수는 이게 얼마나 안 맞느냐 얼마나 안 맞느냐를 측정을 합니다

왜 잘 맞느냐로 안 하고 안 맞느냐로 하느냐

뭐 그거는 저도 잘 모르겠는데 그냥 관습적으로 그래요

보통 안 맞느냐를 측정을 하고 그래서 우리가 모델이 어떤 예측을 하는데 이 예측이랑 실제랑 얼마나 다르냐를 얘기를 합니다.

그럼 이 두 개의 차이가 없어야겠죠.

손실은 없어지는 쪽이 항상 좋은 쪽입니다.

그래서 손실을 최소화하면 성능이 개선이 돼요.

우리가 돈도 씀씀이를 줄이면 돈을 버는 길이죠.

손실 함수가 여러 종류가 있는데 대표적인 손실 함수의 MSE랑 교차 엔트로피가 있습니다.

수학적인 것은 제외하고 간단히 얘기하면 MSE는 보통 연속변수 예측에 사용을 해요.

연속변수라는 건 예를 들면 우리가 가격 같은 걸 예측할 때 가격은 예를 들면 실제 가격이 50만 원인데 우리가 예를 들면 어떤 주식인데 이 주식이 한 50만 원짜리 주식이에요.

근데 우리가 잘못 알고 60만 원에 샀어요.

그럼 우리가 10만 원 손해보는 거죠.

근데 우리가 55만 원에 50만 원에 손해보는 거죠.

근데 우리가 55만 원에 손해보는 거죠.

그러면 손해가 5만 원으로 줄죠.

그래서 연속변수는 조금이라도 오차를 줄이면 잘 맞은 겁니다.

근데 이제 연속변수가 아닌 게 있어요.

범주형 변수가 있는데 범주형 변수는 그냥 이것 아니면 저것 이런 게 범주형 변수입니다.

예를 들면 우리가 숫자를 맞추는 경우에도 우리가 이 숫자가 0이냐 1이냐를 맞춰야 되는데 이거는 0이면 0이고 1이면 1이죠.

이게 0인지 중간이 없어요.

우리가 손글씨를 맞추는데 예를 들면 누가 0을 이따위로 써놨어요.

그럼 이게 0인지 아니면 1을 쓰다가 펜이 흔들려서 두 번 이렇게 쓴 건지 잘 모르겠단 말이에요.

그래도 이거는 왠지 0인 것 같기도 하고 1인 것 같기도 하고 애매하지만 0.5라고 답하면 안 됩니다.

이거는 0 아니면 1이에요.

이런 걸 범주형 계측이라고 하는데 근데 제가 0.5라고 하면 안 된다고 했지만 사실 대답을 할 때 0.5라고 할 수는 있습니다.

0.5라고 대답한 거는 얘가 0.5라는 게 아니고 우리가 잘 모르겠으니까 확률이 50대 50이라는 얘기죠.

그래서 우리가 이렇게 확률로 얘기를 하는데 그러면 예를 들어서 어떤 정답이 1인 게 있어요.

근데 우리가 이거 잘 모르겠는데 50대 50 이랬어요.

그러면 틀린 것도 아니고 맞춘 것도 아니지만 사실 잘 맞춘 건 아니죠.

근데 만약에 이거는 정답이 1일 확률이 99%야.

근데 혹시 모르니까.

그럼 이게 더 잘한 거죠.

우리가 맞출 때는 확률이 높은 쪽이 좋은 겁니다.

근데 만약에 정답이 0인데 내가 야 이거 99% 1이야 라고 했으면 틀리면 개망신이죠.

틀릴 때는 이렇게 해서 틀리면 안 좋아요.

반대로 아 이거 잘 모르겠는데 50대 50인데 라고 하면 그나마 좀 틀렸지만 선방은 한 거죠.

그러니까 맞을 때는 높은 확률로 맞추고 틀리면 안 좋아요.

반대로 아 이거 잘 모르겠는데 50대 50인데 라고 하면 맞추고 틀릴 때는 낮은 확률로 틀려야 좋은 거라고 할 수 있습니다.

그래서 교차인트로피는 우리가 이제 예측한 확률하고 실제 정답을 하고 이 공식에 넣고 계산을 하면 되는데 결국에는 높은 확률로 예측했을 때 맞고 낮은 확률로 예측했을 때 틀려야 이게 떨어지는 이런 형태로 되어 있는 그런 수식입니다.

그래서 이제 우리가 분류 이런 범주형 분류는 범주형 분류를 당분류랑 이항 분류로 나누는데 이항 분류는 둘 중에 하나를 나누는 거고 당분류는 여러 개 중에 하나로 나누는 겁니다.

그런데 우리는 지금 0 아니면 1 이거니까 이항 분류입니다.

둘 중에 하나로 나누는 거예요.

그래서 이때는 크라이트리온을 bce 로스라는 걸 씁니다.

왜 bce냐면 이 b가 binary 이항이라는 뜻이에요.

두 가지 둘 중에 하나 그리고 ce는 그냥 크로스 엔트로피.

그래서 이항 크로스 엔트로피.

bce 로스 이런 얘기입니다.

그래서 설명은 길었지만 우리는 이 모델을 학습을 시킬 건데 이 모델은 bce 로스로 학습을 시키면 되겠죠.

그래서 이 형식으로 된 것입니다.


```python copy
optimizer = torch.optim.RAdam(model.parameters())
```




자 그 다음에 186 쪽으로 넘어가서 그래서 우리가 이제 경사강법을 써야 되는데 경사강법을 이제 우리가 모델을 처음에 만들면 일단 랜덤하게 파라미터를 매겨 놓습니다.

우리가 아까 보면 레이어 중에 여기 이제 리니어 레이어가 있는데 리니어 레이어는 w랑 b가 있습니다.

리니어 레이어는 w랑 b가 있습니다.

그래서 이 랜덤을 일단 모르니까 이 랜덤을 일단 모르니까 아무 숫자나 그냥 집어넣어요.

아무 숫자나 집어넣고 그러면은 뭐 아무 숫자라도 계산을 할 수 있는 거죠.

그러면은 뭐 아무 숫자라도 계산을 할 수 있는 거죠.

우리가 계산을 하면 어떤 그 출력 값이 나옵니다.

y가 나오긴 나와요.

뭔가 나오긴 나옵니다.

그래서 y가 나오면 그 y를 가지고 우리가 이제 정답하고 비교를 할 수 있죠.

실제 y랑 비교를 해 가지고 손실을 계산할 수 있습니다.

손실이 얼마 얼마 이렇게 나오겠죠.

그럼 여기서 미분을 하면 우리가 이제 w로 y의 손실을 손실함수를 미분을 하면 기울기가 나오는데 이 기울기는 뭐냐면 이 파라미터를 이 방향으로 바꾸면 손실이 이만큼 줄어든다 라는 게 기울기입니다.

이건 이제 미분을 하면 되는데 미분 그게 뭐였더라

이런 분도 계시겠지만 뭐 그런 게 있습니다.

그건 우리가 직접 할 건 아니니까 믿으시면 돼요.

그래서 이 미분을 하면은 이거를 어느 방향으로 이 파라미터를 조절해야 되는지를 우리가 알 수 있습니다.

그러니까 샤워실에서 물을 딱 맞았을 때 물을 우리가 물을 맞으면 이걸 뜨거운 쪽으로 돌려야 되는지 차가운 쪽으로 돌려야 되는지 우리가 본능적으로 알 수 있잖아요.

그거를 이제 수학적으로 계산한 게 미분입니다.

이거를 높여야겠다 낮춰야겠다는 알 수 있어요.

근데 이제 얼마나 높여야 되는지 얼마나 낮춰야 되는지는 모릅니다.

방향성은 알아요.

처음 가는 목욕탕에 가서 뜨거운 물을 팍 맞으면은 아잇 뜨거워 하면서 차가운 쪽으로 돌려야 된다는 건 알지만 이걸 많이 돌려야 되는지 조금 돌려야 되는지는 이 샤워기를 잘 모르면 모르잖아요.

그죠?

근데 어쨌든 돌려야 된다는 건 알죠.

죽지 않으려면 돌려야 된다.

이거는 아는데 얼마나 돌려야 되는지 모르는 거죠.

그래서 일단 돌릴 수가 있어요.

그래서 만약에 우리가 이제 샤워장에서 너무 많이 돌렸다.

그러면 너무 여기서 이제 차갑다고 합시다.

너무 차가워서 아잇 차가워 하고 너무 확 돌렸더니 너무 뜨거워요

이번에.

그럼 어떻게 하면 되겠습니까?

왼쪽으로 이제 다시 차가운 쪽으로 돌리면 되겠죠.

그러면 또 아잇 차가워 하면 다시 뜨거운 쪽으로 아 뜨거워

아 차가워

아 뜨거워

아 차가워 이러다가 아 이제 괜찮군 이렇게 되는 거죠.

아니면은 아유 차가워라 하고 돌렸는데 아 여전히 차갑네

하고 돌렸는데 아우 차가워 하고 돌렸는데 어 이제 조금 낮지만 좀 더 따뜻했으면 좋겠는데 조금 조금 조금 아 이렇게 할 수도 있겠죠.

그러니까 뭐 우리가 이렇게 이렇게 이렇게 이렇게 해서 여기까지 갈 수도 있고 뭐 이렇게 이렇게 이렇게 이렇게 이렇게 해서 여기까지 갈 수도 있고 뭐 여러 가지로 갈 수 있지만 어쨌든 계속 돌리는 걸 반복하다 보면은 결국에는 맞는 온도에 도달하게 됩니다.

물론 이제 가끔 샤워기 중에 약간 샤워기가 이상해 가지고 온도가 지멋대로 변하는 샤워기도 가끔 있어요

그죠.

특히 이제 가정용 그 샤워기 같은 경우는 보일러가 이제 늦게 돌아가서 보일러가 돌아갈 때까지는 아무리 뜨겁게 해도 찬물만 나오는 그런 경우가 있는데 그런 건 논외로 하고 이제 온도가 일정하게 나온다고 치면은 우리가 어쨌든 계속 돌리다 보면 조금씩 돌리다 보면은 계속 반복해서 하다 보면은 언젠가는 우리가 손실을 가장 작게 만들 수가 있습니다.

즉 우리가 할 수 있는 최선의 예측을 할 수가 있게 되는 거죠.

그래서 이게 이제 경사하강법이고 그 다음에 이제 방금 제가 샤워기 비유하면서 너무 많이 돌릴 때가 있고 너무 조금 돌릴 때가 있다고 얘기했는데 너무 많이 돌리면 문제가 뭐냐면 그래도 적당히 많이 돌리면 아 뜨거워

아 차거워 아 뜨거워

아 차거워 하면서 이제 이렇게 되는데 너무 많이 돌리면 이제 문제가 돼요.

어떤 문제가 되냐면 차가워서 아이고 뜨거라고 확 올렸는데 또 이게 너무 뜨거워서 아유 차가워라 하고 또 확 돌렸다가는 점점 돌리는 폭이 커지는 거죠.

그러면은 지옥과 지옥을 왔다 갔다 하면서 이제 망하는 거죠.

그래서 이제 그 경제학의 샤워실의 바보라는 말이 있거든요.

이제 뭐냐면 정부 정부에서 정책을 펼치는데 보통 정부에서 경제정책을 펼칠 때 경기가 나빠지면은 확장적 정책을 써가지고 돈을 풀고 경기가 너무 좋으면은 긴축정책을 풀어서서 경기를 좀 가라앉게 만든단 말이에요.

예를 들면 요즘에 보면은 한동안 인플레가 심하니까는 금리를 올려가지고 돈줄을 죄 있는 거죠.

근데 보면은 되게 조금씩 조금씩 조절한단 말이에요.

왜 그러냐면은 어 경기가 너무 물가가 너무 오르네?

그런다고 막 금리를 갑자기 10%씩 올리면은 갑자기 회사들이 멀쩡하다고 회사들이 도산하고 난리가 나겠죠.

그렇다고 또 아이고 갑자기 회사들이 망하냐 하고 돈을 갑자기 확 풀면은 또 갑자기 또 물가가 미친듯이 오르고 난리가 나겠죠.

그래서 그 짓을 몇 번 하면 그냥 날아가요.

나라가 말아먹힙니다.

그래서 이제 항상 모든 정책을 조금씩 조금씩 조절을 하는 거죠.

정확한 그 우리가 금리가 최적금리가 얼만지 이런거 모르기 때문에 계속 조금씩 조금씩 조절하면서 이제 간을 보는 거죠.

근데 이제 샤워실의 바보라는게 뭐냐면 이 앗 뜨거워 차가워 뜨거워 차가워 뜨거워 차가워

이러면서 이러면은 이제 샤워실의 바보다.

조금씩 바보야 조금씩 조절해야지 그걸 이렇게 확확 돌리면 안돼

이런 얘기에요.

그래서 여기 학습률이라는 개념을 쓰는데 학습률은 뭐냐면 우리가 이제 샤워기를 얼마나 돌릴거냐

이런 얘기입니다.

너무 확확 돌리면 안된다

이런 얘기에요.

그러면은 아 조금 조금 돌리면 되겠네

이렇게 할 수 있는데 또 이게 너무 찔끔 돌려도 문제가 되요.

왜냐하면은 여기까지 가야되는데 아 정말 띠클 띠클 띠클 띠클 하면은 너무 오래 걸리겠죠.

빨리빨리 안된단 말이에요.

여러분이 샤워기에 딱 탔는데 너무 뜨거운 물이 나와요.

근데 나는 정확한 온도를 맞추겠어 하면서 1mm씩 돌리고 있으면 사람이 다 익어버리겠죠.

그러니까는 또 어느정도는 빨리빨리 돌려줘야 됩니다.

그래서 이 학습률을 이게 좀 정하는게 되게 어려운데 우리가 어떻게 정하는지는 좀 이따 알아볼건데 어 너무 많이 돌리면 안 되겠죠.

너무 많이 올려도 안되고 너무 살살 올려도 안되고 적당히 적당히 해줘야 됩니다.

근데 항상 적당히 올려서 얼만데 그게 참 어렵습니다.

그래서 이제 예시를 보면은 예시로 이제 y가 3 6 9 10이고 x가 1 2 3 4고 이런 데이터가 있어요.

그래서 모델은 여기 더하기 b가 원래 있어야 되는데 더하기 b를 없애버렸습니다.

그러면은 요거는 뭐 간단하죠.

x에 얼마 곱해야됩니까?

x에 3을 곱해주면 되겠죠.

근데 요거는 뭐 간단하죠.

그래서 우리가 이제 그 정답을 모른다 치고 한번 요 w를 얼마로 해야 되는지 추정을 해보겠습니다.

그래서 일단은 w를 아무 숫자는 넣는데 보통 이제 우리가 0을 좋아하니까 0으로 넣어보겠습니다.

그러면은 w가 0이면 다 x에다 0을 곱하니까 1 2 3 4에 0을 곱하면 0 0 0 0 이렇게 되겠죠.

그래서 이게 우리의 예측값인데 그럼 우리 이제 참값을 y를 아니까 참값하고 차이를 구하면 요렇게 나오게 됩니다.

그 다음에 우리가 이제 여기서는 연속적인 걸 예측을 하니까 mse를 구해보도록 할게요.

mse는 요 오차를 제곱을 해주는 거에요.

그러면은 9 36 81 144 이렇게 나오고 얘를 평균내면은 67.5가 됩니다.

그래서 요거를 요 l을 w로 우리가 w로 미분을 하면은 미분값의 평균이 마이너스 45가 돼요.

그래서 마이너스 45라는 거는 우리가 이제 w를 높여야 오차를 줄일 수 있다.

요런 얘기가 됩니다.

그래서 우리가 이제 학습률을 왜 그러냐면은 기울기가 마이너스라는 건 기울기가 요렇게 생겼다는 거거든요.

그래서 우리가 요 경사면 중에 왼쪽에 있다는 얘기에요.

그럼 기울기가 요렇게 돼 있으니까 w를 높여야 이게 줄어들겠죠.

경사가 요렇게 돼 있는 거니까 지금 마이너스라는 거.

그래서 이제 학습률이 예를 들면 0.1이다.

그러면 경사가 마이너스 45니까 두 개를 곱하면 이제 플러스 4.5로 이동을 하게 됩니다.

그래서 우리가 지금 여기 0에 있었는데 쭉 이쪽으로 이동을 해서 이쪽으로 이동을 해서 플러스 4.5까지 갔습니다.

사실 정답은 3이니까 너무 많이 갔어요.

하지만은 원래 정답에서 3만큼 떨어져 있었는데 정답에서 1.5만큼 떨어진 걸로 오차가 좀 줄었죠.

근데 이제 4.5로 가보니까 1에다 4.5 곱하고 2에다 4.5 곱하고 뭐 이렇게 하면은 요렇게 나오고 요거 실제 값하고 차이 구하면 요렇게 나오고.

그래서 손실을 구하면.

16.88 이렇게 나오는데 요걸 이제 또 미분하면 22.5가 나오는데요.

요렇게 있는데 우리가 이제 정답이 3인데 우리가 지금 4.5에 있어요.

기울기가 이렇게 있으니까 기울기가 이제 플러스로 돼 있습니다.

그러면 기울기가 플러스로 돼 있을 때는 반대로 이쪽으로 가야 내려가는 거죠.

그래서 다시 22.5에다가 0.1을 곱하니까 4.5 빼기 2.25 해서 2.25로 가게 됩니다.

이제 요기 이쪽으로 가는 거죠.

그래서 보면은 아까 0에서 시작을 했는데.

여기 있다가 요렇게 있다가 요렇게 있다가 요런 식으로 이제 점점점점 실제인 3으로 서서히 이제 접근을 하게 됩니다.

계속하면은 언젠가는 이제 3으로 수렴을 해요.

그래서 우리가 이제 뭐 복잡한 그 방식을 쓰지 않아도 계속 이 짓을 그냥 계속 반복하면 됩니다.

우리 옛날에 학교 다닐 때 수학 문제 푸는데 잘 모르겠으면 이제 오지선다형 문제인 경우는 거꾸로 대입해 보면 되잖아요.

하나씩.

약간 그거하고 비슷한 거죠.

숫자를 하나씩 계속 대입을 해 보면서 정답이 될 때까지는.

정답이 될 때 찾을 때까지 계속 대입을 해 보는 거예요.

대입 좀 해 보고 어 이거보다 좀 더 큰가?

그럼 좀 더 큰 값 대입해 보고 어 이거보다 좀 더 작은가?

그럼 작은 값 대입해 보고 이걸 계속 반복을 하면서 정답을 찾아가는 이런 방식이죠.

왜 이렇게 이제 무식하게 하느냐.

요거는 우리가 이제 근의 공식 같은 게 없습니다.

수학적으로 증명이 돼 있어요.

근의 공식 같은 게 없기 때문에 이런 식으로 계속 무한 대입을 하면서 정답을 찾아가는 수밖에 없습니다.

경사강법이 항상 정답이 도달하냐 하면 꼭 그런 건 아니고.

만약에 이제 손실 함수가 손실이 이런 식으로 생긴 함수다.

그러면은 우리가 이런 데서 출발을 하면은 아무리 여기서 이렇게 내려가 봤자 여기까지 밖에 못 내려가요.

손실을 이만큼까지 줄일 수 있는데 이만큼은 우리가 이제 못 줄이는 손실입니다.

그래서 이제 이런 점을 국소 최소라고 하는데요.

그러니까 이제 이거는 이제 전역 최소라고 하고 손실을 최소화해야 되는데.

우리가 이제 줄일 수 있는 손실이.

여기서 출발해 봤자는 여기까지 밖에 못 줄이는 거죠.

예를 들면 이제 우리가 사람도 마찬가지인데.

여러분이 그냥 날 때부터 그냥 뭐 돈을 좀 써야 되는 성격이다.

그러면 뭐 아무리 여러분이 절약을 하려고 해도 절약을 하는데 한계가 있죠.

여러분 그 태생이 시작 출발점이 그런데 어쩌겠어요.

여러분이 뭐 나는 그냥 맨밥에 단무지만 먹어도 행복해.

이런 분들이면 출발점이 아주 좋은 출발점이죠.

월급이 똑같아도 훨씬 절약을 많이 할 수 있습니다.

그냥 별로 돈을 쓰고 싶은 마음이 없어요.

출발점이 좋은 거예요.

그래서 출발점이 어디냐에 따라서 최종적으로 도달하는 지점이 다를 수가 있어요.

그래서 사실 이제 약간 이게 복불복이거든요.

왜냐하면 좋은 출발점이 어딘지를 알 수가 없기 때문에 좀 복불복입니다.

똑같은 모델이라도 두 번 트레이닝 시켰을 때 결과가 다를 수 있어요.

한 번은 잘 나오고 한 번은 잘 안 나오고.

그래서 딥러닝 할 때 어떻게 하냐면 그냥 여러 번 해봅니다.

똑같은 모델이라도 여러 번 돌려봐 가지고.

운이 좋으면 결과가 잘 나오고.

운이 나쁘면 결과가 잘 안 나오고 그렇습니다.

그래서 약간 무슨 이게 공학이라기보다는 약간 어떤 느낌이냐면 도자기 굽는 장인들 보면 도자기 구웠는데 마음에 안 들면 깨버리고 새로 굽고 그러잖아요.

계속 잘 나올 때까지 그냥 굽는단 말이에요.

약간 그런 느낌으로 합니다.

그냥 모델을 만들어서 학습시키고 잘 나오면 오 좋은데 하고 쓰고.

잘 안 나오면 깨버리고 다시 하고.

약간 복불복 스타일로 하는 거죠.

그래서 이제 이런 식으로 경사학왕법을 쓰는데 방금 앞에서 보면 경사를 계산을 할 때 경사를 손실을 여러 번 계산을 해 가지고 그거에 평균으로 경사를 구하거든요.

그래서 이제 이렇게 하는데 그러면은 우리가 이제 생각을 좀 해볼 수가 있는데 어떤 생각을 해 볼 수 있냐면 보통 평균이라는 거는 전체 데이터가 없이 좀 샘플만 있어도 어느 정도는 추정을 할 수 있답니다.

예를 들어서 우리가 우리나라 사람들 키의 평균을 구하고 싶은데 그렇다고 5천만 명의 키를 다 잴 필요는 보통 없어요.

한 천 명, 2천 명 키를 재보면 그걸로 평균을 재도 대충 비슷하게 평균을 잴 수 있죠.

그래서 이제 확률적 경사학왕법이라는 게 나오는데요.

확률적 경사학왕법은 뭐냐면 경사를 구할 때 딱 하나의 사례만 가지고 경사를 계산을 하는 겁니다.

원래는 이렇게 하는 거예요.

원래는 전체 사례를 가지고 경사를 구해서 평균을 내야 되는데 그냥 하나만 가지고 하는 거예요.

그러니까 예를 들면 우리나라 사람들이 키를 재는데 그냥 한 사람의 키를 재서 이 사람 키가 크네.

그럼 한국 사람들 키가 큰가 보다.

이렇게 하는 겁니다.

우리가 외국에 갔는데 공항에서 처음 만난 그 나라 사람의 키가 되게 커요.

그럼 우리 딱 그런 생각을 하죠.

이 나라 사람들 키 되게 큰가 보다.

이렇게 생각할 수 있잖아요.

그럼 옆에서 한 명만 보고 그럴 수 있어 라고 할 수 있죠.

우리가 그렇게 생각하는 게 그렇게 이상하진 않거든요.

그 나라 사람들이 키가 다 작은데 그 사람만 키가 크다.

그 사람만 키가 클 리는 없잖아요.

보통은 대체로 비슷하겠죠.

그래서 이제 어떤 하나의 사례만 가지고 하는 걸 확률적 경사학왕법이라고 하고 이렇게 하면 장점이 뭐냐면 이 경사가 부정확하게 됩니다.

경사가 부정확해져요.

그럼 경사가 부정확한 게 왜 장점이냐.

아까 보시면은 이런 식으로 생겨 가지고 우리가 여기서 출발하면 이렇게 내려가서 여기까지 밖에 못 내려간단 말이에요.

근데 경사가 부정확하니까.

운이 좋으면 이렇게 넘어갈 수가 있습니다.

방향을 잘못 잡아서.

그럼 반대로 여기다가도 일로 갈 수 있지 않나요.

그건 어쩔 수 없죠.

운이 없으면 이상한 데로 빠질 수도 있는데 운이 좋으면 이쪽으로 갈 수가 있는 거죠.

그리고 보통은 이쪽이 골짜기가 더 깊기 때문에 여기서는 운이 나빠 봐야 여기까지 밖에 못 가지만 이런 데서는 운이 좋으면 이렇게 거슬러 올라갈 수 있는 거죠.

그러니까 운이 좋을 가능성이 더 큽니다.

운이 나쁠 가능성 보다는.

왜냐면 이런 데서 운이 나빠 봐야 어차피 이 안에서 노는데 여기서는 운이 좋으면 벗어나니까.

여기에 얕아가지고.

그래서 국소천적화를 피할 가능성이 좀 있고.

그다음에 계산량도 적죠.

원래는 전체 데이터에 대해서 경사를 구해야 되는데 하나의 데이터만 가지고 경사를 구하니까 계산의 효율성이 올라가게 됩니다.

그 대신에 이제 문제는 뭐냐면 예를 들어서 여기서 이제 이렇게 있으면 여기서 이제 좀 내려가야 되는데.

자꾸 여기서 왔다 갔다 왔다 갔다 이럴 수가 있어요.

그렇겠죠.

왜냐하면 경사가 계속 부정확하니까 재수 없으면 여기서 그냥 계속 왔다 갔다 왔다 갔다 하거나 아니면 여기까지 가야 되는데 내렸다 올렸다 내렸다 올렸다

이러면서 여기서 꾸물꾸물거릴 수가 있습니다.

너무 부정확해도 문제죠.

그래서 이제 확률적 경사강법의 약간 파생형으로 미니 배치 경사강법이라는 게 있는데.

이거는 뭐냐면 하나의 사례로 하니까 너무 극단적이다.

그래서 사례를 좀 늘리자.

예를 들면 30개.

30개.

뭐 이런 식으로.

그러면은 뭐 부정확하긴 해도 예를 들면 우리가 어떤 나라 사람 30명 만나봤는데 30명이 하나씩 다 키가 커요.

그러면은 아 이 나라 사람들은 키가 큰가 보네.

내가 한 30명 봐도 다 크더라고.

그러면은 뭐 우연히 그 30명이 뭐 농구팀이었다든가 이럴 수는 있겠지만 그래도 그 정도로 잘못 판정하지는 않겠죠.

어느 정도는 맞겠죠.

그래서 이 미니 배치 경사강법은 이제 확률적 경사강법이 이제 너무 극단적이니까 조금 이제 사례수를 늘리는 겁니다.

하나에서 좀 몇 개로.

늘리는 거고.

그래서 보통은 이 가장 극단적인 형태의 확률적 경사강법을 잘 쓰지 않기 때문에 그냥 미니 배치 경사강법을 확률적 경사강법이라고 보통 이제 퉁차서 얘기합니다.

원래 엄밀하게 말하면은 미니 배치 경사강법이라고 얘기하는데 그러면 이제 말도 길고 하니까 보통은 그냥 확률적 경사강법이라고 쓰고 실제로는 미니 배치 경사강법을 합니다.

좀 약간 그래서 이것도 혼란의 혼동의 여지가 있는데 뭐 그렇게 해요.

약간 그거랑 비슷한 거죠.

왜 우리가 스탱이라고 하는데 스탱이 잘 생각해보면 말이 좀 이상하거든요.

왜냐하면 이게 스테인리스 스텔인데 녹이 안 쓰는 쇠인데 이거를 다 지우고 녹만 넣고

이거잖아요.

녹이 안 쓰는 쇠를 줄여가지고 녹이라고 하니까 사실 말이 좀 이상하단 말이에요.

그죠.

근데 그걸 뭐 이상하다고 생각 안 하잖아요.

그냥 우리가 뭐 그런가 보다.

다 저희가 스탱 가져와라

이러지.

뭐 그런가 보다.

누가 뭐 저희가 스테인리스 스텔 가져와세요.

누가 그렇게 얘기합니까.

그죠.

다 그냥 스댕스댕 하면서 살죠.

그러니까 원래 미니 배치 경사강법이 이제 정확한 표현이고 확률적 경사강법은 가장 극단적인 예인데 그냥 보통 확률적 경사강법이라 쓰고 사실은 다 미니 배치 경사강법이라는 뜻으로 사용을 합니다.

그다음에 이제 또 여러 가지 우리가 경사강법의 문제를 보완을 하기 위해서 여러 가지 트릭을 써요.

그래서 이거를 이제 다 아실 필요는 없고 그냥 이런 것도 있다는 차원에서.

그냥 한번 들어두시면 되는데요.

예를 들면 이제 우리가 경사를 따라 내려갈 때 여기서 출발하면 이제 이걸 그 계곡을 위해서 봤다고 생각하시면 이렇게 가는 게 아니라 이쪽으로 갔다가 다시 처음에 출발할 때 이쪽 방향에서 왔으면 이렇게 갔다가 이렇게 갔다가 이렇게 갔다가 막 이렇게 진동을 할 수가 있거든요.

그래서 모멘텀은 뭐냐면 여기다 일종의 약간 가속도 같은 걸 붙여가지고 이거를 좀 완만하게 이렇게 떨어지도록 하는 거예요.

그래서.

그러니까 이게 또 어떤 기법이 더 있다면.

이런 기법도 있고 그다음에 이제 우리가 학습률을 아까도 얘기 드렸지만 학습률을 너무 커도 안 되고 너무 작아도 안 되니까 학습률을 이제 가변적으로 조정하는 방법이 있습니다.

그래서 이제 어댑티브하게 조정을 한다고 해서 여기 앞에 a d가 붙어있는데 학습률을 너무 많이 가거나 너무 적게 가지 않도록 조정하는데 이거는 뭐 좀 복잡한 얘기인데 기본적인 아이디어는.

그냥 그런 식으로 이렇게 하는 게 아니라 이제 학습률을 가볍게 조정하는 방식이니까.

그래서 이제 어 어댑티브하게.

너무 많이 가거나 너무 적게 가지 않도록 조정하는데 이거는 좀 복잡한 얘기인데 기본적인 아이디어는 뭐냐면 많이 가본 방향은 내가 예를 들면 이렇게 있는데 이쪽으로 이미 많이 왔어요.

그러면 대충 거의 다 왔으면 속도를 서서히 줄이는 거죠.

이쪽으로 많이 와봤으면 물론 아직 더 가야 될 수도 있는데 여러분이 어딜 가는데 차로 막 가는데 처음에는 밟으면서 가다가 슬슬 빠져나가는 고속도로 같은데 운전하시다가 왠지 느낌상 슬슬 이때쯤 되면 빠져나가는 데가 있을 것 같은데 하면 속도를 서서히 줄이면서 좌우를 잘 보잖아요.

약간 마찬가지의 원리로 이쪽 방향으로 많이 와봤다.

그러면 학습률을 좀 낮추고 만약에 출발한 지 얼마 안 됐다.

여기 이렇게 있는데 우리가 출발한 지 얼마 안 됐어요.

그러면 빨리빨리 가는 거예요.

여러분들이 고속도로 탔는데 이제 막 탔어요.

그럼 신나게 밟아야죠.

과속화면 없으면 일단 밟고 속도는 좀 이따 낮추면 되잖아요.

그래서 그런 느낌으로 하는 방법이 있고 그런 방법들이 발전의 발전의 발전을 거듭해서 이렇게 발전을 하고 있습니다.

그 다음에 요즘에는 또 어떤 방법도 있냐면 우리가 기본적으로 경사강법이 깜깜이로 내려가는 거거든요.

내가 방향은 아는데 어디까지 가야 되는지를 잘 모른단 말이에요.

깜깜이로 나가는데 그래서 Look Ahead라고 해서 일종의 선발대를 보내는 거예요.

선발대를 보내 봐 가지고 야 네가 먼저 좀 가 가지고 저기 한번 봐라.

이렇게 하는 거예요.

그래서 선발대를 보내 보니까 쭉 가서 보니까 아 이렇던데요.

그러면은 내가 있는 위치에 선발대를 따라가는 게 아니라 이 둘 사이에 중간으로 갑니다.

왜 중간으로 가냐 하면 보통 이제 여기가 최저예요.

여기가 바닥이고 우리가 깜깜이 상태로 바닥을 가야 되는데 내가 여기서 출발을 해 가지고 선발대를 보내 봤어요.

선발대를 보내 보니까 선발대가 여기면.

기본적으로 여기가 움푹 파여 있는 구조이기 때문에 반드시 여기랑 여기 사이로 가면은 더 최저점으로 도달할 가능성이 높습니다.

선발대가 여기로 가는 게 아닌 이상은 선발대가 여기로 갔어도 중간으로 가면은 결국에는 더 바닥을 향해서 가는 거죠.

그래서 이게 기본적으로 구덩이기 때문에 우리가 잘못하면 재수가 없으면 구덩이 주변을 뱅글뱅글 돌 수가 있어요.

눈 감고서 가기 때문에 깜깜이로 가기 때문에 뱅글뱅글 돌 수가 있는데 그러면 선발대를 먼저 보내 놓고 선발대랑 나 사이로 이동을 하면은 내가 이제 어쨌든 구덩이 쪽으로 더 들어갈 수 있다.

바닥으로 더 가까이 갈 수 있다.

뭐 이런 방법도 있습니다.

이게 이제 가장 최근에 나온 Look Ahead라는 방법이고 뭐 이런 식으로 우리가 경사강법도 약간 오리지널 버전을 그대로 쓰는 게 아니라 뭐 이렇게 이제 여러 가지 어떤 업그레이드가 돼 있어요.

근데 이제 파이토치에서는.

지금까지 소개해 드린 것 중에 여기 아담을 보면은 아담이 이제 발전 발전 발전 해 가지고 R 아담까지 발전했는데 여기까지는 구현이 되는데 Look Ahead는 아직 구현이 안 돼 있습니다.

그래서 이제 최신 가장 최신 알고리즘이 Ranger라고 해서 Look Ahead에다가 R 아담이랑 합친 건데 요거는 있는데 요게 없거든요.

그래서 Ranger는 우리가 이제 써 볼 수가 없고 R 아담은 있기 때문에 요거는 이제 그냥 쓰면 됩니다.

그래서 뭐 사실 지금까지 얘기는 길었는데 그냥 이거 쓰시면 돼요.

이거 쓰시면 되고.

그래서 Look Ahead는 이제 쓰고 싶으면 이것도 사실 코드가 다 공개돼 있기 때문에 그냥 다운받아서 하시면 되는데 뭐 귀찮으니까 우리 현재 하는 수준에서는 이 정도까지 최신 알고리즘은 별로 필요 없습니다.

그래서 우리는 그냥 R 아담을 쓸 거예요.

그래서 R 아담한테 우리 모델에 파라미터를 주고 야 이거를 네가 최적화를 해라

그래서 설명은 되게 길었는데 사실 코드는 뭐 없어요.

그래서 우리가 모델 만들고 그래서 우리가 모델 만들고 그 손실함수 정하고 학습 알고리즘, 경사학법 알고리즘 정하고 학습 알고리즘, 경사학법 알고리즘 정하고 이렇게 해주시면 되고요.

이렇게 해주시면 되고요.


```python copy
from torch.utils.data import DataLoader

BATCH_SIZE = 32

train_loader = DataLoader(
    binary_train_dataset,  # 훈련 데이터
    batch_size=BATCH_SIZE,  # 32개씩
    shuffle=True)  # 섞어서(미니배치마다 조합이 다양하도록)

test_loader = DataLoader(
    binary_test_dataset,  # 테스트 데이터
    batch_size=BATCH_SIZE,  # 32개씩
    shuffle=False)  # 섞지 말고

```




자 그다음에 우리가 이제 데이터를 이제 그 학습 알고리즘에다가 R 아담한테 넣어줘야 되는데 이제 그 학습 알고리즘에다가 R 아담한테 넣어줘야 되는데 아까 전에 말씀드렸듯이 아까도 얘기 드렸다시피 모든 데이터를 다 집어넣는 게 아니라 아까도 얘기 드렸다시피 모든 데이터를 다 집어넣는 게 아니라 조금씩 조금씩 넣습니다.

그게 미니 배치 경사학법 또는 확률적 경사학법이라고 얘기 드렸죠.

그래서 이제 몇 개씩 넣어야 되는지를 정해주는데 우리는 32개씩 넣도록 하겠습니다.

우리는 32개씩 넣도록 하겠습니다.

뭐 특별한 이유가 있는 건 아니고 일단 보통 32개 정도가 그냥 좋은 출발점이에요.

그래서 이걸로 해서 잘 안되면 좀 늘려보기도 하고 좀 줄여보기도 하고 뭐 이렇게 하는데 보통은 이제 2의 배수로 합니다.

32 이렇게 그래서 32, 64, 128 이런 식으로 나가는데 왜 그러냐면 이제 컴퓨터가 보통 계산할 때 32비트, 64비트 뭐 이런 식으로 32비트, 64비트 뭐 이런 식으로 2의 배수...

2의 배수도 아니고 2의 배수도 아니고 뭐라고 해야 되죠?

2의 n승 형태로 이제 계산을 하기 때문에 예를 들면 배치 사이즈를 31로 해도 되는데 예를 들면 배치 사이즈를 31로 해도 되는데 그러면은 컴퓨터가 뭔가 데이터를 주고받을 때 항상 32의 배수, 64의 배수 이런 식으로 하니까 항상 32의 배수, 64의 배수 이런 식으로 하니까 우리가 저거 배치 사이즈를 31로 하면은 고 1만큼의 용량이 낭비가 되거든요.

고 1만큼의 용량이 낭비가 되거든요.

그 용량을 뭐 딴 데 쓰지도 못하고 그냥 항상 무조건 우리가 이제 예를 들면 컨테이너 박스로 물건을 보내면 컨테이너 박스 크기에 꽉 딱 맞춰서 보내야지

이만큼 보낸다고 해서 뭐 딱히 더 빨리 가지도 않아요.

그러니까 이게 무조건 꽉 채워서 보내는 게 약간 용량상 이득이죠.

그래서 보통 32, 64, 128 이런 식으로 2의 n승 형태가 되도록 배치 사이즈는 보통 정합니다.

그래서 이제 우리는 32개씩 모델한테 넣어서 훈련을 시킬 거고 모델한테 넣어서 훈련을 시킬 거고 그 다음에 데이터는 골고루 섞어서 넣을 겁니다.

왜냐하면 이제 지금은 이미 섞여 있으니까 큰 상관은 없는데 가끔 뭐 예를 들면 0은 0끼리 몰려 있고 1은 1끼리 몰려 있고 이런 경우가 있어요.

그럼 0은 0끼리 몰려 있고 32개가 다 0이면 우리가 오차를 계산할 때 0밖에 없으니까 0쪽으로 편향이 된단 말이에요.

왜냐하면 0을 100% 아 무조건 0만 할 걸 이렇게 생각하게 된단 말이에요.

그래서 0하고 1하고 이제 골고루 섞이라고 우리가 데이터를 섞어주고요.

테스트를 할 때는 사실 굳이 섞을 필요가 없죠.

그냥 뭐 어차피 테스트는 뭐 파라미터를 최적화하지 않기 때문에 그냥 섞지 말고 점수만 매기면 됩니다.


```python copy
def fit(model, train_loader, criterion, optimizer, epochs):
    model.train()  # 훈련 모드
    for epoch in range(epochs):
        for i, (x, y) in enumerate(train_loader):  # 미니배치마다
            optimizer.zero_grad()  # 기울기 초기화
            y_hat = model(x)  # 모형 예측
            loss = criterion(y_hat, y.float().view_as(y_hat))  # 오차 계산
            loss.backward()  # 기울기 계산
            optimizer.step()  # 가중치 수정
            print(f'Epoch: {epoch}, Batch: {i}, Loss: {loss.item()}')

```



```python copy
fit(model, train_loader, criterion, optimizer, epochs=3) # 전체 데이터를 3회 학습
```



```python copy
total = 0
correct = 0
mean_loss = 0
model.eval() # 모형을 평가 상태로
for i, (x, y) in enumerate(test_loader):
    y_hat = model(x)
    pred = (y_hat.squeeze() > 0.5).long()  # 0.5를 기준으로 0, 1로 변환
    total += y.size(0) # y의 개수
    correct += (pred == y).sum().item() # 정답과 일치하는 경우
    loss = criterion(y_hat, y.float().view_as(y_hat)) # 손실
    mean_loss -= (mean_loss - loss.item()) / (i + 1) # 평균 손실 계산
print(f'Accuracy: {correct / total}, Mean Loss: {mean_loss}')

```

:::info[output]
```
Accuracy: 0.9995271867612293, Mean Loss: 0.01587683077442891

```

:::




그 다음에 이제 경사강법 함수를 만들어 줘야 되는데요.

이제 코드가 좀 복잡하죠.

그래서 하나씩 설명을 드리면 사실 이제 지금은 이걸 직접 우리가 짜는데 사실 뒤로 가면 우리가 이걸 직접 짤 필요가 없어요.

지금은 그냥 원리를 이해하는 목적에서 직접 짜는 건데 사실 우리가 이제 요 교시 지나면 다시 볼 일은 별로 없습니다.

어쨌든 원리를 좀 보면 일단 이제 우리가 모델을 받아서 모델을 훈련 모드로 세팅을 합니다.

트레인 이렇게 하면 훈련 모드로 바꾸거든요.

그 다음에 이제 에폭이라는 걸 하는데 이 에폭은 뭐냐면 우리가 데이터가 이제 6만 개가 있어요.

6만 개의 데이터를 처음부터 끝까지 한 번 보는 거를 1 에폭이라고 합니다.

그러면 우리가 만약에 총 에폭을 10 에폭으로 정했다.

그럼 그 얘기는 뭐예요?

6만 개를 처음부터 끝까지 한 번 보고 또 한 번 보고 또 한 번 보고 해서 이거를 10번 보겠다.

이런 얘기죠.

여러분들 공부하실 때 예를 들어서 뭐 내가 영어 공부를 할 건데 영어책을 10번 보겠다.

그럼 10 에폭을 공부를 하는 겁니다.

그래서 이제 에폭은 얼마로 정해야 되냐

이거는 좀 이따가 알아보도록 하고 일단은 뭐 우리가 몇 에폭을 할지 일단 정했다 치고 그럼 그 에폭에 대해서 이제 4는 반복을 합니다.

그래서 에폭이 10번이면 여기 밑에 있는 코드를 10번 반복하는 거예요.

5면 5번 반복하고 그 다음에 여기 이제 트레인 로더에 인유머레이트 한 다음에 또 4를 하는데 그럼 어떻게 되냐면 여기서 이제 트레인 로더에서 X랑 Y를 가져오는데 우리가 이제 32로 정해놨기 때문에 X 32개랑 그 X에 Y 32개를 가져오게 됩니다.

X 32개랑 그 X에 Y 32개를 가져오게 됩니다.

한 번에 32개씩 가져와요.

그럼 데이터가 이제 6만 개니까 32개씩 가져오려면 총 몇 번을 가져와야 되죠?

뭐 적당히 계산해 보시면 되겠죠.

6만 나누기 32에 보시면 뭐 몇 번 반복을 할 겁니다.

6만 나누기 32번만큼 요걸 이제 반복을 하겠죠.

그러면은 이제 요 반복을 할 때마다 뭘 하느냐 방금 설명드렸듯이 경상법의 알고리즘을 따라가는데 경상법의 알고리즘을 따라가는데 일단 옵티마이저 제로 그래드 이렇게 하면은 기울기를 초기화해요.

기울기를 일단 0으로 세팅합니다.

왜냐면 우리가 기울기를 모르니까 일단 0으로 세팅을 해놓고 그다음에 이제 모델에다가 X를 집어넣어요.

그다음에 이제 모델에다가 X를 집어넣어요.

그러면은 뭔가 예측값이 나오겠죠.

이 예측값을 Y핏이라고 하고 왜 Y핏이라고 하냐면 수학 기호를 쓸 때 이렇게 표기하기 때문에 그렇습니다.

그럼 이제 이게 우리의 예측이고 이게 Y플롯이 이제 실제에요.

그러면은 이 예측이랑 실제랑 크라이트리온이 로스 함수거든요.

이걸로 계산하면 로스 일종의 오차가 나오겠죠.

우리 예측하고 실제하고 잘 맞는지 안 맞는지 오차가 나올 건데 그러면은 로스점 빼고드 이렇게 하면은 알아서 미분을 해가지고 기울기를 구해줍니다.

아 오차가 이렇게 나온 걸 보니까 이 파라미터는 이 방향으로 늘려야 됩니다.

이 방향으로 줄여야 됩니다.

그러면은 옵티마이저한테 자 한 스텝 가라

이렇게 하는 거예요.

그럼 우리가 이렇게 있는데 여기서 아 늘려야 됩니다

이렇게 나오면 어 그래 늘리는 방향으로 한 스텝 가라 만약에 이렇게 돼서 아 줄여야 됩니다

이렇게 되면 아 줄이는 방향으로 한 스텝 가라 이겁니다.

그럼 그거를 우리가 계속 왔다갔다 왔다갔다 하면서 반복을 해주면은 6만 개의 데이터에 대해서 반복을 해주면 점점 좋아지겠죠

모델이 그래서 이거를 이제 반복을 할 때마다 화면에다가 찍어줄 거예요.

지금이 몇 번째 폭이고 그 중에 몇 번째 데이터인데 그 중에 손실이 지금 얼마다

이렇게 출력을 해주도록 하겠습니다.

그래서 fit 함수에다가 모델이랑 트레인 로더랑 크라이티리언이랑 옵티마이저랑 그 다음에 에포크는 이제 3번 이렇게 하도록 하겠습니다.

전체 데이터를 3번 공부하는 거죠.




```python copy
import matplotlib.pyplot as plt
weight = model.linear.weight.data # Linear 레이어의 가중치
w = weight.reshape(28, 28).detach().cpu().numpy() # 가중치를 28x28 이미지 형태로
plt.imshow(w, cmap='gray') # 시각화
```

:::info[output]
```
<matplotlib.image.AxesImage at 0x23d21fef5c0>
```

```
<Figure size 640x480 with 1 Axes>
```

![](data:image/png;base64,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)

:::



```python copy
import cv2
x = cv2.imread('hand.png', cv2.IMREAD_GRAYSCALE) # 이미지 불러오기

# 모형에 입력할 때는 (이미지 장수, 세로 길이, 가로 길이) 형태여야 함
# 한 장 짜리 이미지의 경우 0번 차원을 추가해야 함
x = np.expand_dims(x, 0) # 차원 추가 (28, 28) → (1, 28, 28)
x = torch.from_numpy(x).float() # 텐서로 변환
x = x / 255.0 # 정규화 [0~255] → [0~1]
p = model(x) # 모형 예측

```



```python copy
p = p.item()
1 - p, p
```

:::info[output]
```
(0.012828052043914795, 0.9871719479560852)
```

:::


그래서 이제 손글씨를 입력을 하려면 일단 그 시작을 누르시고 그림판을 검색하셔서 여시면 되고요

약간 이것저것 좀 해줘야 되는게 있는데 일단 하나씩 해보도록 하죠

일단 그림판을 열어 주시고요

그 다음에 이제 제가 슬라이드 만들었을때랑 UI가 좀 바뀌었는데 여기 보시면은 그 리본에 이미지가 있고 여기 옛날에는 크기 조정이라고 써있었는데 지금은 아이콘만 남아있어요

여기 보시면은 이렇게 이미지를 뭔가 확대하는 듯한 아이콘이 있습니다

여기 좀 확대해 드리면 요 아이콘을 누르시면 이런 창이 뜹니다

그래서 우리가 이제 그 모델에 입력한 이미지의 크기가 픽셀로 가로 세로 28이거든요

그래서 28 이렇게 해주시고 여기 중간에 파란 버튼인데 이걸 눌러서 꺼주세요

왜냐하면 이게 눌려있으면 이게 양쪽이 연동이 되거든요

꺼주고 여기도 28 이렇게 해주시면 픽셀 28 28 그래서 이제 확인을 누르시면은 이렇게 28 28 픽셀이 있는데 너무 작아서 우리가 여기다가 손으로 쓸 수가 없어요

그래서 여기 아래쪽에 돋보기 모양 스크롤을 쫙 키워주시면 이제 확대가 됩니다

그 다음에 이제 원본 데이터가 까만 바탕에 흰 글씨라서 여기 아이콘 중에 여기 위에서 두번째 버튼이 채우기 인데 이걸 누르고 눌러주시면 까만색으로 채우기가 되거든요

이걸 누르고 눌러주시면 까만색으로 채우기가 되거든요

그래서 까만색으로 만들어 주시고 그 다음에 흰색을 선택을 한 다음에 브러쉬를 누르고 여기다가 어 왜 안되지

잠깐만요

잠깐만요 마우스 왼쪽이 검정색이고 마우스 오른쪽이 흰색이고 아 그래요

이 바뀌었구나

아 우클릭을 하시고 그리면 되네요

뭔가 그림판이 좀 바뀌어 가지고 조금 굵게 그릴게

아 너무 굵다

그래서 제거합니다

그래서 조금 요거를 굵게 하신 다음에 뭐 원하시는 걸 그리시면 됩니다

0 이렇게 아니면 1 이렇게 일단 0으로 해보죠

0으로 해가지고 요거를 저장을 합니다

그래서 우리 그 그림 데이터 파일 있는 요 폴더에다가 hand.png 이렇게 저장을 해주시면 되요

이미 있긴 있는데 제가 써놓은게 있거든요

그리기 귀찮으신 분은 제가 그려놓은거 쓰시면 되요

그래서 이렇게 하시면 됩니다

그래서 다시 보여드리면 새로 만들게 해가지고 일단 여기서 이미지에 있는 요 오른쪽에 있는 요 버튼 누르셔서 픽셀 선택하고 28 28 가운데 있는 것은 풀어 주시구요

이렇게 확인하면 이미지 크기가 28 28 로 되구요

너무 작으니까 여기 밑에 있는 그 오른쪽 아래 돋보기 버튼으로 최대로 확대해 주시구요

그 다음에 이제 까만색으로 이렇게 칠해주시고 그 다음 브러쉬 선택하신 다음에 흰색 고르시고 우클릭 하셔가지고 이렇게 그리시면 됩니다

1 이렇게 하셔도 됩니다

그래서 이제 저장은 hand.png로 저장해 주시면 되요

그래서 요거를 이미지 파일을 불러와서 이미지 파일을 불러와서 아이고 아이고 이미지 파일을 불러와서 우리가 이제 그 원래 데이터에 보면은 그 가로 28 세로 28이고 맨 앞에 채널 수가 있거든요

그래서 익스팬드 딤즈 이렇게 하면은 요 모양을 바꿀 수 있습니다

그래서 원래는 가로 28 세로 28 이렇게 돼 있는데 요거를 한 차원을 추가를 해주는 거에요.

요거를 한 차원을 추가를 해주는 거에요.

쉽게 말하면 이렇게 어차피 여기가 이제 1이기 때문에 실제로 숫자가 더 늘어나지 않고 이렇게 해서 숫자가 784개가 있고 1 곱하기 28 곱하기 28에도 숫자가 784개 있습니다.

숫자의 개수는 똑같은데 그냥 모양만 바꿔주는 거에요.

그래서 익스팬드 딤즈 0 하면은 여기 맨 앞에가 0이기 때문에 맨 앞에 딤을 디멘전을 추가해라 확장해라

이런 얘기입니다.

그래서 요 x를 바꿔주고 그 다음에 이제 이게 지금 배열이라서 이걸 이제 다시 템블러에 넣어 주시면 됩니다.

그래서 이제 텐서로 바꿔주구요.

그래서 톳치점 from numpy 하면 이제 numpy 배열에서 텐서로 바뀌고 그 다음에 이제 실수니까 플러스로 변환을 해 주고요.

그 다음에 이미지에서는 정수로 0에서 255 까지가 있는데 원래 요 데이터는 0에서 1로 되어 있는 데이터 거든요

우리 학습시킨 데이터가.

그래서 학습된 데이터랑 똑같이 해 주도록 하기 위해서 255.0 으로 나눠 가지고 요거를 255 로 나눠주면 0은 그대로 해 줍니다.

그래서 학습된 데이터랑 똑같이 해 주도록 하기 위해서 255.0 으로 나눠가지고 요거를 255 로 나눠 주면 0은 그대로 해 줍니다.

그래서 학습된 데이터는 0에서 1로 되어 있는 데이터거든요.

그거는 하나.

그래서 학습된 데이터는 하나.

0이고 255는 1이 되겠죠.

그래서 이렇게 하면은 우리가 트레이닝 된 데이터랑 똑같은 형식이 됩니다.

그래서 항상 그 데이터의 형식은 트레이닝 시킨 거랑 사용하는 거랑 똑같은 형식이어야 돼요.

그래서 여기 모델 204쪽 이 코드를 실행을 하시면 지금 확률이 나오는데 확률이 0.001 이렇게 나오죠.

우리가 지금 제가 쓴 숫자가 뭐냐면 제가 쓴 숫자가 제가 쓴 숫자가 이거거든요.

0을 썼으니까 나오는 확률은 1일 확률입니다.

0.001이라는 건 거의 99.999% 0이라는 얘기죠.

1일 확률은 0.01% 밖에 안되니까 그럼 반대로 이제 이거를 까만색으로 이렇게 덧칠을 하고 흰색으로 1이라고 쓴 다음에 다시 저장을 하고요.

이 코드를 다시 돌리면 이렇게 1일 확률이 86.8% 이렇게 나오게 됩니다.

이거는 1일 확률이 86.8%다.

다시 지우고 이번에는 0을 좀 새로 이렇게 써보면 이런 식으로 써보고 다시 저장하고 돌려보면 이제 확률이 거의 0에 가까워지죠.

또 지우고 지우고 다시 저장하고 돌려보면 이제 확률이 거의 0에 가까워지죠.

다시 지우고 다시 저장하고 돌려보면 이제 확률이 거의 0에 가까워지죠.

이번에는 1을 좀 반듯하게 써볼까요?

이렇게 써서 저장하고 돌려보면 96% 이거는 1이다.

이렇게 나오게 됩니다.

이게 204쪽 코드인데 이미지를 저장을 받고 가면서 잘 인식이 되는지 해보시면 되겠습니다.

잘 인식이 되는지 해보시면 되겠습니다.
*/}