

# 전이 학습

전이라는 것은 옮겨간다

이런거에요

옮겨간다 영어로는 트랜스퍼에 해당되는 표현인데요 기존의 머신러닝의 문제점은 우리가 하려고 하는 어떤 일이 있으면 과업이 있으면 그거에 맞춰서 모델하고 데이터를 만들어야 되는데 모델 만드는 거 우리 해보셔서 아시겠지만 어렵습니다

귀찮아요

복잡하고요

데이터 만드는 거 당연히 또 어렵죠

근데 딥러닝은 데이터를 거기다 더 많이 만들어야 되거든요

거기다가 데이터가 많아지면 컴퓨터 성능도 높아져야 된다 어려운 일 투성이에요

이렇게 어려워서야 우리가 쉽게 활용을 하기가 어렵죠

그래서 이제 전이학습이라는 개념이 나오는데 전이학습은 원래 심리학 용어입니다

심리학 용어인데 여러분이 예를 들면 피아노를 배웠다

그러면 오르간을 잘 칠 수 있겠냐 금방 배우겠죠

예를 들면 또 여러분이 또 뭐가 있을까요

예를 들면 클래식 기타를 배웠다

그러면 전자기타도 좀 다르긴 하지만 그래도 좀 쉽게 배울 수가 있습니다

여러분이 자전거를 탈 줄 안다

오토바이도 좀 쉽게 배우겠죠

하나를 배우면 비슷한 거를 쉽게 배우는 거 그거를 이제 쉽게 배운다 보다는 다른 하나를 배우고 그걸 바탕으로 다른 걸 배우는 거를 전이학습이라고 합니다

학습이 옮겨가는 거예요

트랜스퍼되는 거 근데 이게 이제 사실은 사람도 전이학습이 그렇게 쉽지 않거든요

그래서 예를 들면은 우리가 학생들이 수학 어려운데 수학 왜 배우나요

이러면은 어른들이 입장에서 할 말이 별로 없어요

수학을 왜 배우나

그러면 이제 수학을 배우면 논리적 사고가 수학을 배우면 논리적 사고가 논리적 사고가 함양이 돼 이런 얘기 하거든요

이런 것도 이제 일종의 전이입니다 수학을 배웠는데 논리적 사고가 커지는 거죠

실제로 실험적으로 보면 거의 근거가 없어요

수학을 배우면 수학은 잘하게 되는데 수학 외적인 어떤 논리적 문제를 주면 그냥 수학은 수학이고 그냥 그 문제는 그 문제입니다

우리나라 사람들이 세상에서 수학을 제일 잘하는데 우리나라 사람들이 그럼 세상에서 제일 논리적으로 사고하냐

뭐 그렇지 않죠

그냥 수학만 잘합니다

그러니까 사람도 이 전이학습이 굉장히 어려워요

잘 안 됩니다

우리가 이제 하나를 배우면 열을 난다

이런 말은 과학적으로는 사실이 아니고 어쨌든 사람도 쉽지 않은데 우리가 이제 이 AI를 어떻게 이 전이학습을 잘하게 할 거냐 만약에 이게 된다고 하면은 우리가 이제 과제마다 모델을 새로 만들지 않아도 되고 과제마다 데이터를 새로 만들지 않아도 되기 때문에 뭐 만들더라도 조금만 만들기 때문에 굉장히 쉬워지는 거죠

그래서 이제 전이학습이라는 거는 어떤 근원 문제가 있고 대상 문제가 있을 때 근원 문제 학습시킨 거를 이제 대상 문제에 적용을 해서 우리는 대상 문제는 날로 먹겠다

약간 이런 거고요 그래서 이 근원 문제에다가 학습시키는 거를 사전학습이라고 하고요 대상 문제에다 학습시키는 거를 미세조정이라고 하는데 왜 사후 학습이 아니라 미세조정이냐

이거는 뭐 별로 학습할 게 없어요

이미 다 학습한 거니까 내가 다 배워서 안단 말이에요

그러니까 예를 들면 우리가 회사 와서 이제 처음에 일 배울 때 배우는 게 많지만 한글부터 새로 배우지는 않잖아요

그건 다 배운 거란 말이에요

근데 회사에서 쓰는 특수한 용어들 특수한 표현들 우리 회사만 쓰는 어떤 말들 이런 건 조금 배워야겠죠

그러니까 정말 미세하게 조정만 좀 해 주면 된다

대상 문제에 대해서는 그런 얘기입니다

그래서 어제도 얘기 드렸지만 이 프리트레인의 P가 GPT의 P입니다

GPT는 사전 학습을 굉장히 방대하게 인터넷으로 구할 수 있는 텍스트를 다 학습을 시켜놨기 때문에 영어를 한국어로 번역해라

이런 것도 할 수 있고 한국어를 영어로 번역해라

이런 것도 할 수 있고 별 희한한 거를 다 할 수가 있는 거죠

그다음에 이제 그렇게 채집 GPT처럼 그런 식으로 방대한 어떤 분야에 대해서 사전 학습시켜 놓은 모델을 요즘에 기초 모델이라고 합니다

사실 자연어처리는 제가 생각할 때 약간 끝난 분야에요.

사실 자연어처리는 제가 생각할 때 약간 끝난 분야에요.

실제로 채집 GPT 나오고 나서 대학에 계신 분들 중에 공황 상태에 빠지신 분들이 되게 많습니다

아 이제 뭐 연구하나 뭘 연구해도 채집 GPT보다 좋은 걸 만들기는 어려울 텐데 그리고 채집 GPT 같은 거는 정말 방대한 데이터 학습을 시키기 때문에 돈이 엄청나게 많이 들었거든요

대학 수준에서 만들 수 있는 그런 게 아니에요

그러니까는 이제 아 이제 뭐 연구해야 되지

다들 곤에 빠진 거죠

컴퓨터 비전은 아직 그 정도는 아닌데 컴퓨터 비전은 아직 그 정도는 아닌데 컴퓨터 비전도 이제 점점 그쪽으로 가고 있습니다

그래서 컴퓨터 비전 커리큘럼도 이제 점점 바뀌고 있는데 우리도 이제 보면은 여러 가지를 코드로 짜고 이렇게 했는데 만약에 컴퓨터 비전의 기초 모델이 아주 잘 만들어진다

그 다음에 장기적인 전망은 이 두 가지가 결합이 되는 거거든요

그러면 그 얘기는 뭐냐면 그냥 말로 채집 GPT한테 말로 시키면 대충 알아듣고 잘 한단 말이야 이미지 처리 같은 것도 여기 태권브이 찾아봐

그러면은 이때까지는 우리가 태권브이 아까 뭐 어제 했던 것처럼 복잡하게 찾아봐 처리하는데 어 이거 태권브이 찾아봐 그냥 말로 하면은 되는 거죠

근데 가만히 생각하면 우리가 사람들한테도 그렇잖아요 저기 가서 저거 파란색 휘어져 있는 막대기 가져와라

그러면은 태어나서 처음 그 막대기를 본 사람도 파란색 휘어져 있는 막대기를 가져올 수 있습니다

왜냐면 자연어도 학습돼 있고 시각도 학습이 돼 있기 때문에 할 수 있는 거죠

우리가 컴퓨터도 그렇게 만들면 되겠죠

그렇게 멀리 남았다고 생각되지도 않아요

실제로 채집 GPT 유료 버전 쓰시면 어떤 것도 되냐면 한번 집에 가서 해보세요

그 메뉴판 사진하고 테이블에 음식 먹은 병이랑 뭐 이렇게 이런 거 사진 사진 두 장을 주고 나 먹은 거 얼마 먹었는지 계산해 주라고 하면 계산해 줍니다

지금 벌써 그 정도는 되거든요

그러니까 멀리 남지 않았다

제 생각에 한 5년 봅니다

어쨌든 그래서 이제 우리가 전의학습을 하면은 다른 문제에 학습시킨 모형을 약간만 미세조정해서 쓸 수 있으니까 데이터도 안 들고 계산비용도 안 들고 그래서 성능은 높아지면서 난이도는 낮아지니까 굉장히 이제 쉬워지는 거죠

그리고 이제 어떤 전문가의 전문성이 좀 필요가 없어지는 그런 효과가 생겨나게 됩니다

그래서 이제 미세조정 전의학습을 하는 방법이 여러 가지가 있는데 모델이 있으면 모델을 통째로 미세조정해서 쓰는 방법이 있고요 그다음에 모델의 뒷부분에다가 뭔가 분류기를 붙여 가지고 쓰는 방법이 있고 아예 그냥 미세조정조차 하지 않는 방법 이런 거를 제로샷 러닝이라고 하는데요 이거는 채찍피티 같은 건 아예 미세조정도 안 하는데 우리 그냥 쓰잖아요

아무 생각 없이 쓰는데 그 정도 가면은 제로샷 러닝이라고 합니다 아무런 추가 학습이 없는 거죠

그래서 이제 우리가 오늘 해 볼 거는 이제 미세조정을 하는 방법입니다

이제 이 뒤로 가려면 모델이 더 크고 더 좋아야 돼요

그러니까 우리가 생전 예를 들면 조선업에 종사해 본 적이 없는 사람인데 딱 회사에 오자마자 일을 잘하려면 이 사람은 예를 들어 무슨 공학의 천재라 가지고 기계와 관련된 건 뭐든지 다 잘한다

뭐 약간 이런 정도가 돼야겠죠

근데 보통 사람은 어쨌든 좀 조정을 해야 됩니다

우리 회사 들어오면은 짧게는 한 달 뭐 길게는 몇 달까지도 연수 같은 거 받고 그러잖아요

그러니까 좀 조정을 해 줘야 되는 거죠

이때까지 배운 거 가지고 못 하니까 그래서 우리가 이제 미세조정을 할 거고 미세조정은 지금까지 한 거보단 더 쉽습니다

왜냐면 남이 만들어 놓은 거 갖다 쓰는 거니까 그래서 이제 언제 어떤 방법을 사용해야 되냐 하면은 우리가 제일 먼저 배웠던 OpenCV를 이용해서 고정된 알고리즘을 쓰는 방식은 데이터가 필요가 없어요

그냥 코드를 짜면 됩니다

그 대신에 할 수 있는 일이 좀 단순해요 직선찾기 원찾기 사진 뒤집기 이런 거고 그 다음에 이미지의 종류는 무관하고 하드웨어 성능도 낮아도 됩니다 일이 쉽기 때문에 그래서 이제 일이 좀 복잡해서 복잡해지면 머신러닝 딥러닝을 하거나 아니면 그것에 전의학습을 하는데 직접 모델을 만드시려면 데이터가 많아야 됩니다

그리고 난이도는 뭐 한 중간 정도 왜냐면은 좀 더 어려운 걸 할 수 있는데 예를 들면 신발하고 티셔츠를 구별한다든가 이런 건 좀 더 어렵죠

그래서 이제 좀 더 중간인데 이걸로 어려운 거는 뭐 하려면 할 수 있는데 우리가 아주 전문가가 아닌 이상은 좀 어렵습니다

그다음에 이제 그 해상도가 높고 높은 데이터도 다루기가 힘들어요

왜냐면은 뭐 이것도 이론적으로 할 수는 있는데 그러려면은 데이터가 더 더 더 더 더 더 더 많아야 됩니다

그래서 내가 하려는 게 굉장히 고해상도 이미지에서 어떤 정밀한 뭔가를 하려고 한다 라고 하면 직접 모델을 만드시는 거는 좀 현실적으로 어렵다

그만큼의 데이터를 구하는 게 사실 어렵기 때문에 대신에 이제 하드웨어는 보통 정도면 됩니다

그 다음에 이제 전의학습을 하는 경우는 이미 만들어 놓은 거를 우리가 이제 가진 것들도 있습니다.

그리고 이제 수학을 가지고 가서 튜닝을 하는 거니까 예를 들어 회사로 치면은 사람을 처음부터 가르치는 게 아니고 경력자를 채용을 하는 거죠

데이터가 적어도 됩니다

경쟁 회사에서 우리 회사로 이직해왔어요

그러면 아 그쪽에서는 이렇게 한다고요?

아 우리 이렇게 하는데요?

한마디만 해주면 금방 알아듣겠죠

어쨌든 이 바닥에 일을 한 사람이니까 데이터가 적어도 되고 복잡한 일도 시킬 수가 있습니다

왜냐면 다 배워 가지고 왔기 때문에 다 할 줄 알아요

조금만 조정해 주면 됩니다

그리고 해상도가 높거나 크기가 큰 사진도 잘 합니다

이미 잘 찍은 거니까요.

잘 하는 거니까 그 대신에 모델 자체가 보통 규모가 크기 때문에 돌리려면 좀 성능이 높은 하드웨어가 필요합니다

그래서 이제 내가 하려는 일이 이 중에서 어디에 해당되냐

이거를 잘 생각을 하셔가지고 맞춰서 하시면 되겠죠

그래서 이제 우리가 써 볼 거는 허깅페이스라는 회사예요

허깅페이스는 회사고 이 회사에서 만든 트랜스포머스라는 라이브러리를 쓸 건데요

이 라이브러리는 트랜스포머 계통의 모형을 간편하게 쓸 수 있는 라이브러리고 파이토치 기반으로 되어 있기 때문에 파이토치랑 섞어서 쓸 수도 있습니다

그래서 이것도 설치를 하면 아나콘다 프롬프트를 띄우셔가지고 설치를 해 주시면 됩니다

설치가 되겠고요 여기 아나콘다 프롬프트의 느낌표는 빼고 입력해 주시면 됩니다

자 설치 잘 되시나요

그래서 우리가 이제 모델을 찾아야 되는데 사이트가 있습니다

허깅페이스.co 여기 가시면 회원가입은 안 하셔도 되고요

상단 메뉴에 보시면 모델즈라고 있어요

여기 들어가시면 지금 이제 오늘 날짜로 111만 9천개 한 편이에요

111만 9천개 한 120만 개의 모델이 있는데 여기에서 이제 여러분이 원하시는 거를 찾으시면 됩니다

근데 이제 처음부터 찾으려면 어려우니까 여기 이제 왼쪽을 보시면 그 분류 태그가 있는데 우리가 이제 컴퓨터 비전으로 가보면 이제 뭐 데프트 에스티메이션 이미지 클래시피케이션 오브젝트 디텍션 뭐 이렇게 쭉 있죠

그래서 이제 여러분이 하고 싶은 작업이 이 안에 어쨌든 들어가면은 이제 가져다 그냥 쓰면 됩니다

없으면 아 여기서부터는 이제 고뇌가 시작이 되는 거죠

그래서 우리는 이제 어 그렇고 그 다음에 여기 상단에 보시면 이제 다른 메뉴도 좀 소개 드리면 데이터 세트라고 해서 여기는 이제 공개된 데이터 세트 여기 보시면 데이터가 한 24만 종류의 데이터가 있는데 만약에 내가 데이터가 필요하다 그러면은 여기서 이제 데이터를 찾으시면 됩니다

그래서 뭐 이미지 데이터 같은 것도 여기 보면은 여러 가지가 있죠

그 다음에 이제 스페이스 라고 있는데 이 스페이스는 뭐냐면 이런 그 모델들을 가지고 앱을 만들어 가지고 여기 사이트에 올릴 수가 있어요

그래서 예를 들면 AI 코믹 팩토리 이러면은 이제 만화를 자동으로 만들어 준다던가

그 다음에 이제 제가 이 허깅페이스가 회사라고 얘기 드렸는데 이 회사가 요즘에 엄청 잘 나가는 회사인데 이제 뭘로 돈 버냐면 이런 툴들을 다 제공을 해줍니다

다 공짜로 제공하고 모델들도 다 제공을 해줍니다.

그래서 이제 이 사이트에 다 다운 받는 것도 다 무료로 올려주고 다운 받는 것도 다 무료고 다 공짠데 이제 어디서 돈을 받냐

하면 프로 어카운트를 하면은 이제 GPU를 얼마를 쓰게 해주겠다

이런 식으로 해서 이제 약간 결국 GPU 장사를 하는 거예요

GPU 빌려주기 장사 하드웨어 빌려주기 뭐 이런 거 그럼 사람들이 어차피 여기 걸 쓰니까 우리가 이제 어 나 GPU 필요한데 아 허깅페이스에서 주네

그럼 여기서 쓰는 거죠

이런 식으로 이제 돈 버는 회사고 굉장히 잘 나가는 걸로 알고 있어요

왜냐면 어차피 이제 다들 여기 오니까 약간 포탈같이 된 거죠

여기가 일종의 그래서 여기 이제 돌아보시면 뭐 재밌는 게 굉장히 많으니까 심심할 때 한번 보시면 뭐 희한한 거가 많이 있고요 그래서 우리가 이제 태스크를 선택을 하는데 우리는 이미지 분류를 해 볼 겁니다

여기 요 다음에 액세스 토큰 이런 얘기인데요 요거는 안 하셔도 됩니다

요거 안 하셔도 돼요

그 다음에 노트북 로그인 이것도 안 하셔도 돼요

그 요 두 개는 뭐냐면 이제 여러분들이 업로드 하시려면 로그인을 하셔야 되거든요

다운로드 할 때는 로그인 안 해도 되는데 업로드 하려면 로그인 해도 되는데 우리 업로드를 안 할 거기 때문에 상관없고 그 다음에 이제 모델이 여기 여러 개가 있는데 이미지 클래시피케이션 이미지 분류로 들어가 보면 모델들이 여러 개가 있어요

여기 보면은 14000개의 모델들이 있는데 우리는 이제 뭘 쓸 거냐면 여기 보면 수업시간에 배운 레즈넷도 여기 있고요 그 다음에 이제 구글에서 만든 vit 베이스 이렇게 되는데 이 vit가 비전 트랜스포머 어제 했던 비전 트랜스포머입니다

그래서 보시면은 레즈넷이 인기가 좀 더 좋네요

레즈넷은 다운로드 제일 많이 받은 거는 레즈넷이 1등이네요

비전 트랜스포머는 좀 인기가 떨어지는 것 같습니다

하지만 좋아요로 보면은 비전 트랜스포머가 좋아요는 더 많아요

Most likes 그렇죠 팽팽합니다

이런 모델이 있었거든요

그래서 이제 이런 거 바로 여기서 해볼 수 있거든요

그래서 이미지를 여기다 올리면 야한 건지 아닌지 바로 판정을 해줍니다

이런 것 기능이 있어요

그래서 우리는 여기 VIT 베이스 패치 16 224 요거를 해볼 건데 여기 이제 보면은 설명이 있어요

그래서 영어로 돼 있으니까 한국어로 번역해서 봅시다 그래서 비전 트랜스포머 모델은 해상도 224 224에서 1400만 개의 이미지를 사전 학습을 하고 그 다음에 224 224에서 100만 개의 이미지로 미세 조정을 했습니다

그 다음에 이미지는 16 X 16 단어의 가치가 있다

이거는 약간 번역이 잘못된 건데 책 저기 논문 제목입니다 논문 제목 등 논문에 소개되었으며 이 저장소에서 처음 공개되었습니다

그래서 원래는 작스로 만들었고 그 다음 파이토치로 이제 변환을 했다

그 다음에 그 다음에 그 다음에 그리고 이렇게 이...

모델 카드가 있는데 설명 설명은 VIT 논문 쓴 사람들이 직접 쓴 게 아니고 직접 안 써 줬기 때문에 설명은 허깅 페이스에서 대신 만들었다

뭐 이런 얘기고 주저리 주저리 설명 있죠

설명이 있고 요 뒤에 가면은 사용법이 코드로 되어 있습니다

그래서 몇 줄 안 되는데 이렇게 쓰시면 된다 설명이 있고요 어떤 데이터로 학습을 했고 논처리는 어떻게 했고 프리트레이닝은 어떻게 했고 논문을 인용을 하시려면 어느 논문을 인용하면 되는지 이렇게 설명이 쭉 있어요

그래서 이런 식으로 설명이 있기 때문에 참고하시면 되는데 오늘은 저랑 같이 한번 써보도록 하겠습니다

그래서 우리가 이제 355쪽으로 넘어가서 여기 폰트가 제가 서식을 지정을 안했네요

355쪽은 뭐냐면 우리가 폴더에서 데이터 불러오는 게 있는데 파이토치에도 폴더에서 데이터 불러오는 기능이 있는데 트랜스포머스도 이제 데이터 불러오는 기능이 있습니다

사용법은 비슷한데 로드 데이터셋 한 다음에 데이터 폴더를 지정을 해 주시면 거기에서 지정되고 그 다음에 파이토치 버전이랑 차이는 파이토치는 트레이는 트레인 폴더라고 지정을 해주고 테스트는 테스트 폴더라고 지정을 해주는데 로드 데이터셋 함수는 그냥 상위 폴더만 지정을 하면 캐치앤독스필더들 하면 그 안에서 자기가 트레인 폴더, 테스트 폴더를 자동으로 같이 불러와 줍니다

그 다음에 여기 원래 강의 자료에 데이터 슬래시 이렇게 되는데 요거를 빼주세요 그러면은 0번이 고양이고 1번이 강아지다 자기는 요렇게 데이터를 불러왔다 라고 표시를 해주게 됩니다

그 다음에 우리가 이제 아까 보면은 구글이 만든 vit-base-patch16-224-in21k 이 모델을 쓸 건데 이 모델을 어떻게 쓰면 되냐면 그냥 그 모델 이름을 autoimageprocessor.fromfreetrained 그러니까 사전 학습된 거에서 가져와라 라고만 하시면 됩니다

아 요거는 모델이 아니고 이미지 프로세서라고 해서 이제 전처리를 해주는 거예요

그래서 우리가 어제는 전처리 코드도 직접 짰잖아요

전처리도 그냥 얘네가 다 제공을 해주기 때문에 그냥 거기다 그냥 넣기만 하면 됩니다

우리는 그래서 요거는 그냥 복사해서 이거 쓰시면 되고요 지금 이게 그 진행 막대가 안 나오는데 원래 이제 다운 받으면 막대기가 쫙 올라가거든요

지금 이게 그 진행 막대가 안 나오는데 원래 이제 다운 받으면 막대기가 쫙 올라가거든요.

그래서 뭐 이게 빨간색으로 나오긴 한데 이거는 에러는 아니고요 뭐 그냥 안내 메세지 같은 겁니다

그래서 이렇게 하시면 되고 그 다음에 이제 사이즈는 사실 224224인데 우리가 문서에 보면 나오기 때문에 굳이 이렇게 코드를 할 필요는 없거든요

지금 제가 그냥 코드로 해 놨는데 사실 그냥 문서 보면은 224224로 하라고 돼 있습니다

근데 사실 이제 요 코드를 사용하시면 여기 이미지 프로세서 안에 내장돼 있는 정보를 이용을 해가지고 자동으로 그 숫자를 만들어줘요

그래서 코드가 좀 복잡해 보이지만 결국에는 쇼티스트 엣지가 얼마냐

이거 찾아오는 코드입니다

그래서 보면은 사이즈는 224224다

요것도 해주시고요

그 다음에 이제 전처리를 해야 되는데 우리가 이제 그 전처리하는 거 어제 했었죠

그래서 이제 보면은 투텐서 이렇게 있고 그 다음에 노멀라이즈 이거는 뭐냐면 우리가 이제 이미지 밝기를 조절해야 되는데 그 아까 문서에서 보시면 여기 문서에서 보면은 모델 설명에 224224로 조정이 되고 평균과 표준편차를 통해서 정규화가 됐다

이런 얘기가 있는데 그래서 요거는 그 평균과 표준편차로 이제 조정을 밝기를 조정을 해주는 그 작업입니다

그 다음에 원본 이미지에서 랜덤 리사이즈드 크롭은 이미지를 통째로 쓰지 않고 이렇게 좀 잘라 가지고 랜덤하게 잘라서 쓰는 거죠

그래서 잘라 가지고 랜덤 리사이즈드 크롭을 랜덤 리사이즈드 크롭을 랜덤 리사이즈드 크롭을 랜덤 리사이즈드 크롭을 잘라 가지고 사이즈 224224에다가 맞춰 줘라

이런 얘기입니다

이건 약간 증강을 해 주는 건데 증강을 해 주면 되겠죠

그래서 이제 트랜스폼 펑션 해 가지고 우리가 여기 만들어 놓은 언더바 트랜스폼 이게 우리 변환인데 원래 이미지에서 그 RGB 값이 있으면 그거를 트랜스폼을 해 줘라

이런 얘기입니다

그래서 우리 데이터셋에 위드 트랜스폼 하면은 이 트랜스폼을 일괄 적용해 주게 됩니다

여러분들이 나중에 이제 쓰실 때는 요것만 수정을 해 주시면 돼요

랜덤 리사이즈드 크롭만 빼주시던지 빼고 그냥 리사이즈드 크롭으로 바꾸던지 하시면 됩니다

아니면은 여기다 뭐 다른 증강을 원하시는 게 있으면 우리 어제 배웠죠

다른 증강 로테이션 이런 거 있잖아요

여기다가 넣으시면 돼요

그래서 이제 전처리를 또 임포트를 빼 먹었네요

아 이게 원래는 교환해서 이어지는 거라서 변명을 좀 하자면 임포트 트랜스폼 펑션 그러면은 이제 전처리는 끝났습니다

이거 이제 전처리 끝입니다.

요것만 추가해 주세요

원래 교환이 쭉 이어지는 거라서 앞에 나온 거를 생략을 했는데 임포트 일어나면은 요거 하나 추가해 주시면 됩니다

자 그 다음에 이제 전처리가 다 됐으면 모델을 로딩 해 주는데요

그 모델 로딩 하는 거는 똑같이 여기 체크포인트라고 되어 있는데 모델 이름입니다

모델 이름하고 그 다음에 여기 앞에서 이제 했던 뭐 0번이 고양이고 1번이 강아지고 요런 정보를 같이 넣어줍니다

모델 로딩을 해 주시면은 역시 진행 그래프가 안 나오는데 이제 모델을 다운로드 받아요

근데 요거 시간이 좀 걸리거든요

왜냐면 모델이 용량이 좀 있습니다

용량이 얼마 안 되네요

86메가 밖에 안 되네

86메가라서 얼마 안 되는데 하여간 좀 다운로드 하는데 시간이 걸려요

아 다 됐네요

뭐 쓱 어쨌든 모델을 다운을 받고요 그 다음에 우리가 이제 정확도 평가를 해야 되는데 정확도 평가는 그 트랜스포머스에서는 evaluate라는 라이브러리를 쓰는데 그래서 여기다가 이제 accuracy라고 써주면은 요걸로 이제 정확도를 평가를 해줍니다

그래서 요 함수의 내용은 우리가 예측을 하면은 요게 우리가 예측을 하면은 이게 우리가 예측을 하면은 이게 우리가 예측이 있고 이 레이블이 실제에요

그래서 이제 argmax는 뭐냐면 그 예측이 뭐가 나오냐면 이제 확률들이 쭉 나오거든요

강아지일 확률 고양이일 확률 요거는 우리 아침에 퀴즈 봤듯이 그 이항분류라도 무조건 다항 분류로 구현이 돼 있습니다

그러니까는 예측이 두 개가 나와요

그래서 그 예측 중에 argmax는 뭐냐면 어느 쪽이 크냐

이겁니다

어느 쪽이 큰가

우리 이제 고양이 강아지에 대해서는 이제 고양이 강아지 두 개의 확률이 나올 테니까 고양이 확률이 크냐 강아지 확률이 크냐

이거를 찾는 거예요

그래서 요 prediction은 우리가 이제 고양이가 크냐 강아지가 크냐

이거고 고양이가 크면 고양이라고 나오고 강아지가 크면 강아지라고 나오는데 레퍼런스는 이제 정답이죠

실제 데이터에는 뭐라고 돼 있냐 그러면은 요 두 개가 얼마나 일치하는지를 카운트해서 정확도를 계산을 해줍니다

그래서 요것도 뭐 사실 그냥 복사해서 쓰시면 되고요 그래서 다른 지표를 쓰실 분들은 여기 뭐 accuracy 대신에 이렇게 evaluate 홈페이지에 가면 다른 지표들이 쫙 있거든요

거기서 지원하는 지표들을 쓰시면 됩니다

그래서 이거는 그 evaluate는 여기 허깅페이스 홈페이지에 가시면 여기 pricing 옆에 이렇게 조그맣게 이렇게 조그맣게 이렇게 조그맣게 그 햄버거 모양 버튼인데 요거 누르면은 아 그 옆에 왼쪽에 docs라고 있거든요

docs가 이제 공식문서 모음인데 pricing 옆에 왼쪽에 여기 보시면은 evaluate가 여기 있죠

evaluate 여기 들어가면은 이제 how to guide 해가지고 올바른 매트릭 선택하기 뭐 이런 게 있거든요

여기 들어가 보면은 그 지원하는 어 여기가 아닌가 퀵투어에 있나 여기 이제 여기 보시면은 뭐 그 아닌데 어디 있었지 왜 없지

요건 제가 다시 찾아보고 얘기 드릴게요

하여간 요거 여기서 그 지표들을 이제 다 목록을 볼 수 있고요 그래서 뭐 큐러씨 말고 다른 지표를 이제 쓰고 싶으면 그 지표를 이제 쓰시면 됩니다

그다음에 훈련을 이제 설정을 해야 되는데 이것도 뭐 그냥 복사해서 쓰시면 되는데요 데이터 콜레이터는 뭐냐면 그냥 데이터를 우리가 32개씩 뭐 이렇게 합쳐주는 건데 이거는 그냥 뭐 딱히 설정할 건 없고요 그다음에 이제 출력 디렉터는 이제 뭐 이런 거에요.

또 디렉터는 드래그 스토리 이거는 이것도 경로명에 한글이 들어가면 안 됩니다

그래서 뭐 임시디렉터리를 하시든지 하시면 되는데 그냥 c 땡땡 슬래시 마이 모델 슬래시 이렇게 하면은 그냥 c 드라이브에 마이 모델에 저장을 해줍니다

그래서 학습이 끝나면 자동으로 저장이 되고요 그다음에 이제 나머지는 특별히 건드릴 건 없는데 여기 러닝 레이트를 보면은 학습률을 설정하는데 보통 학습률을 굉장히 작게 설정하거든요.

그래서 5,2-5 하면은 5X10에 맞춰서 설정하는 거예요.

그래서 5,2-5 하면은 5X10에 맞춰서 설정하는 거예요.

그러니까 0.00005 정도에요.

보통 이렇게 작게 설정해줍니다

우리 어제 할 때 보면은 0.01이나 0.001 정도로 10에 마이너스 2승이나 3승 정도로 해줬잖아요

일반적인 학습에서 하는 거보다 훨씬 작은 거의 백분의 일이나 천분의 일 정도로 작은 학습률을 쓰는데 왜냐면 말 그대로 미세조정이라서 그래요

그다음에 이제 나머지는 뭐 특별히 건드리실 거 없고 그래서 요거 요거 또 요거 또 그냥 이제 복사해서 쓰시면 됩니다

그냥 이제 복사해서 쓰시면 됩니다

아 요거 버전이 안 맞는다구요 여기 보시면은 플리즈런 해가지고 PIP 인스톨 트랜스포머스 각괄화하고 토치 이렇게 하라고 돼 있는데 이렇게 에러 나시면 이거 복사하셔가지고 이렇게 에러 나시면 이거 복사하셔가지고 이렇게 에러 나시면 이거 복사하셔가지고 우리 아나콘다 프롬프트에 요거 다시 붙여 넣어주세요 이렇게 설치를 해주시고 그다음에 요거를 여기 그 동그라미 설치를 요거 다시 해주신 다음에 동그라미 버튼을 눌러서 재시작을 해주세요

메모장에다가 적어 드리면 요거 아나콘다 프롬프트에 요거 다시 붙여 넣어주세요 요거 아나콘다 프롬프트에 요거 다시 붙여 넣어주세요

메모장에다가 적어 드리면 그 다음에 주피터 노트북 재시작 해주시고 그 다음에 이제 런 해가지고 여기 보시면 런 올 셀즈라고 있습니다

다 다시 실행해 주시면 됩니다

런 홈에서 이제 런을 하시면 됩니다

그러면은 이제 지금 보시면 에러 없이 잘 실행이 됐죠

그래서 이제 이게 트레이닝이 시간이 좀 오래 걸려서 트레이닝을 돌려보면 트레이닝 뭐 옵션이 뭐 많이 있는데 모델은 모델 우리가 그 지정한 모델을 돌려라

이런 거고 아그스는 방금 설정한 요 설정으로 해라

이런 얘기고 데이터 콜레이터는 그냥 이것도 아까 우리가 데이터 합치는 거 디폴트로 그냥 합쳐 주면 돼요

그다음 트레인 데이터 세팅은요?

이 데이터로 훈련한 다음에 이 데이터로 평가를 해라

이런 거고 그 다음 전처리는 아까 받아온 이미지 프로세서로 전처리해라

이런 거고 컴퓨터 매트릭스는 이걸로 평가해라

이런 겁니다


```python copy
# transformers의 자체 데이터셋 함수
from datasets import load_dataset

dataset = load_dataset("imagefolder", data_dir="cats_and_dogs_filtered") # data/ 삭제
# 레이블
labels = dataset["train"].features["label"].names
label2id, id2label = dict(), dict()
for i, label in enumerate(labels):
    label2id[label] = str(i)
    id2label[str(i)] = label
id2label 

```



```python copy
# 모델의 이미지 프로세서
from transformers import AutoImageProcessor
checkpoint = "google/vit-base-patch16-224-in21k"
image_processor = AutoImageProcessor.from_pretrained(checkpoint)

# 모델이 처리하는 이미지 크기
size = (
    image_processor.size["shortest_edge"] # 가장 짧은 변의 길이
    if "shortest_edge" in image_processor.size  # 만약 지원한다면
    else (image_processor.size["height"], image_processor.size["width"]))  
    # 그렇지 않으면 높이와 너비
```

:::info[output]
```
Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.

```

:::



```python copy
size
```

:::info[output]
```
(224, 224)
```

:::



```python copy
from torchvision import transforms
_transforms = transforms.Compose([
    transforms.RandomResizedCrop(size), 
    transforms.ToTensor(), 
    transforms.Normalize(
        mean=image_processor.image_mean, std=image_processor.image_std)])

def transform_function(examples):
    examples["pixel_values"] = [  # 이미지 변환
        _transforms(img.convert("RGB")) for img in examples["image"]]
    del examples["image"]
    return examples

dataset = dataset.with_transform(transform_function) # 전처리 함수 일괄 적용

```



```python copy
from transformers import AutoModelForImageClassification

model = AutoModelForImageClassification.from_pretrained(
    checkpoint,
    num_labels=len(labels),
    id2label=id2label,
    label2id=label2id,
)

```

:::info[output]
```
Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

```

:::



```python copy
import evaluate
import numpy as np

accuracy = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return accuracy.compute(predictions=predictions, references=labels)

```



```python copy
from transformers import DefaultDataCollator
from transformers import TrainingArguments, Trainer
data_collator = DefaultDataCollator()  # 데이터를 배치로 합침

training_args = TrainingArguments(
    output_dir="C:/my_model/", # 경로명에 한글이 들어가면 안됨
    remove_unused_columns=False, eval_strategy="epoch", save_strategy="epoch",
    learning_rate=5e-5, per_device_train_batch_size=16, gradient_accumulation_steps=4,
    per_device_eval_batch_size=16, num_train_epochs=1, warmup_ratio=0.1,
    logging_steps=10, load_best_model_at_end=True, metric_for_best_model="accuracy")

```



```python copy
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=dataset['train'],
    eval_dataset=dataset['test'],
    processing_class=image_processor,
    compute_metrics=compute_metrics,
)

```



```python copy
trainer.train()
```



```python copy
# 이미지 불러오기
from PIL import Image
image = Image.open("cats_and_dogs_filtered/test/cats/cat.2000.jpg") # data/ 삭제

# 이미지 분류
x = _transforms(image) # 변환
x = x.unsqueeze(0)  # 차원 추가
output = model(x)  # 모델에 입력
output.logits.argmax(dim=-1)  # 가장 확률이 높은 카테고리

```

:::info[output]
```
tensor([0])
```

:::


