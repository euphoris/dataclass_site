# 비전 트랜스포머

자 그 다음에 우리가 이제 어 오늘 cnn 이라는 구조를 알아봤는데요 가장 많이 사용되는 구조 이기는 합니다

근데 요즘에 더 많이 인기를 끌고 있는 구조 중에 비전 트랜스포머 라는 구조가 있는데 둘다 사실 인기가 있어요 어느 쪽이 더 좋다

이렇게 말할 수는 없고 뭐 둘다 인기가 있는데 약간 점점 비전 트랜스포머가 좀 치고 올라오는 그런 느낌이 일단 이제 cnn 의 한계를 보면은 cnn 의 기본적인 논리는 작은 부분을 처리를 해서 그걸 모아 가지고 좀 더 큰 부분을 처리하겠다

이런 얘기에요

그 문제가 뭐냐면 서로 멀리 떨어진 부분이 있을 때 얘네끼리의 관계를 잘 처리하지 못한다는 것 예를 들면은 어떤 눈의 모양을 처리하는 필터가 있으면 얘는 눈만 처리하지 코가 뭐 어떻게 돼 있는지는 신경쓰지 않습니다.

코를 처리하는 필터가 있으면 얘는 코만 처리하지 눈을 처리를 하지 않습니다

그러면 예를 들어서 요 얼굴과 요 얼굴이 있을 때 처리 결과가 다를 것이냐 하면은 거의 비슷할 수가 있어요

왜냐하면은 여기서 눈을 처리를 했죠

그 다음에 뭐 이런식으로 범위를 잡아서 맥스 풀링을 하면 눈은 사실 여기 있으나 여기 있으나 처리되는 결과 똑같습니다

그럼 뭐 대충 눈도 있고 코도 있고 있으니까 이것도 얼굴이고 이것도 얼굴이고 뭐 이런식으로 처리가 돼 버려서 어떤 그 멀리 떨어져 있는 거와의 관계를 잘 처리를 못하는 그런 문제가 있습니다

그러면 멀리 떨어진 피처들끼리의 관계를 고려하려면 어떻게 되냐

처음부터 필터를 큰거를 적용하면 되죠

이런걸 적용해서 요 눈을 처리할 때 옆에 있는 눈이나 코까지 같이 처리할 수 있게 하면 되는데 그러면은 애초에 CNN을 하는 의미가 별로 없단 말이에요

CNN이라는게 우리가 계산의 효율성을 높이기 위해서 조그마한 필터로 일단 피처를 추출하고 그걸 바탕으로 더 큰 피쳐 이렇게 가는 건데 처음부터 필터 displayed가 커버리면은 그냥 다층신경망을 하는거랑 똑같은 어떻게 되는거죠

컴퓨터 비전에서는 CNN 구조를 쓰고 처리에서는 RNN 이란 구조를 원래 많이 썼습니다

그래서 RNN 은 순환신경망이라는 구조인데 기본적으로 이제 자연어 치료 영역에 따라서는 사용해 문장이 한 줄로 쭉 이어진단 말이에요.

예를 들면 이 영화 재미없다

이런 문장이 있으면 이거를 쭉 처리를 해서 그래서 이 사람이 이 영화가 좋은지 싫은지 이런 거를 판명을 해야 되는데 우리가 그러면은 영화라는 단어를 처리할 때는 이 라는 단어의 처리 결과를 반영을 해야 되고 그 다음에 재미라는 단어를 처리할 때는 영화라는 단어의 처리 결과를 반영을 해야 되고 없다라는 단어를 처리할 때는 또 이 재미를 처리한 결과를 반영을 해야 되고 이런 식으로 해야 되는데 그러면은 이거를 어떻게 구조를 만들 수 있냐면 옆에 왼쪽처럼 만들어서 2라는 단어를 집어넣으면 뭔가 처리가 되겠죠.

그 다음에 그 처리 결과가 한번 순환이 돼서 영화라는 단어를 입력을 할 때 앞에 처리 결과가 되돌아오는 거예요.

그러면은 이거를 시간에 따라서 펼치면은 마치 오른쪽처럼 모양을 만든 거랑 똑같은 구조가 됩니다.

문장에서 단어를 하나씩 순서대로 집어넣으면 앞에 처리 결과가 메아리 쳐서 돌아오니까 결과적으로는 모든 단어를 반영한 어떤 결과를 얻을 수가 있는 거죠.

근데 이제 이렇게 해도 문제가 뭐냐면 약간 그 예능 프로 같은 거 보면 소리 안 들리는 귀마개 쓰고 앞사람이 어떤 동작으로 단어를 설명하면 그걸 다음 사람한테는 설명하고 다음 사람한테 설명하고 이런 식으로 설명을 하잖아요.

그러면 나중에 한 4사람 5사람 지나가면 영어 엉뚱한 소리가 돼가지고 되게 웃기고 이런 거 있잖아요.

어둠 고요 속의 외침인가?

뭐 이렇게 하는데 그거랑 똑같은 문제가 생깁니다.

이 영화 재미없다.

이렇게 있으면 이 앞에 있는 단어가 건너 건너 건너 영향을 미치니까 이쯤 가면은 굉장히 정보가 왜곡이 돼요.

그래서 여러 가지 어떤 문제가 있게 됩니다.

그래서 이제 이 문제를 해결하기 위해서 나온 게 주의 메커니즘이라고 합니다.

주의 메커니즘이 뭐냐면 우리가 자 저기 태극기를 보세요

하면 여러분이 태극기에 주의를 기울이잖아요.

그러면은 딱 이 말을 듣는 순간 주의를 기울이시면 저기 태극기가 있었어?

이런 분도 아마 계실 거예요.

그죠?

수업을 여기서 4일이나 했는데도 저기 태극기가 있다는 거를 지금 처음 아신 분도 있을 거거든요.

주의를 안 기울이면 우리는 모릅니다.

눈앞에 있었는데 분명히 내 눈에 한 번 내 막막에 저 태극기가 비쳤을 텐데도 주의를 기울이지 않으면 몰라요.

그래서 가끔 그럴 때 있거든요.

뭐 필요한 게 있다면 사야 되는데 아 그거 파는 가게 어디 있지?

하고 딱 보면 우리 집 앞에 있어요.

우리 집에 10년 동안 살았는데 내가 그 가게를 그 가게가 10년 동안 있었는데도 맨날 그 지나다니면서도 필요하다는 생각이 없으면 주의를 안 기울이니까 눈앞에 있어도 안 보이는 거예요.

주의의 효과가 그런 겁니다.

그래서 마찬가지로 우리가 이제 번역기를 만드는데 번역 같은 경우에 예를 들면 영어를 프랑스어로 번역을 한다.

그럼 보통 영어랑 프랑스어는 어순이 똑같기 때문에 여기가 나면 여기도 나아야 돼.

근데 문제는 얘가 이제 다리 다리 다리 건너서 오니까 얘를 번역을 시작하려고 보면 잠깐 주어가 뭐였더라?

이렇게 되는 거죠.

그래서 이 앞으로 주의를 돌립니다.

앞으로 가서 원래 여기가 뭐였지?

이렇게 한번 보는 거예요.

그런 거를 이제 구조를 만들어가지고 주의 메커니즘이라는 이름을 붙여 놓은 겁니다.

앞쪽으로 주의를 돌릴 수가 있어요.

맨 지금 직전에 있었던 말만 보는 게 아니고 앞으로 가서 아 여기 이 정보가 나한테 필요한 거였어.

가져와서 이제 번역을 하는 거죠.

그래서 이제 이게 프랑스어에서 나에 해당되는 단어입니다.

그래서 이제 이거를 주의 메커니즘이라고 하는데 이걸 만들어 놓고 나서 생각을 해보니까 밑에는 순환신경망 위에는 주의 메커니즘 이렇게 돼 있는데 한 2017년쯤에 구글에 있던 사람들 몇 명이서 그 생각을 합니다.

야 핵심은 요건데 밑에가 필요해?

이런 생각을 해요.

그래서 얘를 없애보자.

밑에 있던 순환신경망을 없애보자.

이런 생각을 합니다.

그래서 이제 2017년에 attention is all you need. 그래서 당신이 필요한 건 주의 뿐이라는 논문을 쓰는데 이게 어마어마한 충격을 주게 되고 실제로 돌려보니까 성능이 어마어마하게 잘 나온다.

순환신경망보다 성능이 어마어마하게 잘 나온다.

라는 것이 밝혀집니다.

그래서 거의 한 2018년 정도 되면은 아무도 순환신경망으로 그때부터 논문을 안 써요.

다들 회사들도 순환신경망으로 그때부터 논문을 안 써요.

순환신경망으로 구현했던 걸 다 걷어내고 다 트랜스포머로 전향을 합니다.

그래서 너도 트랜스포머 나도 트랜스포머 이렇게 전향을 하다가 오픈 AI라는 회사에서 이 트랜스포머가 주의 메커니즘을 쓰는 모델 이름인데 이 트랜스포머를 미리 트레이닝을 시켜요.

프리트레이닝을 시키는데 어떻게 프리트레이닝을 시키냐면 뭔가 문장을 생성할 수 있는 형태로 프리트레이닝을 시킵니다.

프리트레이닝을 시킵니다.

그래서 제너러티브 프리 트레인드 트랜스포머라는 모델을 발표하는데 이거의 첫 자가 GPT가 되는 거죠.

그래서 이 트랜스포머를 기반으로 GPT가 만들어지고 그 GPT를 상업화한 게 첫 GPT입니다.

여기서 이제 재밌는 점은 트랜스포머는 원래 구글이 만든 거라는 거죠.

그러니까 재주는 곰이 돕고 돈은 누가 먹는다고 트랜스포머를 막상 만든 거는 구글인데 지금 뭐 구글이 만드는 챗봇이 재미나이라고 있거든요.

여러분 아마 재미나이 이름도 못 들어보신 분들이 많으실 거예요.

그러니까 이게 참 웃기는 게 기술은 막상 구글이 만들었는데 그 기술로 대박을 친 거는 오픈 AI고 그다음에 MS고 구글은 자기네가 만든 기술을 이제 와서 따라잡겠다고 허덕허덕대고 있습니다.

그런 거 보면 참 기업의 세계는 재밌는 거 같아요.

그리고 트랜스포머를 만든 사람들은 지금 다 구글에서 나갔습니다.

다 자기 사업하겠다고 이제 너도 나도 회사를 차려서 나가고 있습니다.

그래서 약간 가끔 그런 경우 있잖아요.

보면은 제가 요즘에 삼성전자가 좀 사정이 어려우니까 아 이게 뭐 기술자가 사장을 안 하고 저기 재무쟁이가 사장을 하니까 회사가 저모여야지 사람들이 이런 얘기 하더라고요.

근데 제가 가끔 보면 꼭 기술자라고 해서 그렇게 선견지명이 있는 거 같지 않다.

약간 이런 생각이 드는데 보면은 이제 구글도 자기네가 만들어 놓고 다 남전일 시켜주는 그런 경우 되게 많거든요.

그래서 약간 이제 이건 좀 기술자들의 문제가니까 그래서 약간 이제 이건 좀 기술자들의 문제가니까 그래서 제품화를 잘 시켜야 되는데 제품화를 좀 잘 못하는 그런 게 이거죠.

어쨌든 얘기가 좀 샜는데 그래서 이 트랜스포머가 주의 메커니즘만을 이용한 자연화 처리 모형입니다.

그래서 얘는 앞에 무슨 단어가 있는지 이걸 보고 그걸 참고해서 다음 단어를 뭘 해야 되겠다.

무슨 말을 해야겠다.

이런 거를 계산을 하는 모형이죠.

그래서 디테일한 설명은 여기서 중요한 건 아니고 이 주의 메커니즘이라는 걸 쓴다.

그래서 앞에서 얘기했던 이 문제도 해결할 수 있는 거죠.

예를 들면 이 눈은 이제 주의 메커니즘이라는 걸 쓴다.

이 눈을 처리하는데 오른쪽 눈에 정보가 필요해요.

그럼 이쪽에다 주의를 기울이면 됩니다.

만약에 여기 왼쪽 눈을 처리하는데 오른쪽 눈에 오른쪽 눈 어디 갔지?

오른쪽 눈 여기 있네.

그러면 이렇게 멀리 떨어져 있어도 우리가 주의를 이리로 옮기면 되니까 이거가 멀리 떨어져 있던 가까이 있던 관련이 없는 거죠.

그래서 이제 트랜스포머가 자연화 처리에서 대박을 내니까 당연히 컴퓨터 비전 하는 사람들도 우리도 해볼까 라고 해서 비전 트랜스포머라고 해서 비전 트랜스포머는 이제 트랜스포머의 컴퓨터 비전판인데 줄여서 부를 때는 보통 vit라고 써요.

i를 보통 소문자로 씁니다.

똑같은 얘기예요.

그래서 이제 비전 트랜스포머 원래 이제 트랜스포머가 자연화 처리의 용도로 나온 거라서 트랜스포머는 기본적으로 단어를 입력을 받아야 되거든요.

이미지는 단어가 아니잖아요.

그래서 어떻게 할까 생각을 하다가 이미지를 대충 썹니다.

조각으로 썰어서 이 조각 하나하나를 일종의 단어처럼 취급해요.

그래서 이 조각들의 어떤 조각들로 이루어진 단어를 트랜스포머로 처리를 하는 거죠.

그러면은 이걸 가지고 여러 가지를 할 수 있고 그래서 이제 실제 이걸 처리를 할 때 이 조각 단위에서는 cnn 구조를 쓰기도 해요.

그래서 이 하단부에 이 부분은 cnn을 쓰고 이 윗단은 트랜스포머를 쓰고 이런 구조를 최근에 많이 시도를 하고 있습니다.

그래서 지금 chatgpt에 보면은 chatgpt 무료 계정에는 아마 안 될 텐데

어 무료 계정에도 되나요?

무료 계정에는 안 되는군요.

그 유료 계정에서는 이미지 같은 거를 올리면 사진 같은 거 올리고 이거 뭐야?

이렇게 물어본다든가 하면은 대답을 해 주거든요.

이거 어디서 찍은 사진이야

하면 대답을 해 주는데 그게 가능한 이유가 뭐냐면 트랜스포머 입장에서는 텍스트도 그냥 단어들이 쭉 이어진 거고 이미지도 일종의 이런 조각조각 하나를 일종의 단어로 취급해서 그냥 이런 어떤 글을 읽는 것처럼 그냥 읽습니다.

그러니까는 뭐 모델 입장에서는 이게 이미지든 텍스트든 별로 상관이 없는 거죠.

그래서 이런 식으로 처리를 한다.

그러면은 이제 트랜스포머가 왜 잘 되냐

이거에 대해서 이제 정확하게 아는 사람은 없지만 이게 좀 더 중요한 게 있는데 일단 이제 귀납현향이라는 개념이 있어요.

귀납현향은 뭐냐면 우리가 데이터에 대해서 모형을 만들 때 어떤 가정을 가지고 만들거든요.

이 데이터는 이러이러한 특성을 가지고 있을 거야.

그래서 예를 들면 이제 순환신경망 같은 경우는 텍스트라는 거는 이렇게 뭔가 순서대로 흘러갈 거야 라는 가정이 있고 그다음에 CNN 같은 경우는 가정이 부분이 모여서 뭔가 전체가 될 거야 라는 가정이 있습니다.

그래서 이런 걸 처리해서 모아서 전체를 만들면 될 거야.

이런 가정이 있는데 이런 가정이 안 맞을 수가 있는 거죠.

예를 들면 우리가 축구 시합 같은 걸 보면 공을 빵 찬 선수가 있고 그 공이 좌쪽으로 이렇게 날아갔으면 이미지상에서 공하고 선수는 멀리 떨어져 있거든요.

그렇지만은 우리가 그거를 처리할 때는 걔네를 묶어서 처리를 해야 됩니다.

거리적으로 떨어져 있어도 부분 부분을 단순하게 모으면 전체가 되는 게 아니에요.

전체적인 그림을 봐야 그 부분이 더 잘 이해되는 그런 경우도 있습니다.

그래서 이런 가정은 안 맞을 수가 있는 거죠.

그렇기 때문에 우리가 이렇게 정보를 만들면 될 수 있는 거죠.

그다음에 이제 잔여 처리 같은 경우도 우리가 그 반전 영화 이런 거가 왜 재밌습니까?

영화를 처음부터 순서대로 쭉 보다가 마지막에 반전이 탁 하면 아 앞에서 그 장면 그 장면이 그런 뜻이었구나 하고 이해가 되는 게 좋은 반전 영화죠.

그래서 뭐 식스센스 이런 영화가 보신 분들 아시겠지만 식스센스가 이제 반전 영화의 대표작인데 안 보신 분들을 위해서 20년 전 영화인데 스포일러를 해야 되나

말아야 되나 어쨌든 영화를 끝까지 볼 수 있는 것 같아요.

영화를 끝까지 보고 나면 갑자기 앞에 있는 모든 장면에 대한 우리가 이해가 이제 원래 주인공이 아내하고 사이가 나쁜 걸로 이제 영화 내내 그렇게 보이는데 영화를 끝까지 보고 나면은 사이가 나빴던 게 아니에요.

완전 우리가 이제 관객이 오해를 하고 있었던 거죠.

장면 장면의 의미가 전혀 다르게 이해가 됩니다.

두 번째 보면은 안 보이는 장면들이 막 보이거든요.

그러니까 뭡니까?

이게 우리가 어떤 이해라는 게 무조건 요방형으로만 되는 게 아니라 반대로도 이루어진다는 거예요.

그래서 이런 우리가 이제 귀나 편향 이걸 이제 귀나 편향이라고 하는데 우리의 가정이 이제 안 맞는 그런 경우가 생기면 그만큼이 이제 성능에 저하로 오는 거죠.

그래서 머신러닝에서 이제 편향 분산 교환이라는 말을 쓰는데 이거는 뭐냐면 편향이 커지면 분산이 작아지고 편향이 작아지면 분산이 커진다

이런 얘기입니다.

서로 반대로 움직인다

이런 얘기입니다.

우리가 어떤 가정을 강하게 하면은 예를 들어서 여러분이 세상은 못 믿을 사람 천지야 누구도 믿어서는 안 돼 라고 생각하시면은요.

여러분이 굉장히 강한 그런 생각을 가지고 있습니다.

그럼 그게 여러분의 편향인 거죠.

그러면은 여러분은 누구를 만나도 결국 아 역시 저 자식도 믿을 수 없는 놈이야.

검은 머리 짐승은 거두는 것이 아니랬거든.

항상 남을 못 믿으니까 남들이 다 자기들 나쁘게 대하겠죠.

그러니까 여러분의 결론은 항상 똑같습니다.

그러니까 편향이 강하면 분산이 작아져요.

결론이 항상 똑같습니다.

어떤 데이터를 가져다 줘도 결론이 똑같아요.

가끔 그 이제 우리가 왜?

보통 정치 얘기 잘 안 하는 그런 사람들끼리 이제 정책이 잘 아닌 거 뭡니까?

자기 지지하는 정당에 따라서 그 믿음이 되게 강한 분들은 결론이 항상 똑같아요.

경제가 좋아도 아 우리 당이 잘한 거야.

경제가 나빠도 우리 당이 잘한 거야.

항상 결론이 똑같거든요.

그러니까 얘기를 해봤자 싸움 밖에 안 나는 거야.

그래서 우리가 정책 같은 거 잘 안 하죠.

그러니까 이제 그것도 뭐냐면 이제 정치 같은 거는 대부분 자기 어떤 편향이 있단 말이에요.

좌편향이든 우편향이든 자기 편향이 있기 때문에 결론이 대체로 정해져 있어요.

얘기해봤자 소용이 없는 거죠.

그래서 이제 분산이 작아진다는 얘기는 뭐냐면 분산이라는 거는 이랬다.

저랬다 하는 건데 항상 편향이 강하면 이랬다

저랬다 하는 게 없는 거죠.

그래서 이게 장점일 수도 있고 단점일 수도 있는데 어떨 때 장점이냐면 데이터가 작을 때 장점이 있습니다.

왜냐하면 우리가 만약에 편향을 좀 좋은 편향을 가지고 있다.

그러니까 편향인 건 맞는데 그게 실제랑 일치하는 어떤 편향이다.

예를 들면 텍스트가 정말로 순서대로 처리가 된다라고 한다면 그 데이터가 작아도 성능이 잘 나옵니다.

근데 데이터에 따라서 결과가 그렇게 달라지지 않기 때문에 애초에 자기 믿음이 그런 거잖아요.

데이터에 영향을 안 받기 때문에 편향이 강할 때는 문제가 뭐냐면 데이터를 늘려봤자 성능이 안 올라가요.

그래서 편향이 강한 경우는 데이터가 작을 때는 좋은데 데이터가 커지면 좀 이득이 없습니다.

성능을 보면 데이터 크기가 있으면 성능이 이렇게 원만하게 올라갑니다.

왜냐면 애초에 편향되어 있기 때문에 편향이 약하면 그러니까 아무 선입견이 없어요.

그 저기 뭐야

아무 선입견이 없어가지고 남자랑 남자가 같이 있으면 어 니네 사귀냐

이러고 여자랑 여자가 있어도 사귀냐

이러고 남자랑 여자가 있어도 사귀냐

이러고 남자가 뭐 나무랑 있어도 나무랑 사귀냐

이러고 아무런 편견이 없어요.

그냥 보이는 대로 믿어요.

그런 사람이 있어요.

그러면 이 사람은 누구랑 누구를 보여주냐에 따라서 결론이 달라지겠죠.

어떤 사람이 컴퓨터 가지고 있어요.

너 컴퓨터랑 사귀니?

뭐 이러면서 아무 편견이 없어요.

그러니까 이 사람은 그냥 본대로 믿으니까 결론이 항상 뭐 이랬다 저랬다 하고 뭘 봤냐에 따라서 오늘 뭘 봤냐에 따라서 결론이 달라지겠죠.

팔랑귀니까.

그래서 이제 이런 경우는 데이터가 적을 때는 그 성능이 굉장히 떨어집니다.

왜냐하면은 적은 데이터면 어떤 그 노이즈가 하나만 딱 들어가도 얘가 결론이 막 이상한 결론이 나고 이렇기 때문에 성능이 안 나오는데 데이터가 많아지면은 그 많은 데이터에 있는 어떤 패턴들을 쏙쏙 흡수를 할 수 있기 때문에 성능이 확 올라갑니다.

그래서 어떤 일정 데이터 이상에서는 편향이 약한 게 더 성능이 좋을 수가 있어요.

그러니까 아무런 경우는 그런 게 아니거든요.

그래서 그 부분은 그냥 가정을 안 하는 게.

그래서 cna는 부분이 모여서 전체가 된다는 가정이 있고 순환신경망은 순서대로 처리된다는 가정이 있는데 트랜스포머는 뭐 그런 가정이 별로 없어요.

그냥 이거랑 이거랑 관련 있으면 뭐 같이 처리하면 된다.

이 정도의 가정만 있을 뿐이고 얘네가 가까워야 되는지 멀어도 되는지 뭐 이런 순서대로 해야 되는지 앞에서 뒤로 가야 되는지 뒤에서 앞으로 가야 되는지 아무런 그런 가정이 없습니다.

그래서 어 순서라든가 위치라든가 이런 데에 대한 가정이 없고 그래서 편향이 적다.

편향이 적으니까 데이터만 충분히 많다면 성능을 잘 낼 수 있다.

반대로 말하면 데이터가 적을 때는 좀 성능이 안 나오겠죠.

그래서 그런 장점이 있고 그것 때문에 잘 작동한다.

이렇게 보고 있고요.

그럼 이제 데이터가 적은 분야에서는 트랜스포머를 좀 활용하기가 어렵겠죠.

근데 요것도 우리가 이제 내일 알아보겠지만 요걸 이제 극복하는 방법이 있는데.

GPT의 그 PE가 얘기하듯이 프리트레이닝을 많이 시켜놓으면 된다.

다양한 질병을 학습을 시켜놓으면 특이한 희귀 질환이라고 하더라도 뭐 어차피 희귀 질환이라도 질환은 질환이니까 이제 잘 처리할 수 있게 되는 거죠.

예를 들면 뭐 우리가 뭐 질환으로 치면 희귀 질환을 진단하는 의사라도 어쨌든 의대 나오고 의학을 배우니까 희귀 질환도 다른 병이랑 좀 다르긴 하지만 기본적으로 원리가 똑같은 부분이 있으니까 금방 배울 수 있는 거잖아요.

사람도 뭐 그 이제 자기가 전공 분야가 그래서 우리가 대학교 때는 여러 가지를 배우잖아요.

이것도 배우고 저것도 배우고.

왜냐하면 다양하게 배워놓으면 나중에 회사 가서 사실 써먹는 건 별로 없는데 그중에 뭘 무슨 업무를 맡든지 간에 대충 내가 뭐 학교에서 이것도 배우고 저것도 배워가지고 그래도 대충 감은 있으니까 그때 가서 좀 쉽게 배울 수 있단 말이에요.

약간 이런 식으로 이제 프리트레이닝을 하는 게 요즘에 좋은 효과를 보이고 있고요.

그다음에 이제 계산 효율이 낮은 문제가 있는데.

트랜스포머는 항상.

예를 들어서 이렇게 3X3가 있다.

그럼 얘를 처리해야 되는데 얘랑 뭐를 묶어야 되는지에 대한 어떤 전제가 없으니까 얘가 있으면 얘랑 얘랑 얘랑 얘랑 얘랑 얘랑 얘랑 얘랑 얘랑 얘랑 얘랑 얘랑.

이걸 다 짝지어서 나랑 관련 있는 애가 누구냐

이걸 계산을 다 합니다.

그래서 데이터가 N개의 어떤 구성요소로 돼 있으면 이 계산을 각각을 다 서로 서로 해봐야 되니까 N 곱하기 N번 해가지고 N제곱번 계산을 해야 되거든요.

그럼 문제가 뭐냐면 이 구성요소가 100개다.

그럼 계산을 만 번을 해야 되는 거죠.

구성요소가 1000개면 계산을 100만 번을 해야 됩니다.

계산을 조금만 늘어나면 계산이 엄청나게 늘어나요.

그래서 이거를 보실 수 있는 게 여러분들이 ChatGPT 써보시면은 대화가 좀 길어지면은 앞에 했었던 얘기를 기억을 못하거든요.

그게 기억을 못하는 게 아니라 계산량이 너무 많아지니까 대화가 길어지면 대화 앞부분을 그냥 계산해서 잘라버립니다.

계산을 안 해요.

그래서 최근 대화 몇 마디만 가지고 항상 계산을 합니다.

그래서 ChatGPT 가지고.

대화가 길어지면 갑자기 경고문이 뜨면서 대화가 너무 길면 답변을 잘 못할 수 있습니다.

새 대화를 여세요.

이런 얘기를 하는데 그런 이유 때문에 그렇습니다.

그래서 계산율 낮은 문제는 최근에 굉장히 연구가 진행이 되는 그런 부분이고.

그래서 ChatGPT는 그런 게 반영이 안 돼 있는데 구글의 재미나이 같은 경우는 쉽게 말하면 100만 단어 수준까지 계산을 할 수 있게 우리가 만들었다.

이렇게 얘기를 하고 있어요.

100만 단어면 거의 책 한 권.

이 정도 분량이거든요.

그러니까 여러분들이 어떤 책을 그냥 던져놓고 이 책에서 이런 이런 거 어떻게 되는 거야

하고 물어보면 대답을 해 주겠죠.

그래서 만약에 기술이 더 발전해서 한 권이 아니라 책을 10권, 100권을 넣어도 다 계산할 수 있게 된다.

효율적으로 계산할 수 있게 된다.

그러면 사실 공부를 할 필요가 없는 거죠.

그냥 책을 다 집어넣고 내가 궁금하면 그때 물어보면 그때그때 바로 그 책에서 답변을 해 줄 수 있겠죠.