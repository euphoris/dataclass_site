# 다층 신경망

## 선형 분리 linearly separable
*   단층 신경망은 데이터 공간을 하나의 직선(또는 평면)으로 나누는 것
*   데이터가 복잡하면 하나 이상의 선으로 나눌 필요

## 선형 vs. 비선형

## XOR 문제
*   수학에서 A OR B는 A와 B 둘 다 참인 경우에도 참
*   일상적인 의미에서 "또는"은 둘 중에 하나만 참인 경우에 사용
*   XOR(Exclusive OR): A 또는 B 둘 중에 하나만 참인 경우에 참
*   선형 분리할 수 없음 → 다층 신경망으로 해결

## 다층신경망 Multi-Layer Perceptron
*   여러 개의 층으로 이뤄진 신경망
*   선형 분리 불가능한 비선형 데이터를 다룰 수 있음
*   입력층 Input layer
*   은닉층 Hidden layers
    *   신경망의 학습 과정에서 과제 수행에 필요한 특징을 학습
    *   입력층에 가까운 은닉층은 단순한 특징을, 출력층에 가까운 은닉층은 복잡한 특징을 학습
    *   은닉층의 형태에는 제약이 없음
    *   은닉층의 적절한 수와 크기는 문제에 따라 달라짐
    *   다양한 시도를 통해 가장 성능이 좋은 것으로 결정
*   출력층 Output layer

## 다층신경망 multi-layer perceptron
```python
class MLPClassifier(pl.LightningModule):
    def __init__(self):
        super().__init__()
        self.linear1 = nn.Linear(28*28, 16) # 16개의 뉴런을 가진 은닉층
        self.linear2 = nn.Linear(16, 1) # 한 개의 출력 뉴런
        self.accuracy = BinaryAccuracy()
 
    def forward(self, x):
        x = x.view(-1, 784) # Flatten 레이어 없이, 텐서 모양을 (N, 28, 28) → (N, 784)
        x = F.relu(self.linear1(x)) # ReLU 활성화 함수
        x = F.sigmoid(self.linear2(x))
        return x

    def training_step(self, batch, batch_idx): # 훈련의 한 단계
        images, labels = batch
        outputs = self(images).squeeze() # 이미지를 모형에 입력하여 출력을 얻음
        loss = F.binary_cross_entropy(outputs, labels.float()) # 손실 계산
        self.log('loss', loss) # 손실 기록
        accuracy = self.accuracy(outputs, labels) # 정확도 계산
        self.log('accuracy', accuracy) # 정확도 기록
        return loss

    def test_step(self, batch, batch_idx): # 테스트도 훈련과 동일하게
        return self.training_step(batch, batch_idx)

    def configure_optimizers(self): # 알고리즘 설정
        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)
        return optimizer
```

## 미분의 연쇄 규칙 chain rule
*   f와 g가 미분가능한 함수라고 할 때, y = f(u)이고 u = g(x)이면, 다음이 성립:
    dy/dx = (dy/du) * (du/dx)
*   다층신경망의 구성 (x: 입력, L: 손실)
*   손실 L의 i번째 레이어의 파라미터 θi에 대한 미분:
    *   이후 레이어(K-1)의 미분을 이전 레이어(K-2)의 미분에 재사용할 수 있음

## 역전파 Backpropagation
*   다층신경망에서 예측을 할 때는 forward pass로 계산
    input → hidden(1) → hidden(2) → … → hidden(K-1) → hidden(K)→ output
*   파라미터를 업데이트할 때는 역순인 backward pass로 계산
    hidden(1) ← hidden(2) ← … ← hidden(K-1) ← hidden(K)
*   이를 "오차의 역전파"라 함

## 사라지는 경사 vanishing gradient
*   미분의 연쇄 규칙에 따르면 경사는 곱셈 형태
*   경사가 0~1 사이인 레이어가 많으면, 입력층과 가까운 초반 레이어는 손실의 경사가 0에 가까워짐
*   신경망 초반 레이어에서 변화가 손실에 영향을 주지 못함
*   초반 레이어로 오차 역전파가 X → 초반 레이어의 파라미터 업데이트 X
*   레이어가 늘어나도 학습이 되지 않으므로, 딥러닝의 의미 상실


## 퀴즈

<iframe src="https://tally.so/embed/n0Y4lQ?alignLeft=1&hideTitle=1&transparentBackground=1&dynamicHeight=1" loading="lazy" width="100%" height="1125" frameborder="0" marginheight="0" marginwidth="0" title="[CV] 다층 신경망"></iframe>