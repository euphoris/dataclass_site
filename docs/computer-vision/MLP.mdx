# 다층 신경망

## 선형 분리 linearly separable
*   단층 신경망은 데이터 공간을 하나의 직선(또는 평면)으로 나누는 것
*   데이터가 복잡하면 하나 이상의 선으로 나눌 필요

## 선형 vs. 비선형

## XOR 문제
*   수학에서 A OR B는 A와 B 둘 다 참인 경우에도 참
*   일상적인 의미에서 "또는"은 둘 중에 하나만 참인 경우에 사용
*   XOR(Exclusive OR): A 또는 B 둘 중에 하나만 참인 경우에 참
*   선형 분리할 수 없음 → 다층 신경망으로 해결

## 다층신경망 Multi-Layer Perceptron
*   여러 개의 층으로 이뤄진 신경망
*   선형 분리 불가능한 비선형 데이터를 다룰 수 있음
*   입력층 Input layer
*   은닉층 Hidden layers
    *   신경망의 학습 과정에서 과제 수행에 필요한 특징을 학습
    *   입력층에 가까운 은닉층은 단순한 특징을, 출력층에 가까운 은닉층은 복잡한 특징을 학습
    *   은닉층의 형태에는 제약이 없음
    *   은닉층의 적절한 수와 크기는 문제에 따라 달라짐
    *   다양한 시도를 통해 가장 성능이 좋은 것으로 결정
*   출력층 Output layer

## 다층신경망 multi-layer perceptron
```python
model = keras.models.Sequential(
    [
        keras.layers.Rescaling(1/255),
        keras.layers.Flatten(),
        keras.layers.Dense(16, activation='relu'),
        keras.layers.Dense(1, activation='sigmoid')
    ]
)
```

## 미분의 연쇄 규칙 chain rule
*   f와 g가 미분가능한 함수라고 할 때, y = f(u)이고 u = g(x)이면, 다음이 성립:
    dy/dx = (dy/du) * (du/dx)
*   다층신경망의 구성 (x: 입력, L: 손실)
*   손실 L의 i번째 레이어의 파라미터 θi에 대한 미분:
    *   이후 레이어(K-1)의 미분을 이전 레이어(K-2)의 미분에 재사용할 수 있음

## 역전파 Backpropagation
*   다층신경망에서 예측을 할 때는 forward pass로 계산
    input → hidden(1) → hidden(2) → … → hidden(K-1) → hidden(K)→ output
*   파라미터를 업데이트할 때는 역순인 backward pass로 계산
    hidden(1) ← hidden(2) ← … ← hidden(K-1) ← hidden(K)
*   이를 "오차의 역전파"라 함

## 사라지는 경사 vanishing gradient
*   미분의 연쇄 규칙에 따르면 경사는 곱셈 형태
*   경사가 0~1 사이인 레이어가 많으면, 입력층과 가까운 초반 레이어는 손실의 경사가 0에 가까워짐
*   신경망 초반 레이어에서 변화가 손실에 영향을 주지 못함
*   초반 레이어로 오차 역전파가 X → 초반 레이어의 파라미터 업데이트 X
*   레이어가 늘어나도 학습이 되지 않으므로, 딥러닝의 의미 상실


## 퀴즈


import { QuizComponent } from "@/components/QuizComponent";

<QuizComponent quizId="MLP" quizItems={
    [
    {
        "item_type": "radio",
        "question": "이항 분류에서 단층신경망은 기하학적으로 무엇을 수행하는 것과 같습니까?",
        "options": [
            "데이터 공간을 하나의 직선 또는 평면으로 나누는 것",
            "데이터 공간을 여러 개의 직선 또는 평면으로 나누는 것",
            "데이터 공간을 하나의 곡선 또는 곡면으로 나누는 것",
            "데이터 공간을 여러 개의 곡선 또는 곡면으로 나누는 것"
        ],
        "hint": "단층 신경망은 선형(linear) 경계면밖에 만들 수 없습니다.",
        "solution": "데이터 공간을 하나의 직선 또는 평면으로 나누는 것"
    },
    {
        "item_type": "radio",
        "question": "다층 신경망에서 은닉층에 대한 설명으로 올바르지 않은 것은?",
        "options": [
            "데이터를 직접 입력 받거나 출력하지 않는다",
            "과제 수행에 필요한 특징을 학습한다",
            "은닉층을 이루는 뉴런의 개수나, 은닉층의 수는 자유롭게 정할 수 있다",
            "크면 클 수록 성능이 높아진다"
        ],
        "hint": "은닉층의 크기가 무조건 크다고 좋은 것은 아닙니다. 과대적합의 원인이 될 수 있습니다.",
        "solution": "크면 클 수록 성능이 높아진다"
    },
    {
        "item_type": "radio",
        "question": "다층신경망을 학습시키는 알고리즘을 무엇이라고 합니까?",
        "options": [
            "역전이 알고리즘",
            "역전파 알고리즘",
            "학습 알고리즘",
            "사라지는 경사 알고리즘"
        ],
        "hint": "출력층의 오차를 입력층 방향으로 거꾸로 전파시키며 가중치를 업데이트합니다.",
        "solution": "역전파 알고리즘"
    },
    {
        "item_type": "radio",
        "question": "사라지는 경사는 어떤 문제입니까?",
        "options": [
            "신경망 초반 레이어의 경사가 0에 가까워지는 문제",
            "신경망 중간 레이어의 경사가 0에 가까워지는 문제",
            "신경망 후반 레이어의 경사가 0에 가까워지는 문제"
        ],
        "hint": "역전파 과정에서 경사(gradient)가 뒤로 전달될수록 점점 작아져서 앞쪽 레이어의 학습이 잘 이루어지지 않는 문제입니다.",
        "solution": "신경망 초반 레이어의 경사가 0에 가까워지는 문제"
    }
]
} />


## Q&A
<iframe src="https://tally.so/embed/wbOOKg?alignLeft=1&hideTitle=1&transparentBackground=1&dynamicHeight=0" loading="lazy" width="100%" height="274" frameborder="0" marginheight="0" marginwidth="0" title="Q&A"></iframe>