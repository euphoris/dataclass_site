# 합성곱 신경망

## 다층 신경망의 문제점
*   이미지는 2차원이지만 다층 신경망의 입력은 1차원
*   이미지를 2차원 → 1차원으로 flatten하여 입력
*   2차원 이미지를 1차원으로 flatten하면서 공간적 정보가 사라짐
*   Dense 레이어는 입력의 크기 × 출력의 크기만큼 파라미터가 필요
*   이미지의 크기가 커지면 파라미터가 폭증

## 합성곱 신경망 Convolutional Neural Network
*   합성곱 신경망은 입력 이미지를 작은 단위로 나누어 각각을 처리
    *   이 작은 단위를 필터(filter) 또는 커널(kernel)이라고 부름
    *   필터는 이미지의 특징을 추출하는 역할
*   컴퓨터 비전에서 가장 널리 사용되어 온 방식
    *   최근에는 비전 트랜스포머라는 모델도 널리 사용
*   작은 크기의 필터를 반복 적용하여 부분적인 패턴을 인식
*   필터의 값은 데이터로부터 학습

## 합성곱 레이어
*   Receptive Field: 이미지에서 필터가 적용되는 영역
*   Feature Map: 합성곱 레이어의 출력 결과
    *   이미지에서 발견된 특징을 사상(map)한 것이므로 feature map이라고 함

## 부분에서 전체로
*   합성곱 신경망은 여러 가지 필터들을 이용해 이미지의 한 작은 부분에서 특징들을 그리고 여러 개의 필터를 사용하여 입력 이미지에서 다양한 특징을 추출
*   사람의 얼굴을 인식한다면, 처음에는 수직선, 수평선, 대각선 등의 특징을 추출
*   이렇게 추출한 특징들에 다시 필터를 적용하면 좀 더 큰 특징을 추출할 수 있음
*   선들이 모여 이루는 눈, 코, 입 같은 부분들이 됨
*   여기에 다시 필터를 또 적용하면 부분이 모여서 이루는 얼굴 전체를 추출할 수 있음

## 합성곱 적용의 효과
*   커널 또는 필터의 수만큼 채널이 바뀌는 효과가 있음
*   피처 맵의 크기는 감소
*   다양한 필터를 적용하여 하나의 이미지로부터 여러 가지 특징들을 추출

## CNN의 기본 구조
*   특징 추출 부분과 분류 부분으로 구성
*   합성곱 신경망의 전반부에는 부분적 특징들을 추출하기 위해 합성곱 레이어를 사용
*   합성곱 신경망의 후반부에는 추출된 특징을 바탕으로 최종 예측을 하기 위해 Dense 레이어를 사용
*   두 부분 사이에 Flatten

## CNN 코드 예시
```python
class CNN(pl.LightningModule):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3) # 크기가 2만큼 줄어듦: 28 → 26
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3) # 크기가 2만큼 줄어듦: 26 → 24
        self.fc = nn.Linear(64*24*24, 10)
        self.accuracy = MulticlassAccuracy(10)

    def forward(self, x):
        # 입력에는 Flatten이 없음
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x)) 
        x = x.view(-1, 64*24*24) # 여기서 Flatten
        x = self.fc(x) # 분류
        return x
```

## 패딩 padding
*   이미지의 테두리에 빈 픽셀을 추가
*   제로(zero) 패딩:
    *   패딩 없음
    *   입력보다 출력 이미지가 작음
*   하프(half) 패딩 / 세임(same) 패딩:
    *   필터 크기의 절반만큼 패딩
    *   입력과 출력의 이미지가 같음
*   풀(full) 패딩
    *   필터 크기 - 1만큼 패딩
    *   모든 픽셀에 필터의 모든 부분이 적용됨
    *   출력이 입력보다 큼

## 패딩 적용 코드 예시
```python
def __init__(self):
    super().__init__()
    self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1) # 세임 패딩: 28 → 28
    self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1) # 세임 패딩: 28 → 28
    self.fc = nn.Linear(64*28*28, 10)
    self.accuracy = MulticlassAccuracy(10) 

def forward(self, x):
    x = F.relu(self.conv1(x))
    x = F.relu(self.conv2(x))
    x = x.view(-1, 64*28*28) # Flatten
    x = self.fc(x)
    return x
```

## 풀링 pooling
*   서브샘플링(subsampling) 또는 다운샘플링(downsampling)이라고도 함
*   이미지 또는 특징 맵의 크기를 축소
*   이미지의 세부적인 특징이 전체 이미지를 분류하는데 크게 중요하지 않은 경우가 많으므로 성능에는 영향이 적음
*   적용할 합성곱 레이어의 수가 줄어듦 → 학습할 파라미터가 줄어듦
*   최대 풀링(Max Pooling):
    *   이미지에서 작은 부분(예: 2x2)들에서 최댓값(가장 강한 특징)을 추출
    *   CNN에 주로 사용
*   평균 풀링(Average Pooling):
    *   이미지에서 평균값을 추출

## 풀링 적용 코드 예시
```python
def __init__(self):
    super().__init__()
    self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
    self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
    self.fc = nn.Linear(64*7*7, 10)
    self.accuracy = MulticlassAccuracy(10)

def forward(self, x):
    x = F.relu(self.conv1(x))
    x = F.max_pool2d(x, 2) # 28 → 14
    x = F.relu(self.conv2(x))
    x = F.max_pool2d(x, 2) # 14 → 7
    x = x.view(-1, 64*7*7)
    x = self.fc(x) # 분류
    return x
```

## 스트라이드 stride
*   필터를 적용하는 간격
*   풀링을 사용하는 대신 필터를 일정 간격으로 띄워서 사용하기도 함

## 스트라이드 적용 코드 예시
```python
class CNN(pl.LightningModule):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1, stride=2)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1, stride=2)
        self.fc = nn.Linear(64*7*7, 10)
        self.accuracy = MulticlassAccuracy(10)

    def forward(self, x):
        x = F.relu(self.conv1(x)) # 28 → 14
        x = F.relu(self.conv2(x)) # 14 → 7
        x = x.view(-1, 64*7*7)
        x = self.fc(x)
        return x
```

## CNN의 다양한 구조
*   LeNet-5
*   AlexNet
*   VGGNet
*   GoogLeNet
*   ResNet

## LeNet-5

## AlexNet
*   2012년 ImageNet Large Scale Visual Recognition Challenge (ILSVRC)에서 1위를 하여 딥러닝 유행을 선도
*   특징:
    *   ReLU
    *   Dropout
    *   Batch Normalization
    *   Weight regularization

## VGGNet
*   합성곱 레이어는 모두 3x3 크기
*   풀링 레이어는 모두 2x2 크기
*   3x3 합성곱 2개는 5x5 합성곱와 receptive field 크기가 같으나 파라미터의 수가 더 적고, 1개의 레이어 대신 2개의 레이어를 사용하므로 비선형성을 추가할 수 있음

## GoogLeNet
*   Inception 모듈이라는 구조를 포함
*   합성곱 레이어를 직렬이 아닌 병렬로 연결

## 1x1 Conv. Layer
*   피처맵의 크기는 같음
*   채널의 수를 줄이는 효과
*   1x1 합성곱 레이어를 통해 채널의 크기를 줄여 계산량을 감소시킴
*   인셉션 모듈의 다른 경로에도 1x1 합성곱을 추가하여 출력 채널을 줄임

## ResNet Residual Network
*   skip connection: 레이어를 건너 뛰는 경로를 만듦
    *   최소한 이전 레이어에서 학습된 만큼의 성능은 보장
*   사라지는 경사 문제를 완화하고, 깊은 네트워크에서도 안정적인 학습이 가능
*   신경망의 깊이를 늘려도 학습이 잘 되므로 성능이 향상


## 퀴즈

<iframe src="https://tally.so/embed/mOZvPM?alignLeft=1&hideTitle=1&transparentBackground=1&dynamicHeight=1" loading="lazy" width="100%" height="909" frameborder="0" marginheight="0" marginwidth="0" title="[CV] 합성곱 신경망"></iframe>
{/* 

# 합성곱 신경망

자 그러면은 다음으로 넘어가서 그 다음에 정칙화라는 게 있는데요 요거는 건너뛰겠습니다

그냥 먼지만 간단히 얘기 드리자면 모델을 우리가 이제 크게 만들면 크게 만들수록 성능이 좋아지는데 그에 비례해서 약간 노이즈 같은 거에도 되게 민감해지거든요

데이터가 모든 데이터에 노이즈가 있어서 너무 모델이 노이즈에 민감해도 성능이 떨어져요

예측 성능이 노이즈라는 거는 말 그대로 그냥 우연히 튀는 건데 좀 튀는 데이터 하면 무시를 해야 되는데 너무 학습 능력이 좋아지다 보니까 노이즈까지 같이 학습을 해버리는 거죠

그래서 정칙화는 영어로 Regularization이라고 하는데 Regular라는 게 어떤 규칙적인 이런 거잖아요

Regularize 규칙적으로 만든다

이런 얘기입니다

너무 불규칙하게 튀는 거를 좀 무시하게 하는 여러 가지 기법들이 있습니다

네 근데 이거는 우리가 이제 설명하지 않고 그냥 넘어갈 겁니다

이런 것도 있다

이런 거는 이제 좀 디테일한 부분이기 때문에 좀 넘어가고요 297쪽으로 넘어가서 합성곱 신경망으로 넘어가겠습니다

여기 이제 합성곱이라고 한 거는 우리가 앞에서 한번 배웠었는데 컨볼루션이라는 거를 우리말로 합성곱이라고 해요

컨볼루션은 이제 기억나실지 모르겠지만 우리가 뭐 흐리게 한다든가 아니면은 날카롭게 하거나 아니면은 그 우리 어제 퀴즈에서 나왔는데 어떤 경계선 같은 걸 뽑을 때 어떻게 하냐면 11100-1-1-1 이렇게 해가지고 그러면은 걸어주면은 왼쪽이 밝고 오른쪽이 어두워야 어떤 이 차이가 나서 이렇게 수직으로 생겨있는 경계선을 탐지를 할 수 있죠

이런 거를 하는 게 다 이제 합성곱이라는 계산인데 이 합성곱에 기반한 신경망 구조를 합성곱 신경망 보통 줄여서 CNN이라고 합니다

그럼 이거는 이제 왜 쓰느냐

우리가 이제 지금까지 했던 다층신경망 구조는 기본적으로 이미지라는 게 2차원인데 항상 이거를 1차원으로 플래튼을 해가지고 어떤 벡터로 만든 다음에 처리를 해요

이렇게 하면은 어떤 공간적인 정보가 없어지는 거죠

왜냐면 이미지라는 게 분명히 가로 세로가 있어서 요 점 위에는 이게 있고 왼쪽에는 이게 있고 오른쪽에 있고 아래는 이게 있고 이런 구조가 있는데 이거를 플래튼 해버리면 그냥 이렇게 돼 있는 숫자의 나열이 되는 거죠

뭐가 위에 있고 뭐가 아래 있고 이런 거는 그냥 없이 그냥 한 줄로 쭉 펼쳐진 구조가 됩니다

그래서 이미지에서는 이 공간적인 구조가 중요한데 그거를 싹 무시해버리니까 일단 그게 안 맞다 라는 거고 또 하나는 이 벡터 점마다 뭔가 어떤 값을 곱해 주거든요

우리 어제 시각화 해보셨으면 기억나시겠지만 점 하나하나마다 어떤 가중치를 주는데 그러면 문제가 뭐냐면 이미지의 크기가 커지면 커질수록 그 계산해 줘야 되는 가중치도 같이 늘어납니다

거기다가 카테고리가 늘어나면 또 카테고리가 늘어나는 만큼 또 가중치의 개수가 늘어납니다

그래서 단층신경망일 경우에 우리가 784차원에다가 거기다가 10개의 카테고리로 분류해야 된다 그러면은 사실은 여기다 더기를 해줘야 돼서 총 카테고리가 7850개가 필요해요

근데 보통 이런 파라미터의 개수에 대비해서 데이터가 못해도 100배는 필요하거든요

그러니까 이게 성능이 충분히 나오려면 78만 5천 개의 데이터가 있어야 돼요

100개는 좀 심했나?

78,500개인데 우리 데이터는 60,000개밖에 안 된단 말이에요

파라미터가 너무 크면은 그 파라미터들을 충분히 튜닝할 만큼의 데이터를 확보하는 게 굉장히 문제가 되고 확보했다 치더라도 그걸 또 계산을 해야 되니까 계산이 오래 걸린다는 문제가 있습니다

그래서 파라미터가 너무 많이 늘어나는 거는 별로 우리가 바람직하지 않아요 성능이 늘어나는 건 좋은데 너무 쓸데없이 많이 늘어난단 말이에요

그래서 이제 합성곱신경망은 우리가 앞에서 배웠던 것과 비슷한 것들인데 어떤 합성곱 구조를 이용을 하는데 합성곱의 특징이 뭐냐면 이미지가 이렇게 있을 때 어떤 커널 또는 필터라고 부르는 작은 그게 있으면 이거를 왼쪽 여기다가 한번 적용해주고 그다음에 한 칸 옮겨서 그 옆에다가 한번 적용해주고 또 한 칸 옮겨서 여기다가 한번 적용해주고 이런 식으로 한 칸 한 칸 한 칸 옮겨가면서 적용을 해요

그러면 이 이미지가 아무리 이렇게 커도 얘 사이즈가 고정된 이상은 파라미터는 어차피 얘만 가지고 있단 말이에요

그러니까 입력의 이미지가 커지든 말든 파라미터 수가 증가를 안 하는 거죠

그래서 예를 들면 우리가 사람의 얼굴을 인식할 수 있는 커널이 있다

그러면 이미지가 어마어마하게 큰데 이 어마어마하게 큰 이미지에서 사람의 얼굴을 찾으려면 그냥 얘를 계속 이동을 시키면서 사람 얼굴을 찾을 때까지 계속 이동시키면 되겠죠

이미지가 더 커져도 계속 이동을 시키면 되겠죠

얘가 더 커지지 않는 이상은 파라미터가 똑같다 라는 거에요

그래서 이거를 도식화한 게 여기 아래 그림인데 우리가 리니어 레이어의 경우에는 여기가 입력이고 여기가 출력이면 입력과 출력이 다 서로서로 연결되어 있기 때문에 입력이 5개 출력이 3개다

그럼 이 선의 개수가 15개가 되겠죠

근데 합성곱에서는 이거 3개를 합성곱을 해서 이거 하나 만들고 그다음에 이거 3개를 합성곱해서 이거 하나 만들고 그다음에 이거 3개 합해서 이거 하나 만들고 이런 식이니까 파라미터가 3개만 있으면 됩니다

이게 이미지가 커지면 커질수록 이 차이가 더 심해지게 되는 거죠

그다음에 이미지라는 게 우리가 생각을 해보면 이미지는 보통 어떤 부분적인 패턴이 모여서 더 큰 부분이 돼요 사람 얼굴 같은 경우를 생각해보면 눈 코 입이 모여서 얼굴이 되거든요

그리고 눈이라는 거는 대체로 이렇게 이렇게 생겼습니다

그러면 이걸 구성요소별로 생각을 해보면 이렇게 위로 휘어지는 눈썹 있는 데가 있고 속눈썹 있는 데가 있고 눈 아래쪽 살이 있고 그다음에 눈동자가 있고 이런 식의 어떤 부분들이 있단 말이에요

그럼 우리가 눈동자를 인식할 수 있는 커널이 있고 속눈썹을 인식할 수 있는 커널이 있고 눈 아랫부분을 인식할 수 있는 커널이 있으면 걔네 3개의 출력을 합치면 눈을 인식할 수 있는 어떤 커널이 되겠죠

그래서 그런 식으로 이렇게 작은 부분을 쌓아서 더 큰 부분을 이루는 게 이미지의 특징이기 때문에 모델 구조도 그런 식으로 만들어져요

그러면 더 효율적으로 이미지를 처리를 할 수 있겠죠

그래서 이제 우리가 합성고 레이어를 만들면 합성고 레이어에서 이미지에 필터가 적용되는 영역 필터는 커널이랑 똑같은 표현이에요

필터 커널 다 똑같은 얘기에요

적용되는 영역을 리셉티브 필드라고 하고요 그다음에 출력되는 결과를 피처맵이라고 합니다

왜냐하면 피처라는 게 어떤 특징이거든요

그래서 특징을 맵핑했다

맵핑은 수학적 용어로 사상인 것이죠

그러니까 뭐라고 해야 될까요

우리말에 딱 쉬운 표현이 없는데 맵핑이 어딘가에 딱 대응시키는 그런 거를 맵핑한다고 합니다

특징을 뽑아서 대응을 시켰다

이런 얘기고 그래서 여기 보시면 리셉티브 필드가 있는데 여기 보면 커널이 이렇게 3칸 3칸 그다음에 옆으로 옮겨가서 3칸 3칸 또 옆으로 옮겨서 3칸 3칸 이런 식으로 적용이 되면 그 커널의 적용 결과가 하나씩 이렇게 튀어나오는 거죠

그러면 이제 여기가 피처맵이 됩니다

그래서 실제로 어떤 식으로 합성고 신경망이 인식을 하냐면 여기 왼쪽을 보시면 왼쪽은 어떤 이미지에 가장 가까운 부분에 적용되는 필터들인데 이 필터들마다 인식하는 패턴이 달라요

또는 커널이라고도 하는데 어떤 커널은 보시면 여기 가운데가 밝고 위에 모서리가 어두운 패턴 어떤 커널은 위쪽 모서리가 어두운 패턴 어떤 커널은 위에는 밝고 아래는 어두운 패턴 자기가 데이터로부터 학습을 하는 거예요

누가 뭐라고 시키지 않아도 우리가 일일이 이렇게 하라고 시킨 게 아니라 데이터로부터 자기가 이렇게 이런 패턴을 탐지해야 가장 성능이 잘 나온다라는 거를 각각의 커널이 거기에 맞게 튜닝이 되는 거고 그러면 이 출력들이 합쳐져서 여기 보시면 이런 건 약간 코 모양이죠

이런 건 눈 모양이고 이거는 약간 눈 모양이죠

약간 벌린 입 모양이고 이런 식으로 이 커널들이 내놓는 출력을 조합을 해서 더 복잡한 어떤 패턴을 인식하는 커널이 만들어집니다

그러면 그게 또 오버랩이 되면 그다음 레이어로 올라가면 사람의 얼굴에 코 중심으로 하는 부분이라든지 이마 부분이라든지 턱 부분이라든지 이런 다양한 어떤 특징들을 인식하는 또 커널이 만들어지는 거예요

그래서 이런 식으로 부분에서 전체로 차근차근 모델이 만들어지고 파라미터 수도 그러면서도 그렇게 크게 많이 필요하지 않은 것이 특징이 됩니다

그래서 우리가 보통 이미지 처리를 하면 아까 앞에서 했던 거 같은 그런 단순한 구조의 신경망은 거의 안 쓰고요 보통은 합성곱 신경망 아니면 요즘에는 비전 트랜스포머라는 걸 쓰는데 거기에 대해서는 나중에 얘기를 하도록 하고 일단은 합성곱 신경망을 이런 식으로 만든다

그래서 합성곱 신경망을 이제 적용을 하면 우리가 이제 커널 또는 필터를 몇 개 적용하느냐에 따라서 채널이 바뀝니다

무슨 말이냐면 예를 들어서 우리가 이제 눈을 인식하는 패널이 있다

그럼

그걸 이미지에 적용하면 눈의 피처맵이 나옵니다

이 눈의 피처맵에서는 눈이 있는 부분만 숫자가 높게 나오고 예를 들면 사람 얼굴이 이렇게 있는데 사람 얼굴이 이렇게 있는데 예를 들어서 우리가 눈을 적용하는 필터를 여기다 쫙 적용하면 눈이 있는 데만 이렇게 출력이 나오고 나머지는 이제 출력이 안 나와요

그 다음에 코를 인식하는 필터를 적용하면 코 있는 데만 뭔가 이렇게 출력이 나오고 나머지는 출력이 안 나와요 입을 인식하는 필터를 적용을 하면 입 있는 데만 이렇게 출력이 나오고 나머지는 출력이 안 나와요

그래서 그러면은 원래는 이제 이미지가 여기 채널이 하나였는데 그때는 채널이 하나였는데 필터마다 각각 피처맵을 출력하니까 채널이 필터가 세 개니까 채널이 세 개로 늘어났죠

그래서 필터 수만큼 채널이 바뀌는 그런 특징이 있고 그 다음에 피처맵의 크기가 대체로 감소하는데 왜 감소하냐면 우리가 이제 가로 세로 4짜리 이미지가 있는데 여기에다가 가로 세로 3짜리 필터를 적용을 하면 색깔을 초록색으로 할까요

전광판 그러면은 처음에 이렇게 3X3에 적용이 되겠죠

그래서 출력이 뭔가 하나 나옵니다

노란색이 잘 안 보이네요

노란색 말고 초록색 뭔가 하나가 출력이 나오겠죠

그 다음에는 이렇게 아홉 개에다가 적용이 됩니다

그러면은 그 다음에 출력이 또 하나 나오겠죠

그 다음에 이제 오른쪽으로 더 갈 수 없으니까 그 다음에는 여기에 아홉 개에 적용이 됩니다

그럼 이 밑으로 출력이 나오겠죠

그 다음에는 이렇게 아홉 개에 적용이 됩니다

그러면 출력이 또 하나 이렇게 나오겠죠

그러면은 원래 이미지는 원래 이미지는 가로 세로 4개였는데 이거를 필터를 한번 적용하니까 가로 세로 2개로 한 픽셀씩 좌우에서 한 픽셀씩 줄어들어요

그래서 피처맵의 크기가 감소를 하게 됩니다

그래서 이제 여기 가운데 그림을 보시면 처음에 되게 큰 그림이 있었는데 이 그림이 점점 작아지죠

작아지고 그림의 어떤 두께가 늘어나는데 이 두께는 원래는 이제 RGB라서 채널이 3개였는데 필터의 개수가 늘어나니까 점점점점점 이렇게 두꺼워지게 됩니다

그 얘기는 뭐냐면 어떤 이미지에서 뽑아내는 특징의 종류는 많아지고 각각의 특징은 굉장히 추상적인 특징이기 때문에 우리가 원래는 지금 224, 224짜리 그림을 보고 있었는데 이제 굉장히 추상화되서 나중에는 13 곱하기 13 대략 얼굴이 이쯤에 있다 대략 손발이 이쯤에 있다 이런 식으로 축소된 어떤 내용을 가지게 되는 겁니다

그래서 이제 그런 식으로 기본 구조를 쭉 쌓아 올리면 여기로 가면 아주 어떤 추상적인 특징과 대략 어느 정도 위치에 뭐가 있다

이런 정보만 남게 되고 그럼 여기다가 우리가 지금까지 했던 것처럼 다층신경망을 쌓아 올려서 예를 들면 머리가 위쪽에 있고 발이 아래쪽에 있으면 이 사람은 서 있는 거죠

발이 위에 있고 머리가 밑에 있으면 이 사람은 물구나무 서 있는 거죠

이런 식으로 어떤 대략적인 정보를 통합해서 최종적인 출력을 하게 됩니다

이런 식으로 만드는 게 CNN의 기본 구조가 되고 여기에서 뭐 이런저런 변형이 가해지게 됩니다

그래서 이제 CNN도 한번 구현을 해보면 여기 보시면은 여기에도 이제 우리 CNN 구현을 하는데 여기 이제 컴보 2D 이렇게 해서 2D 이미지에 합성구업을 적용을 하는데 여기에 이제 그 원래 이제 흑백 이미지이기 때문에 채널이 하나로 시작해서 이제 우리가 32개의 커널을 적용을 하게 되고요 그다음에 커널 사이즈는 가로 세로 3이 됩니다

크기가 28, 28이기 때문에 이제 크기 3짜리 커널을 적용하면은 아래에서 하나 위에서 하나가 날라가거든요

그래서 사이즈가 26으로 줄어들어요

이미지 사이즈가 피쳐맵 사이즈가 그다음에 이제 32개에서 64개의 어떤 특징을 뽑아내는데 그럼 이제 피쳐맵의 개수가 64개가 되고 또 커널 사이즈가 3이면 앞뒤로 하나씩 빠져가지고 피쳐맵의 사이즈가 24로 이제 줄어들게 됩니다

그러면은 원래는 가로 28 세로 28에 흑백이니까 채널 수가 하나인 여기는 이제 하나죠

이런 이미지가 들어왔는데 최종적으로는 64개의 특징과 가로 세로 24인 어떤 피쳐맵이 만들어지게 됩니다

그래서 이 피쳐맵을 가지고 최종 예측을 하는 이런 합성구업 신경망입니다

그리고 여기는 보시면 플래튼에 해당되는 부분이 없어요

왜냐하면 이미지 그대로 들어가서 처리가 되기 때문에 그래서 앞에 다층 신경망하고 구조를 보시면 앞에 다층 신경망에서는 여기 보시면 x.view 해가지고 이 그림을 플래튼하는 부분인데 이 부분이 이제 없어집니다

필요 없기 때문에 그래서 이렇게 되는 것이 기본적인 합성구업 신경망의 구조가 된다

이런 얘기고요 이 뒤에가 지금 잘려서 없는데 이제 이거를 모델로 만드시려면 이거 왜 이렇게 됐죠

PL.lightning 모듈 이렇게 되는데 여기 멀티클래스 모델에서 프로세스 배치부터 이거를 복사해서 쓰시던지 아니면은 그냥 복사해서 씁시다

그래서 이닛이랑 포워드는 똑같은 새로 정했으니까 이건 빼고 밑에 있는 애들을 복사해가지고 여기다 이렇게 붙여 넣으시면 됩니다 ...

그 다음에 이제 트레이닝 시킬 때는 코드는 똑같고 여기 모델 정의하는 부분만 바꾸시면 돼요 ...

그러면은 나머지는 똑같습니다



```python copy
class CNN(pl.LightningModule):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)  # 크기가 2만큼 줄어듦: 28 → 26
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3) # 크기가 2만큼 줄어듦: 26 → 24
        self.fc = nn.Linear(64*24*24, 10)
        self.accuracy = MulticlassAccuracy(10)
        
    def forward(self, x):
        # 입력에는 Flatten이 없음
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x)) 
        x = x.view(-1, 64*24*24) # 여기서 Flatten
        x = self.fc(x) # 분류
        return x
    def process_batch(self, batch):
        images, labels = batch
        outputs = self(images)  # 모형은 확률이 아닌 로짓(logit)을 출력
        loss = F.cross_entropy(outputs, labels)
        return outputs, loss
    def training_step(self, batch):
        outputs, loss = self.process_batch(batch)
        self.log('train_loss', loss)
        return loss

    def test_step(self, batch):
        outputs, loss = self.process_batch(batch)
        _, labels = batch
        accuracy = self.accuracy(outputs, labels)
        self.log('test_accuracy', accuracy)
        return loss
    
    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)
        return optimizer

```



```python copy
# 훈련
model = CNN() # 여기만 바꿔주세요
trainer = Trainer(max_epochs=3, default_root_dir=log_dir)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
trainer.fit(model, train_loader)

```



```python copy
# 테스트
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
trainer.test(model, test_loader)

```

:::info[output]
```
Testing: |                                                                                       | 0/? [00:00<…
```

```
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃[1m [0m[1m       Test metric       [0m[1m [0m┃[1m [0m[1m      DataLoader 0       [0m[1m [0m┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│[36m [0m[36m      test_accuracy      [0m[36m [0m│[35m [0m[35m   0.9041757583618164    [0m[35m [0m│
└───────────────────────────┴───────────────────────────┘

```

```
[{'test_accuracy': 0.9041757583618164}]
```

:::



```python copy
np = 0
for param in model.parameters():
    np += sum(param.shape)
np
```

:::info[output]
```
37121
```

:::



```python copy
sum(param.shape)
```

:::info[output]
```
39
```

:::


## 패딩

우리가 이제 이 필터를 적용을 할 때마다 이미지 크기가 조금씩 줄어드는데 요거를 막는 방법이 몇 가지 있습니다

한 가지 방법은 패딩이라는 건데요 우리가 3x3를 한번 적용을 하면 왼쪽에서 한 칸 오른쪽에서 한 칸 이렇게 깎여 나가거든요

그러면은 피처맵의 크기가 줄어들지 않게 하려면 어떻게 하면 되겠습니까

옆에다가 빈 픽셀을 하나 더 적용을 해 주면 되겠죠

그래서 이제 빈 픽셀을 이렇게 적용하는 거를 패딩이라고 합니다

우리가 이제 패딩 잠바 입으면은 솜을 채워 가지고 몸이 뚱뚱해지잖아요

그림에다가 비어 있는 픽셀을 이렇게 붙여 주는 거 그래서 이렇게 붙여 주면은 어떤 효과가 있냐면 예를 들어 어떤 필터가 있는데 3x3 필터가 있는데 예를 들어서 이게 코를 인식하는 부분이 여기고 눈이 이렇게 눈 코 입 이렇게 인식을 한다

예를 들어서 좀 말은 안 되지만 근데 어떤 사람이 사진을 찍었는데 얼굴이 여기 너무 모서리 쪽에 있어서 이렇게 짤린 거예요

문제가 뭐냐면 패딩을 안 하면 이 사람의 요 코를 인식을 할 수가 없습니다

왜냐하면 여기까지밖에 필터가 안 들어가니까 근데 패딩을 해 주면은 여기에다 이만큼 이제 비어 있는 거를 적용을 하면은 이렇게 필터가 적용을 해 주니까 필터를 이렇게 적용을 할 수 있겠죠

그러면은 여기 이제 눈은 어차피 없으니까 됐고 근데 이제 코랑 입이 있으니까 이 필터가 이제 적용이 될 수가 있습니다

즉 뭔가 패딩을 해 주면 생기는 장점이 피처맵이 안 줄어든다는 것도 있지만 이렇게 구석탱이에 있는 것도 어느 정도 탐지를 할 수 있다

구석탱이나 화면가에 걸리는 이런 것도 탐지를 할 수 있다

이런 장점이 있습니다

그래서 이제 패딩을 이렇게 얘기를 하는데 제로패딩은 뭐냐면은 패딩을 안 하는 겁니다

제로패딩은 사실 뭐 이런 이름이 필요 없죠

그냥 패딩을 안 하는 거니까 그다음에 하프패딩이라고도 하고 세임패딩이라고도 하는데 요거는 뭐냐면 우리가 이제 3X3인 경우에는 한 픽셀씩 더 추가를 해 주면 입력하고 출력의 크기가 똑같기 때문에 하프라는 건 왜 하프냐면은 3을 2로 나누면은 이제 1.5인데 내림하면 1이 되잖아요

그래서 그런 의미에서 하프패딩이라고 하고 세임패딩이라고 하는 거는 결과물의 크기가 입력하고 똑같기 때문에 세임패딩이라고 합니다

하프세임 같은 얘기고요 그다음에 풀패딩은 뭐냐면 지금 우리가 이제 여기 한 칸씩 더 하면 이렇게는 적용할 수 있는데 뭐 예를 들면 여기 얼굴에 점이 있어요

그거는 만약에 이 사람이 더 많이 잘려 가지고 뺨만 이렇게 나와요 점이 이렇게 있으면 요거는 하프패딩을 해도 여기까지는 걸리지 않죠?

풀패딩은 뭐냐면 여기다 2칸을 더 해주는 겁니다

그러면은 요 부분도 여기 걸릴 수가 있겠죠

그래서 풀패딩은 필터의 모든 부분이 모든 픽셀에 다 걸릴 수 있도록 필터 크기 빼기 1만큼 그러니까 우리가 지금 3칸짜리 필터를 쓰면은 2칸을 패딩을 해 주는 거를 풀패딩이라고 해요

풀패딩을 해 주면 당연히 출력이 입력보다 커지게 됩니다

왜냐면 왼쪽으로 2줄 오른쪽으로 2줄이 들어가는데 한 줄 한 줄 깎이니까 결과적으로는 한 칸만큼 늘어나는 거죠

그래서 이 중에서 제일 일반적으로 많이 쓰이는 거는 이제 세임패딩 하프패딩이고요 그래서 이제 하프패딩을 넣고 싶다

그러면은 패딩은 1 이렇게 넣어 주시면 됩니다

그러면은 원래 이제 28 곱하기 28 이미지가 들어갔는데 하프패딩 또는 세임패딩을 해 줬기 때문에 여기도 28 곱하기 28이 되고 그래서 요 숫자가 들어가는 부분을 다 28 곱하기 28로 해 주시면 됩니다

근데 요 사이즈를 딱딱 맞춰 줘야 되기 때문에 사이즈를 계산을 해 봐야 되거든요

그래서 그렇게 하시면 되고 우리 여기 코드에서는 우리 코드에서는 이제 요 부분이 바뀌고 요 부분이 이렇게 바뀌고 여기 세 줄이 바뀌고 그 다음에 여기 뷰에서도 28 이렇게 바뀌게 되겠죠

그래서 이제 306쪽에 있는 것처럼 여기 고쳐 주시면 세임패딩을 적용을 할 수 있다


```python copy
class CNN(pl.LightningModule):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1) # 세임 패딩: 28 → 28
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1) # 세임 패딩: 28 → 28
        self.fc = nn.Linear(64*28*28, 10)
        self.accuracy = MulticlassAccuracy(10)
        
    def forward(self, x):
        # 입력에는 Flatten이 없음
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x)) 
        x = x.view(-1, 64*28*28) # 여기서 Flatten
        x = self.fc(x) # 분류
        return x
    def process_batch(self, batch):
        images, labels = batch
        outputs = self(images)  # 모형은 확률이 아닌 로짓(logit)을 출력
        loss = F.cross_entropy(outputs, labels)
        return outputs, loss
    def training_step(self, batch):
        outputs, loss = self.process_batch(batch)
        self.log('train_loss', loss)
        return loss

    def test_step(self, batch):
        outputs, loss = self.process_batch(batch)
        _, labels = batch
        accuracy = self.accuracy(outputs, labels)
        self.log('test_accuracy', accuracy)
        return loss
    
    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)
        return optimizer

```


## 풀링


요거는 이제 트레이닝 안 시키고 넘어가도록 하고요 그 다음에 이제 풀링이라는 것도 있는데 이 풀링은 뭐냐면 우리가 이제 그 그저께 기억나실지 모르겠지만은 직선 검출할 때 차선이 이렇게 있으면 직선이 하나만 요렇게 나와야 되는데 차선 왼쪽 차선 오른쪽 이런 식으로 나와서 제가 이렇게 좀 없애보려고 했는데 잘 안 됐던 거 기억나시나요?

그러니까 우리가 어떤 피처를 추출을 하면은 대부분의 경우 피처가 이렇게 하나만 나오는 게 아니라 그 옆에 비슷한 게 또 나오는 그런 경우가 많이 있습니다

예를 들면 사람 얼굴이다

이러면은 여러분이 이렇게 멀리서 눈을 이렇게 하고 이제 좀 보신다고 생각하면 사람 얼굴이 이렇게 여러 개로 상이 겹쳐 보이거든요

그러니까 어떤 이미지에서 피처를 추출할 때 정확하게 그 건반 딱 추출되는 경우가 잘 없고 그 근처에서 비슷비슷한 피처가 여러 번 추출이 되는 특징이 있어요

그래서 이렇게 피처가 여러 번 추출되면 되게 불필요한 계산이기 때문에 이 피처가 여러 번 추출되면 피처마다 뭔가 계산을 해줘야 되잖아요

굉장히 불필요하기 때문에 풀링 또는 서브 샘플링 또는 다운 샘플링이라고 하는 거를 해서 이 최종적으로 추출된 피처를 축소를 해줍니다

그래서 뭔가 중복되는 피처를 제거를 하려는 게 목적이에요

그래서 중복되는 피처를 제거를 하면 그 파라미터가 줄어들기 때문에 더 속도를 높일 수가 있는 거죠

그래서 이제 보통 최대 풀링이라는 방법을 쓰는데 어떻게 하냐면 우리가 예를 들면 사람의 눈을 추출한다

얼굴이 이렇게 있는데 눈을 추출한다

근데 눈이 눈을 추출하는데 이게 딱 눈에 가서 정확하게 우리가 걸리는 게 아니고 약간 여기도 눈 같고 여기도 눈 같고 여기도 눈 같고 눈의 후보가 여러 개가 동시에 추출이 되거든요

그중에서 제일 눈 같은 부분을 남기는 거예요

그래서 2X2로 4개의 어떤 피처가 있으면 이 4개 중에 제일 눈 같은 데가 어디냐?

그럼 여기가 제일 눈 같다

그럼 나머지 3개는 그냥 버리는 겁니다

만약에 4개 중에 여기가 제일 눈 같다

그럼 여기는 눈이 없겠죠

그래서 이런 데는 다 버리는 거예요

그래서 이 풀링을 적용을 하면 이 피처 맵의 크기가 반토막으로 납니다

원래 28X28이었는데 2X2로 해서 4개 중에 하나만 뽑으니까 1분의 4로 크기가 줄어들겠죠

그래서 이쪽으로 반 이쪽으로 반 14X14가 되고요 여기가 눈 같다

여기가 눈 같다

7X7이어야죠

이런 식으로 줄어들게 됩니다

그래서 여기서 코드에서도 보시면 여기가 원래는 지금 강의 자료 보시면 28X28로 되어 있는데 원래는 28X28인데 컨볼루션 레이어와 컨볼루션 레이어 사이에 풀링을 집어넣으면 여기 풀링 한 다음에 2 이렇게 하면 원래 크기가 28이었던 게 14로 줄어들고 그다음에 여기가 14가 되니까 다시 한번 풀링을 적용하면 14로 줄어들고 14에서 7로 반씩 줄어듭니다

그래서 이제 7X7 이렇게 되는 거고 여기도 이제 7X7이 되는 거죠

그래서 이거하고 비교하면 파라미터가 여기에서 파라미터가 64X28X28이었는데 64X7X7이니까 이쪽이 4분의 1이고 이쪽이 4분의 1이니까 전체적으로 파라미터가 16분의 1로 파라미터 수를 줄일 수가 있는 거죠

그래서 파라미터 수를 줄이면은 당연히 계산이 빨라지겠죠

그 대신에 이제 뭐 이거 풀링을 잘못하면 약간 이제 중복된 거를 제거한다고 풀링을 하는 건데 중복이 아닐 수도 있는 거죠

그래서 무조건 풀링을 한다고 좋지는 않은데 일반적으로는 풀링을 하는 것이 성능에 도움이 된다 라고 관찰이 되고 있습니다

그래서 이런 방법도 있다

이런 거고 이거 또 뭐 그냥 뭐 해볼 건 없으니까 그냥 아 이렇게 하는구나

정도로 생각하시면 됩니다

## 스트라이드


그 다음에 풀링을 사용하는 방법 대신에 풀링은 어떻게 하냐면 예를 들어서 이미지가 이렇게 4X4의 이미지가 있고 여기에다가 3X3의 커널 또는 필터를 적용하면 결과적으로 이제 2X2의 출력이 나오는데 이 중에서 제일 센 거를 하나 적용을 하는 거예요

그러면은 최종적으로 출력이 1이 됩니다

하나만 나오겠죠

스트라이드는 어떻게 하는 거냐면 아 이거 4X4라고 하지 말고 스트라이드는 어떻게 하는 거냐면 어차피 하나만 뽑을 거면 뭐하러 필터를 이렇게 촘촘하게 적용을 하냐

이거예요

필터를 예를 들면 좀 5X5다

그러면은 필터를 여기다가 적용을 했다

여기다가 적용을 했으면 그 다음에 옆으로 한 칸을 옮기는 게 아니라 예를 들면은 두 칸을 옮기는 거야

그러면은 어차피 옆으로 한 칸 옮겼다가 풀링에서 하나만 남길 거면 그냥 처음부터 두 칸을 건너뛰면 되는 거 아니냐

이거죠

그래서 스트라이드라고 해가지고 한 칸씩 옮기는 게 아니라 여기 밑에 그림을 보시면 한 칸씩 옮기는 게 아니라 여기 적용했으면 그 다음에는 두 칸을 건너뛰는 이런 방식을 쓰기도 합니다

근데 제 쓰기도 한다고 말씀은 드렸는데 실제로 보면은 저는 스트라이드 쓰는 거를 거의 본 적이 없거든요

잘 안 써요

일반적으로는 풀링을 더 많이 씁니다

근데 이제 이런 방법을 쓰기도 한다 라는 거고 그래서 이제 스트라이드를 적용을 하고 싶으면 여기다 그냥 스트라이드라고 써주시면 돼요

아이고 아이고 여기 이제 스트라이드 2 이렇게 적용이 되어있죠

그래서 이렇게 해도 결과적으로는 나오는 크기는 7x7이 됩니다

그래서 앞에랑 비교해서 보시면 풀링을 하는 경우는 풀링을 여기다 끼워 넣어줘야 돼요

풀링을 끼워 넣어줘야 되는데 스트라이드를 하는 경우에는 따로 뭐 끼워줄 게 없습니다

왜냐하면 스트라이드를 하는 거 자체로 이미 사이즈가 줄어들기 때문에 그래서 스트라이드를 2로 하면은 이제 반으로 줄어드는데 스트라이드를 3으로 하면 3분의 1로 줄어들겠죠

## CNN 모델 구조



그래서 이제 이런 식으로 우리가 CNN을 만들 수가 있는데 사실 지금까지 여러 가지 구조를 설명을 드렸지만은 우리가 이걸 다 조합을 하려면 조합이 너무 많기 때문에 사실 이걸 직접 조합하는 경우는 잘 없습니다

그래서 실제로는 어떻게 하냐면 기존에 남들이 만든 구조를 보고 그냥 똑같이 만들거든요

왜냐면 남들이 이제 시행착오를 해서 CNN을 이렇게 만드니까 잘 되더라 라는 거를 검증을 해 놓은 게 있는데 우리가 굳이 그걸 수십 년 전에 만들었을 때 그 구조를 다시 만들었을 때 굳이 그걸 수십 년 동안 해 왔는데 굳이 처음부터 다 다시 할 필요는 없겠죠

그래서 이런 것들을 일반적으로는 뭐 르넷5라든지 알렉스넷이라든지 뭐 이런 구조가 있습니다

잘 알려진 구조가 있고 보통 그냥 그걸 그냥 그대로 가져다 써요

그리고 검색을 해 보시면은 코드도 다 있거든요

그래서 직접 만드시는 경우는 잘 없고 그래서 우리가 수업에서는 직접 만드는 걸 해보지만은 이게 수업이니까 그렇지

실제로 하실 때는 그냥 코드 컨트롤 C 컨트롤 V가 답니다

그래서 어떤 구조를 만들든지 어떤 구조가 있느냐

사실 뭐 구조의 내무도 모르셔도 되지만 그래도 수업이니까 알긴 알아야겠죠

그래서 보면은 르넷5라는 구조가 있는데 이거는 이름이 어디서 나왔냐면 얀 르쿤이라고 아마 뉴욕대에 계시죠 뉴욕대 교수고 지금 페이스북 현재 이름은 메타라고 하는 그 회사의 AI 약간 총책임자 같은 정확한 위치는 잘 모르겠어요 직함이 우리나라랑 좀 달라서 얀 르쿤 교수라고 있습니다

딥러닝 분야의 대가 중에 한 명인데 얀 르쿤 교수가 만들어서 이름이 르넷입니다

이 사람도 원래 프랑스 출신이거든요

그래서 약간 제가 어제도 그런 얘기 들었는데 딥러닝 쪽에 보면은 그 대가들이 조금 뭐랄까

메인스트림은 좀 아닌 그런 느낌 약간 그런 게 좀 있어요

어쨌든 이 분이 만든 구조인데 우리가 배운 구조랑 기본적으로 똑같습니다

그래서 보면은 원래 32x32 이미지를 받아서 컨볼루션 합성 곱을 한번 적용을 하고요 그러면 28x28로 줄어드는데 이걸 보니까 아 패딩을 적용을 안 했구나

이런 거를 알 수 있죠

그다음에 풀링을 적용해서 반으로 줄어들고 그다음에 다시 합성 곱을 해서 10x10으로 줄어들고 여기서 다시 풀링을 해가지고 5x5가 되고 이거를 보면은 그 다음에 채널 수가 이제 이렇게 6에서 한번은 6으로 했다가 한번은 16으로 했다가 이런 구조인데 이거는 사실 굉장히 오래된 구조입니다

거의 한 2000년 이 정도?

굉장히 오래된 구조고 기본적으로는 우리가 했던 거랑 동일한 구조예요

그다음에 이제 알렉스넷이라고 해서 이게 2012년에 ILSVRC라는 대회에서 1등을 해가지고 이 이후로 딥러닝 유행이 시작이 됩니다

딥러닝 유행이 생각보다 그렇게 오래 안 됐어요

한 십 몇 년?

이 정도밖에 안 됐는데 보통 학계에서 뭔가 유행하는 속도를 생각을 해보면 정말 지금 반짝 유행을 하는 거죠

얼마나 갈지는 잘 모르겠지만 그래서 보시면은 이 대회가 이제 2011년 대회를 했을 때 이게 에러레이트기 때문에 밑으로 갈수록 좋은 거거든요

2011년에 1등이 여기까지 했습니다

2012년에 보면은 2012년에도 성적이 비슷해요

그러니까 다른 분야는 뭐 그닥 밝아졌지만 발전이 없었던 거죠

근데 딥러닝으로 한 알렉스넷이 엄청난 차이로 그러니까 1년 전하고 비교를 해보시면은 굉장한 차이로 성능을 내고 2등하고도 격차가 어마어마하죠

그러니까 정말 얼마나 충격적이었겠어요

다 보면은 고만고만한 사람들끼리 고만고만한 기술로 대결을 하는 건데 갑자기 엄청난 성능 격차를 보여주니까 사람들이 다 충격을 받아서 다음 대회 때부터는 다 딥러닝으로 갈아탑니다

그래서 성적이 좀 더 높은 것 같아요 쭉쭉쭉쭉해서 대화를 더 이상 안 하는데 하는 게 의미가 없어가지고 그래서 이게 유행을 선도했던 모델이고 근데 구조를 보시면 사실 르넷이랑 별로 큰 차이가 나지는 않습니다

보시면은 스트라이드도 집어넣고요 그다음에 5X5, 3X3 뭐 이런 거 하고 그다음에 이제 맥스플링 집어넣고 우리가 방금 했던 구조랑 이것도 그렇게 다르진 않아요

기본적으로는 비슷한 구조고 렐루를 쓴다던가 이거 우리가 건너뛴 정칙화 기법을 쓴다던가 이런 디테일이 들어가게 됩니다

그래서 뭐 이런 거 저런 거 이런 게 들어가게 되고요 그다음에 이제 VGGNET이라는 구조가 또 나오게 되는데 이 VGGNET이 아마 2013년인가

2014년 대회 1등을 했던 걸로 기억을 하는데 원래는 여기까지만 해도 컨볼루션 한 번, 풀링 한 번 컨볼루션 한 번, 풀링 한 번 이런 식으로 섞어서 썼는데 보니까 컨볼루션 세 번, 풀링 한 번 컨볼루션 두 번, 풀링 한 번 이런 식으로 하는 것도 괜찮다

뭐 이런 조합을 찾아내게 되고 레시피죠

이런 레시피를 찾아내게 되고 이렇게 하는 것이 더 괜찮더라

근데 이게 이제 왜 이렇게 되냐면 3X3를 어차피 두 번 적용하면 5X5를 한 번 적용하는 거랑 최종적인 결과가 비슷하거든요

5X5를 한 번 적용하는 거 3X3를 뭔가에 적용하는 거랑은 비슷하거든요

3X3를 적용을 하면 원래 5X5에다가 3X3를 적용하면 결과물이 3X3가 돼요

거기다가 다시 3X3를 적용하면 1X1이 되거든요

근데 5X5에다가 5X5를 한 번 적용하면 역시 결과물이 1X1이 됩니다

그러니까 입력하고 출력이 3X3 두 번 적용하나 5X5 한 번 적용하나 똑같은데 3X3는 파라미터 개수가 3, 3, 9니까 두 번 해봐야 18개거든요

근데 5X5는 5, 5, 25니까 파라미터가 이쪽이 더 많죠

그래서 처리할 수 있는 범위는 비슷한데 파라미터 개수가 더 적기 때문에 더 좋다

약간 이런 논리로 사실 대부분 이런 논리는 약간 사후 정당화 같은 거에요

그것 때문에 좋은지는 모르겠지만 하여간 뭐 아마도 그런 것 때문에 좋은 거 아니냐

이런 식의 얘기가 나오게 되고 그 다음에 이제 구글넷이라고 해서 이건 이름을 보시면 아시겠지만 약간 르넷의 패러디입니다

이름이 근데 이제 구글에서 만든 거죠

그래서 이제 이 사람들이 무슨 생각을 하냐면 지금까지는 레이어를 우리가 위로 위로 위로 위로 이렇게 순서대로 쌓아 올렸는데 그렇게 하지 말고 이거 위에다가 이렇게 병렬로 쌓아 올리는 방법을 찾아내요

근데 이렇게 하면 이제 뭐가 좋으냐

우리가 이제 3X3가 좋은지 5X5가 좋은지 잘 모른단 말이에요

그래서 병렬로 다 해보자 이거죠

병렬로 다 하면 이쪽이 좋으면 이쪽에 가중치가 많이 갈 거고 이쪽이 좋으면 이쪽에 가중치가 많이 갈 거고 그건 학습하는 과정에서 저절로 될 거니까 잘 모르겠고 병렬로 다 해보자

이런 거죠

그래서 이제 이런 구조도 나오게 되고 이거는 이제 건너뛰고요 그 다음에 이제 레즈넷이라는 모델도 나오는데 레즈넷을 만든 사람들은 무슨 생각을 하냐면 구글넷 같은 경우는 여기 레이어가 있는데 원래는 레이어를 순서대로 쌓아 올리다가 구글넷에서는 레이어를 병렬로 놓고 이쪽으로도 가고 이쪽으로 간 다음에 다시 합쳐서 이렇게 가는 그런 구조인데 레즈넷을 만든 사람들이 아니 꼭 레이어를 이렇게 쌓아 올리다가 레이저를 거칠 필요가 있어요?

이런 생각을 하는 거야

그래서 이런 식으로 만드는 거죠

그래서 원래는 이 레이어를 거쳐가야 되는데 아무것도 안 하고 그냥 건너뛰는 거를 하나 더 만듭니다

그래서 이거를 이제 스킵커넥션이라고 하는데 이렇게 만드는 논리는 뭐냐면 이 레이어가 꼭 쓸모가 있다는 보장이 뭐가 있냐

이거죠

그냥 건너뛰면 되지

그런 것도 있고 또 하나는 우리가 이제 사라지는 경사 문제가 있다는 얘기를 들었는데 그러니까 이게 계속 기울기기 때문에 기울기가 곱하기가 되기 때문에 이쪽으로 가면 기울기가 되게 작아진단 말이에요

근데 이렇게 건너뛰는 게 있으면 이거는 일종의 더하기처럼 되는 거죠

그래서 이쪽에서 예를 들면 경사가 거의 0이어가지고 곱하기 0 곱하기 0 곱하기 0 해서 경사가 없어져도 이쪽으로 전달되는 그 경로가 또 하나 있기 때문에 얘가 어쨌든 학습을 할 수는 있다

그래서 이렇게 건너뛰는 거를 넣는 것도 좋더라

이런 식으로 해서 이제 발전을 하게 됩니다

그래서 이제 최근에는 이제 구조가 굉장히 복잡해졌는데 그래서 제가 계속 얘기 드리지만은 그래서 모델을 직접 안 만든 이유가 이렇게 십 몇 년에 걸쳐서 수십 년에 걸쳐서 계속 구조가 복잡화돼서 사실 이거를 직접 만드는 거는 별로 의미가 없고 그래서 그냥 인터넷에다가 파이토치 레저넷 이렇게 검색을 하시면 다 코드가 있어요

이거는 아니고 그래서 레저넷이 아니라 그래서 레저넷 구현하기 뭐 이런 거 있으면 그냥 복사해서 쓰시면 됩니다 */}