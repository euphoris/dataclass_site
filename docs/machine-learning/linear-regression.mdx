# 선형 회귀

## 선형 모형

  * y: 종속변수 
  * x: 독립변수 
  * w: 가중치 또는 기울기 
  * b: 절편 ($x=0$ 일 때, y의 예측치) 
  * : 예측 오차 

## 잔차 residual

  * 잔차: 실제값과 예측값의 차이 
  * 잔차분산: 잔차를 제곱하여 평균낸 것 
    * cf. 분산: 편차(실제값과 평균의 차이) 제곱의 평균 
    * 잔차분산이 크다 → 예측이 잘 맞지 않음 
    * 잔차분산이 작다 → 예측이 잘 맞음 
  * 최소제곱법(Ordinary Least Squares): 잔차 분산이 최소가 되게 하는 w, b 등 계수를 추정 
    * 최소'제곱'법인 이유: 분산의 계산에 제곱이 들어가므로 
    * 가장 널리 사용되는 추정방법 

## 선형 모형 훈련

  * 훈련



```python
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, y_train)
```

  * 평가

```python
model.score(X_train, y_train)
```

  * 테스트



```python
model.score(X_test, y_test)
```

## 선형 모형의 계수 해석

```python
model.coef_
```

  * 데이터 준비할 때 입력한 순서: mileage, year, my\_car\_damage, other\_car\_damage, model(K3) 
  * 더미 변수의 경우 ABC 순으로 제일 처음인 것(예: Avante)이 0이고, 다른 것은 1이므로, 기울기는 그 둘의 차이가 됨 

## 범주형 변수에서 기울기 해석

  * 절편: 모든 특징이 0일 때 예측값 
      * 정규화된 데이터이므로 모든 값이 최소(0)일 때 + ABC 순 제일 처음 인 범주 
      * 표준화된 데이터일 경우 모든 값이 평균( )일 때 + ABC 순 제일 처음 인 범주 
  * 전처리에 따라서 절편의 기준 위치가 달라지지만 실질적으로 예측에 차이는 없음 



```python
model.intercept_
```

## 선형 모형의 정칙화

  * 라쏘(Lasso): 손실 함수에 L1 노름을 추가한 것. 기울기를 0으로 만드는 경향이 있음 
  * 릿지(Ridge): 손실 함수에 L2 노름을 추가한 것. 기울기를 전반적으로 작게 만드는 경향이 있음 
  * 엘라스틱 넷(Elastic Net): 손실 함수에 L1+L2 노름을 추가한 것 

```python
from sklearn.linear_model import Lasso, Ridge, ElasticNet
model = ElasticNet(alpha = 10., l1_ratio = 1.0)
model.fit(X_train, y_train)
model.score(X_train, y_train)
model.coef_
```

  * alpha: 클 수록 정칙화가 강해짐(계수가 작아짐). 과대적합을 억제하지만 과소적합이 될 수도 있음 
  * l1\_ratio: L1의 비율(0.0-1.0). 1.0일 경우는 Lasso와 같음. 0.0일 경우는 Ridge와 같음 



## 선형 SVR (Support Vector Regression)

  * 오차 범위(margin)를 최대화하는 직선을 찾음
  * 일반적인 선형 회귀는 오차(잔차)의 제곱 합을 최소화, 선형 SVR은 특정 오차 허용 범위(`epsilon`) 안으로 최대한 많은 데이터 샘플이 포함되도록 하는 가장 폭이 넓은 통로(street)를 찾는 것이 목표
  * 서포트 벡터(Support Vectors): 통로의 경계선상에 있거나 경계선 밖에 위치한 데이터 포인트
  * 장점:
      * 이상치에 강함: 모델이 주로 서포트 벡터에 의해 결정되므로, 오차 허용 범위 내에 있는 다른 데이터들이나 약간의 이상치(outlier)가 모델에 큰 영향을 주지 않습니다.
      * 정칙화: SVR은 `C`가 작을 수록 정칙화를 강하게 하여, 과대적합을 방지

$$
\min_{\mathbf{w}, b} \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^{N} \max(0, |y_i - f(x_i)| - \epsilon)
$$


```python
from sklearn.svm import LinearSVR

model = LinearSVR(epsilon=10.0, C=1.0, random_state=42)
model.fit(X_train, y_train)
model.coef_
```

## 퀴즈

import { QuizComponent } from "@/components/QuizComponent";

<QuizComponent quizId="regularization" quizItems={
    [
    {
        "item_type": "radio",
        "question": "선형 회귀에서 사용되는 최소제곱법(Ordinary Least Squares)의 목표는 무엇입니까?",
        "options": [
            "잔차(residual)의 분산을 최대화하는 것",
            "잔차(residual)의 분산을 최소화하는 것",
            "독립변수와 종속변수의 평균을 같게 만드는 것",
            "모든 잔차를 0으로 만드는 것"
        ],
        "hint": "잔차는 실제값과 예측값의 차이(오차)를 의미합니다. 모델의 성능이 좋으려면 오차가 작아야 합니다.",
        "solution": "잔차(residual)의 분산을 최소화하는 것"
    },
    {
        "item_type": "radio",
        "question": "선형 회귀 모델에서 '잔차(residual)'란 무엇을 의미합니까?",
        "options": [
            "독립변수의 평균",
            "실제값과 예측값의 차이",
            "회귀선의 기울기",
            "독립변수와 종속변수의 상관관계"
        ],
        "hint": "모델이 예측하고 남은 '나머지'라고 생각할 수 있습니다.",
        "solution": "실제값과 예측값의 차이"
    },
    {
        "item_type": "checkbox",
        "question": "선형 모델의 과대적합을 방지하기 위해 사용되는 정칙화(regularization) 기법을 모두 고르세요.",
        "options": [
            "라쏘 (Lasso)",
            "릿지 (Ridge)",
            "엘라스틱 넷 (Elastic Net)",
            "최소제곱법 (OLS)"
        ],
        "hint": "손실 함수에 L1 또는 L2 노름과 같은 페널티 항을 추가하는 방법들입니다.",
        "solution": [
            "라쏘 (Lasso)",
            "릿지 (Ridge)",
            "엘라스틱 넷 (Elastic Net)"
        ]
    },
    {
        "item_type": "radio",
        "question": "라쏘(Lasso) 정칙화의 가장 두드러진 특징은 무엇입니까?",
        "options": [
            "모든 계수(기울기)를 전반적으로 작게 만든다",
            "중요하지 않은 변수의 계수(기울기)를 0으로 만드는 경향이 있다",
            "모델의 절편(intercept)에만 영향을 준다",
            "항상 릿지(Ridge)보다 더 나은 성능을 보인다"
        ],
        "hint": "이러한 특징 때문에 라쏘는 변수 선택(feature selection) 효과가 있다고 말합니다.",
        "solution": "중요하지 않은 변수의 계수(기울기)를 0으로 만드는 경향이 있다"
    },
    {
        "item_type": "radio",
        "question": "scikit-learn의 `ElasticNet` 모델에서 `l1_ratio` 파라미터를 1.0으로 설정하면 어떤 모델과 동일해집니까?",
        "options": [
            "릿지 (Ridge)",
            "라쏘 (Lasso)",
            "일반 선형 회귀 (LinearRegression)",
            "선형 SVR (LinearSVR)"
        ],
        "hint": "`l1_ratio`는 L1 페널티(라쏘)의 비율을 의미합니다. 1.0은 100%를 뜻합니다.",
        "solution": "라쏘 (Lasso)"
    },
    {
        "item_type": "radio",
        "question": "선형 SVR(Support Vector Regression)의 주된 목표는 무엇입니까?",
        "options": [
            "모든 데이터의 잔차 제곱 합을 최소화하는 것",
            "특정 오차 허용 범위(epsilon) 안으로 가장 많은 데이터가 포함되는 가장 폭이 넓은 통로(street)를 찾는 것",
            "데이터의 분산을 최대화하는 것",
            "데이터를 두 개의 그룹으로 분류하는 것"
        ],
        "hint": "일반적인 선형 회귀와 달리, SVR은 특정 범위 내의 오차는 허용합니다.",
        "solution": "특정 오차 허용 범위(epsilon) 안으로 가장 많은 데이터가 포함되는 가장 폭이 넓은 통로(street)를 찾는 것"
    },
    {
        "item_type": "radio",
        "question": "선형 SVR이 이상치(outlier)에 강한 이유로 가장 적절한 것은 무엇입니까?",
        "options": [
            "학습 전에 모든 이상치를 자동으로 제거하기 때문에",
            "모델이 주로 통로의 경계선(margin)에 있거나 밖에 있는 서포트 벡터(Support Vectors)에 의해 결정되기 때문에",
            "오차 계산 시 제곱을 하지 않기 때문에",
            "모든 데이터에 동일한 가중치를 부여하기 때문에"
        ],
        "hint": "오차 허용 범위 안에 있는 '평범한' 데이터들은 모델 결정에 큰 영향을 주지 않습니다.",
        "solution": "모델이 주로 통로의 경계선(margin)에 있거나 밖에 있는 서포트 벡터(Support Vectors)에 의해 결정되기 때문에"
    }
]
} />


## Q&A
<iframe src="https://tally.so/embed/wbOOKg?alignLeft=1&hideTitle=1&transparentBackground=1&dynamicHeight=0" loading="lazy" width="100%" height="274" frameborder="0" marginheight="0" marginwidth="0" title="Q&A"></iframe>