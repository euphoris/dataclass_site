# 강화 학습

## 강화 학습 (Reinforcement Learning)

*   **핵심:** **행위자(Agent)**가 **환경(Environment)**과 **상호작용**하며 **시행착오** 통해 **보상(Reward)** 최대화하는 **행동(Action)** 학습.
*   **구성 요소:**
    *   **행위자(Agent):** 학습 주체. 행동 결정.
    *   **환경(Environment):** 행위자가 상호작용하는 외부 세계. 상태 변화, 보상 제공.
    *   **상태(State, S):** 현재 상황 나타내는 정보.
    *   **행동(Action, A):** 행위자가 취할 수 있는 선택지.
    *   **보상(Reward, R):** 행동 결과로 환경이 제공하는 즉각적 피드백 (좋음/나쁨).
    *   **정책(Policy, π):** 특정 상태에서 어떤 행동 선택할지 결정하는 전략/규칙.
    *   **수익(Return, G):** 미래까지 고려한 **장기 누적 보상** 합계. 강화학습 목표는 이 **수익 최대화**.
*   **목표:** 현재 상태(S)에서 장기 수익(G) 가장 크게 하는 최적 정책(π*) 찾는 것.
    $$ A = \max_{\pi} E[G_t | S_t=s] $$
    *(t 시점 상태 s에서 정책 π를 따를 때의 기대 누적 보상)*

*(슬라이드 241 그림: 강화학습 기본 구조 참고)*

## 강화 학습 예시

*   **게임 인공지능:** 알파고(바둑), Atari 게임 등. 환경(게임 규칙) 내에서 스스로 학습하여 최적 전략(정책) 수행.
*   **로봇 제어:** 로봇이 다양한 환경에서 시행착오 통해 걷기, 물건 잡기 등 최적 행동 학습.
*   **자율 주행:** 주행 상황(상태) 맞춰 차선 변경, 속도 조절 등 최적 경로/행동(정책) 선택.
*   **추천 시스템, 자원 관리 등**

## ChatGPT와 강화 학습 (RLHF)

*   ChatGPT 같은 대규모 언어 모델 훈련 시 **RLHF (Reinforcement Learning from Human Feedback)** 기법 활용.
*   **과정:**
    1.  (지도학습) 초기 모델 학습.
    2.  (보상 모델 학습) 모델 생성 여러 응답에 대해 **인간이 선호도 순위** 매김. 이 데이터로 **보상 모델** 학습 (좋은 응답 = 높은 보상).
    3.  (강화학습) 초기 모델이 보상 모델로부터 **높은 보상 받도록** 강화학습 알고리즘(PPO 등) 통해 **정책(응답 생성 방식) 미세 조정**.

## 지도 학습과 강화 학습의 차이

| 구분         | 지도학습 (Supervised Learning)                | 강화학습 (Reinforcement Learning)                   |
| :----------- | :------------------------------------------ | :-------------------------------------------------- |
| **문제 정의** | X 주어졌을 때 **Y 예측** 문제             | X(상태)에서 **최대 보상 얻는 행동(A)** 찾는 문제 (Y 불필요) |
| **목표**     | 예측 오차 (Y와 Ŷ 차이) 최소화             | 누적 보상 (Return) 최대화                         |
| **필요 데이터**| (X, Y) 정답 **라벨 있는 데이터**            | 라벨 데이터 대신 **환경과의 상호작용**(시행착오) 필요 |
| **학습 신호** | **정답(Y)** 과의 비교 (오차)                | 환경이 주는 **보상(R)**                             |
| **예시(바둑)** | 현재 상황(X) → 프로 기사 둔 수(Y) 모방 학습 | 현재 판세(X) → 여러 수(A) **둬보고** 결과(보상)로 학습 |
| **예시(투자)** | 기업 정보(X) → 과거 주가(Y) 예측 학습     | 기업 정보(X) → 매수/매도/보유(A) **해보고** 수익(보상)으로 학습 |

*(슬라이드 245 얀 르쿤 비유 참고: 정보량 관점에서 강화학습은 지도학습보다 적은 피드백(체리) 사용)*

## 강화학습의 현업 적용 이슈

*   **데이터 부족:** 지도학습과 달리 명시적 (상태, 행동, 다음 상태, 보상) 데이터셋 구하기 어려움.
*   **탐색 비용:** 행위자가 최적 정책 찾기 위해 **직접 많은 시행착오** 필요. 현실 세계 적용 시 시간/비용/안전 문제 발생 가능 → **시뮬레이션 환경** 구축 중요 (바둑이 빨리 정복된 이유: 완벽한 시뮬레이션 가능).
*   **보상 함수 설계 어려움:** 목표 달성 유도하는 **적절한 보상 함수 정의** 어려움. 잘못 설계 시 **의도치 않은 허점(exploit)** 찾는 방향으로 학습 가능.
    *   예: 테트리스 '오래' 플레이 보상 → 게임 진행 안 하고 '일시정지'하는 행동 학습.

## 퀴즈

<iframe src="https://tally.so/embed/nGb8Pk?alignLeft=1&hideTitle=1&transparentBackground=1&dynamicHeight=1" loading="lazy" width="100%" height="2312" frameborder="0" marginheight="0" marginwidth="0" title="[통계] 강화학습"></iframe>