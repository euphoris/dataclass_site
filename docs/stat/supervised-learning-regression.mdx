# 지도학습: 회귀

## 회귀 알고리즘

*   Linear Regression (선형 회귀)
*   Support Vector Regression (SVR)
*   Nearest Neighbors (k-NN 회귀)
*   Decision Tree (회귀 트리)
*   Naive Bayes (연속형 변수 처리 방식 따라 회귀 가능)
*   Artificial Neural Network (ANN)

## 중고차 가격 데이터 (회귀 예제)

*   데이터 로드.
    ```python
    df = pd.read_excel('car.xlsx')
    ```
*   X (독립변수), y (종속변수 'price') 분리 및 더미 코딩.
    ```python
    y = df['price']
    X = df.drop(columns='price')
    X = pd.get_dummies(X, drop_first=True) # 범주형 변수 더미 코딩
    ```
*   데이터 분할.
    ```python
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    ```

## 선형 모델 (scikit-learn)

*   기본 선형 회귀분석과 동일 기능.
*   훈련:
    ```python
    from sklearn.linear_model import LinearRegression
    linear_model = LinearRegression()
    linear_model.fit(X_train, y_train)
    ```
*   계수 확인: (`statsmodels`와 달리 통계적 유의성 정보 등 제공 안 함)
    ```python
    print("Coefficients:", linear_model.coef_)
    print("Intercept:", linear_model.intercept_)
    ```
*   예측:
    ```python
    y_pred_linear = linear_model.predict(X_test)
    ```

## 선형 모델 성능 테스트 (회귀)

*   **RMSE (Root Mean Squared Error):** 평균 제곱 오차(MSE)의 제곱근. 예측 오차 크기 나타냄. 작을수록 좋음.
*   **결정계수 ($R^2$):** 모형 설명력. 0~1 범위, 1에 가까울수록 좋음.
```python
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# RMSE 계산
rmse_linear = np.sqrt(mean_squared_error(y_test, y_pred_linear))
# 또는 root_mean_squared_error 사용 (최신 버전)

# R^2 계산
r2_linear = r2_score(y_test, y_pred_linear)

print(f"Linear Regression RMSE: {rmse_linear:.2f}")
print(f"Linear Regression R2: {r2_linear:.4f}")
```

## 지표 직접 계산 (참고)

```python
import numpy as np

# RMSE
rmse_manual = np.sqrt(np.mean((y_test - y_pred_linear) ** 2))

# R^2
mse = np.mean((y_test - y_pred_linear) ** 2)
var_y = np.var(y_test) # 또는 np.mean((y_test - np.mean(y_test)) ** 2)
r2_manual = 1 - (mse / var_y)
```

## 랜덤 포레스트 회귀 (Random Forest Regression)

*   여러 회귀 트리(Regression Tree) 예측값의 **평균** 사용.
*   훈련:
    ```python
    from sklearn.ensemble import RandomForestRegressor
    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
    rf_model.fit(X_train, y_train)
    ```
*   예측:
    ```python
    y_pred_rf = rf_model.predict(X_test)
    ```

## 특징 중요도 (Feature Importance)

*   랜덤 포레스트 같은 트리 기반 앙상블 모델에서, 각 **특징(독립변수)이 예측에 얼마나 기여**했는지 상대적 중요도 계산 가능.
    *   보통 트리 분기 시 불순도(impurity) 감소량 등을 기준으로 계산.
*   해석은 불가능해도, 어떤 변수가 중요한지는 파악 가능.
```python
import matplotlib.pyplot as plt
import numpy as np

importances = rf_model.feature_importances_
feature_names = X.columns # 또는 X_train.columns
indices = np.argsort(importances)[::-1] # 중요도 내림차순 정렬

plt.figure(figsize=(10, 6))
plt.bar(range(X.shape[1]), importances[indices], align='center')
plt.xticks(range(X.shape[1]), feature_names[indices], rotation=90)
plt.title('Feature Importance (Random Forest Regression)')
plt.tight_layout()
plt.show()
```

## 랜덤 포레스트 회귀 평가

```python
# RMSE 계산
rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))

# R^2 계산
r2_rf = r2_score(y_test, y_pred_rf)

print(f"Random Forest RMSE: {rmse_rf:.2f}")
print(f"Random Forest R2: {r2_rf:.4f}")
```
*   선형 모형 비해 설명가능성 낮지만, 대체로 예측 성능 높음.

## 퀴즈

<iframe src="https://tally.so/embed/3xzOWv?alignLeft=1&hideTitle=1&transparentBackground=1&dynamicHeight=1" loading="lazy" width="100%" height="2269" frameborder="0" marginheight="0" marginwidth="0" title="[통계] 지도학습 - 회귀"></iframe>
