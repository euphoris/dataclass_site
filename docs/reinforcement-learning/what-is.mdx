# 강화학습이란 무엇인가?

머신러닝을 여러가지 방식으로 구분할 수 있는데 일단 학습 방식에 따라서 구분을 할 수 있어요.

지도학습, 비지도학습, 강화학습 이렇게 구분하는데 그러면은 이 세가지가 각각 무엇이냐.

그래서 우리가 이제 강화학습이 뭔지를 이해를 하려면 다른 머신러닝이랑 강화학습이 뭐가 다르냐.

이것을 좀 이해를 해봐야겠죠.

## 지도학습

일단 지도학습부터 알아보도록 하겠습니다.

지도학습은 머신러닝의 가장 대표적인 형태인데 여기서 지도라는 것은 영어로 supervised, 관리, 감독한다

이런 뜻입니다.

사람이 답을 가르쳐주고 그 답을 그대로 약간 외우게 시키는 게 지도학습이에요.

그래서 여기 보시면 라벨이 있는 데이터로 학습을 한다

이렇게 되는데 우리가 이 데이터는 이런 거야, 이거는 이런 거야 라고 이제 라벨을 다 붙여줘요.

그러면 그 붙여준 거를 외우는 겁니다.

그래서 기본적으로 입력하고 출력이 있고 얘네 둘이 어떻게 관계가 있느냐.

그래서 라벨이라는 것은 이 출력을 말해요.

그래서 이런 식으로 학습을 하게 되고 그래서 대부분의 머신러닝이라고 하면 실제로는 지도학습을 말하게 됩니다.

90% 이상이다

이렇게 얘기하는데 90%도 아니고 거의 한 99%라고 해야 되지 않나 이렇게 생각이 되고요.

지도학습도 다시 종류가 나눠지는데 회기분류 이렇게 나누는데 회기라고 하면 뭔가 예측을 하는데 연속적인 거를 예측을 하는 거예요.

예를 들면 우리가 어떤 제품의 물건의 가격을 예측을 한다.

이런 물건이 있는데 이 물건은 얼마 하겠냐?

그러면 가격이라는 거는 연속적인 단위를 쓰는 거죠.

그러면 회기 문제가 되고요.

분류 문제는 어떤 종류를 예측하는 거예요.

이거냐 저거냐.

예를 들면 어떤 부품이 있는데 부품 사진을 찍어서 이거는 분량, 이거는 정상 이렇게 라벨을 붙여서 학습시키면 새로운 부품을 보여줬을 때 이건 뭐야

그러면 그거는 분량입니다.

또는 정상입니다.

몇 개 중에 하나로 판정을 하는 거죠.

그래서 지도학습을 가장 대표적인 머신러닝의 종류고 그래서 예를 들면 이메일 내용을 보여주고 이게 스팸이냐

아니냐.

이거 지도학습이죠.

그다음에 이미지를 보여주고 이게 특정 카테고리가 이것도 지도학습이고 환자 데이터를 보여주면 이 사람의 병에 걸렸냐

안 걸렸냐

이런 것도 지도학습이고 기업 정보를 보여주고 이 회사 잘 나갈 것 같아 안 나갈 것 같아

이것도 지도학습.

주택 정보를 보여주고 이 정도 집이면 어느 정도 가격이 되겠어

이런 것도 지도학습.

그다음에 개인의 금융생활 정보를 보여주고 이 사람 빚 갚을 것 같아

이렇게 하면 이것도 지도학습.

그다음에 우리 회사 같은 데 보면 이제 회사 들어갈 때 자기소개서 쓰죠.

그러면 이 사람을 뽑으면 자기소개서 쓴 걸로 보아하니 회사에 들어오면 어떤 평가를 받을 것 같아

이런 것도 지도학습입니다.

그래서 지도학습이 되려면 전부 이쪽에 Y에 해당되는 부분에 레이블이 있어야 돼요.

사람이 실제 데이터를 넣어줘야 돼요.

그래야 예측이 됩니다.

그래서 여러분들도 AI로 뭐 한다

이러면은 일단 제가 저 데이터가 있겠나 이 생각을 한번 해보세요.

그래서 그런 생각을 하셔야 되고 그래서 대부분 되게 어려운 것 중에 하나가 우리가 이제 약간 악순환인데 뭔가 AI를 도입을 하려면 데이터가 충분히 쌓여 있어야 됩니다.

근데 보통 우리가 AI를 하고 싶어 하는 부분들은 이때까지 사람들이 하던 일이거든요.

근데 사람이 하던 걸 AI를 시키려면 데이터가 있어야 되는데 사람이 할 때는 보통 데이터가 안 쌓여요.

왜냐하면 내가 눈대중으로 보고 내가 감으로 알고 이런 건데 그게 어떻게 데이터로 쌓여요.

그죠.

그러니까 데이터가 없고 데이터 없으니까 AI 머신너링 도입이 안 되고 머신너링 AI 도입이 안 되니까 또 사람이 하고 약간 약간 악순환에 있죠.

그래서 지도학습을 하시려면 사람이 하더라도 뭔가 데이터로 기록하는 과정을 거쳐가지고 데이터를 계속 축적을 해야 됩니다.

## 비지도학습

어쨌든 그렇고 지도학습은 정답을 주고 학습을 시키는 거라면 정답이 우리도 없는 경우가 있어요.

이런 거를 이제 비지도학습이라고 하는데 그러면은 정답이 없는데 어떻게 학습을 시키냐 하면은 뭔가 잠재변수를 가정을 합니다.

우리가 보이지는 않지만 이런 변수가 숨어 있을 거야 라고 생각을 하고 그 숨어 있는 변수의 특징을 어떤 식으로든 만듭니다.

그래서 그럼 우리 이제 가정을 하는 거죠.

이러이러한 가정 하에서 우리가 관찰되는 현상들이 나타난다.

이렇게 하면은 그 숨어 있는 변수를 간접적으로 측정을 할 수 있어요.

이게 좀 낯설게 들리지만 생각해보면 우리가 배우고 있는 대부분의 과학이 이런 식이거든요.

예를 들면 여러분 전자를 눈에 본 적은 없지만 우리 다 전자가 있을 거라고 믿잖아요.

왜냐하면 전자가 이러이러 할 거야 라고 가정을 하면 기가 막히게 전기 현상이 다 설명이 된단 말이에요.

그래서 우리는 본 적도 없지만 믿을 수 밖에 없는 거죠.

아니 그게 없으면 어떻게 이게 다 설명이 돼.

뭔가 존재할 거라고 가정을 하고 그 가정 하에 우리가 보이는 것이 다 설명된다

이렇게 생각하는 건 많이 하는 방식입니다.

그래서 이렇게 하는 거는 이제 비지도학습이에요.

우리가 가정한 거 내에서 답을 찾는 겁니다.

근데 우리가 그 가정이 맞는지 아닌지는 몰라요.

전자가 정말 존재할까요?

아무도 모릅니다.

예를 들면 우리가 이제 SF영화처럼 외계인들이 지구에 쳐들어왔는데 나중에 얘기를 해보니까 우리는 전자라는 걸 믿습니다.

외계인들이 역시 미개종족들. 그럴 수도 있잖아요.

그러니까 우리 과학사에서도 보면 그런 거 많이 있단 말이에요.

예를 들면 옛날 사람들은 에테르 이런 게 있어 가지고 빛이 에테르 에테르 라는 물질 공간에 있어 가지고 거기서 파장으로 되는 거야.

한때는 옛날에 그렇게 생각을 했다가 에테르 같은 건 없어요.

사람들이 다 충격받고 아인스탄 업적 중에 하나 그죠?

에테르는 없어.

그러니까 사람들이 에테르가 없었다니.

나 이때까지 에테르 있는 줄 알았는데.

그래서 뭐 이런 식으로 우리가 있었다고 생각했는데 아니 이건 없다고 생각하는 게 낫겠는 걸 에테르를 본 적은 아무도 없어요.

있다 없다 얘기는 있었는데 그게 보고 그런 게 아니라 그러면 에테르가 있다 치면 빛의 속도가 어떻게 달라져야 되는데 빛의 속도가 안 달라지네.

그럼 에테르는 없는 셈 치자

이런 식으로 한 거죠.

마찬가지로 우리가 이제 잠재변수라는 거는 있다 치고 해보면 이렇게 분석이 될 수 있는데 그게 뭐 있는지 없는지 모릅니다.

다른 사람은 저런 게 있다고 치고 분석을 하는데 그 사람 결론도 나름대로 말이 될 수도 있어요.

약간 누가 더 말이 되냐 싸움이지 있는지 없는지 끝까지 모르는 거죠.

그래서 뭐 예를 들면은 우리가 이제 성격 같은 거 얘기할 때 요즘에 뭐 유행하는 MBTI 이런 거 있잖아요.

MBTI 하면은 뭐 어떻게 합니까?

MBTI 잘 모르시는 분도 외향적, 내양적 이런 거는 아시죠?

우리도 많이 얘기하잖아요.

저 사람 참 외향적이야.

그 사람 성격이라는 거는 어떤 뇌의 특성인데 우리가 그 사람 두개골 열고 뇌를 받냐

이거죠.

뇌를 보면은 거기에 뭐 외향적 이렇게 써 있냐 말이죠.

그런 거 아니란 말이에요.

근데 우리가 이 사람은 아 외향적이야.

이 사람 참 내양적이야.

이렇게 얘기하는 거는 겉보기의 행동을 보고 아 저 사람 마음에는 어떤 외향적이라는 특성이 있을 거야.

이렇게 생각을 하는 거죠.

여러분들 저 보시면은 어 좀 약간 강사가 외향적이지 않을까

생각할 수도 있지만 저 굉장히 내양적이거든요.

그래서 이제 이렇게 강의를 하러 오면 연기를 합니다.

외향적인 연기를.

농담도 하고 막 재밌는 척 하지만 이렇게 강의 끝나면 어허허허 이러고 있거든요.

사회성을 다 써버렸어.

이러는데 그러니까 겉보기만으로는 모르는 거죠.

저 사람이 외향적인지 내양지 모르는데 우리가 가정을 하는 겁니다.

이럴 거야.

그래서 그러면 이제 성격 같은 경우도 뭐 mbti만 있는 게 아니라 여러 가지가 있단 말이에요.

뭐 심리학자들이 많이 쓰는 건 빅5라고 있습니다.

심리학자들은 mbti 별로 안 좋아하거든요.

왜냐하면 mbti에서 가정하는 게 뭐 틀렸냐 하면 그렇게 얘기할 수는 없어요.

왜냐하면 mbti에서 가정하는 게 뭐 s가 있고 n이 있고 이러는데 이게 눈에 보이는 게 아니기 때문에 맞았는지 틀렸는지는 잘 몰라요.

근데 이제 보통 mbti에서 가정하는 게 뭐 t가 있고 f가 있고 뭐 이렇잖아요.

p가 있고 j가 있고 이런 식인데 이런 식으로 4가지가 있다고 가정하면 사람들 행동이랑 좀 잘 안 맞아요.

그래서 이제 이름의 빅5인데 보통 심리학자들이 보기에 그래도 5가지는 있다고 쳐야 어느 정도 맞지.

4개는 좀 아닌 것 같다

이런 게 있고 또 하나는 이게 양극형이잖아요.

2면 아니면 i 이런 식인데 심리학자들이 보기에 아니 약간 정규봉포처럼 가운데가 많고 옆에가 적어야지 이거는 2가 많고 i가 많고 약간 중간이 없다

이런 느낌인데 그게 좀 약간 실제 사람들 데이터랑 안 맞는다.

사람들이 이렇게 양극단이 많을 리가 있겠냐 대체로 비슷하지

이렇게 생각한단 말이야.

이게 뭐 어떤 원천적으로 틀렸다기보다는 좀 사람들 데이터랑 잘 안 맞아요.

그래서 이제 약간 별로 안 좋아하는데 근데 또 그렇다고 해서 뭐 이거 뭐 100% 틀렸냐.

mbti 좋아하시는 분들은 mbti 잘 맞아요.

이런단 말이에요.

그러니까 결국에는 어떤 얘기냐면 50% 맞느냐 70% 맞느냐

이런 차이의 싸움이지 100% 맞거나 100% 틀리거나 이러진 않습니다.

이렇게 가정하면 그것도 일리는 있지만 뭔가 좀 안 맞고 이렇게 가정하면 좀 더 잘 맞지만 그래도 좀 미진한 부분이 있고 약간 이런 식이에요.

요즘에 이제 하나 더 있다 해서 6개 점점 늘어납니다.

왜냐하면 이제 자꾸 데이터를 보다 보면 하나 더 있어도 될 것 같은데 자꾸 이런 생각이 들어서 점점 늘어난다고 그래서 보통 그렇고 그래서 이제 우리가 뭐 비지도학습의 여러가지 예시를 보면 고객을 세분화 할 때 이제 우리가 유튜브 같은 거 여러분 해보시면 유튜브 보면 여러분 첫 화면에 유튜브 들어가면은 첫 화면에 한 6개 아니면 8개 이 정도 영상이 뜨거든요.

PC에서 들어가면 근데 그거를 여러분이 구독하시는 영상만 뜨는 게 아니라 가끔 되게 뜬금없는 영상이 뜬단 말이에요.

저 같은 경우에도 얼마 전에 보니까 무슨 교회에서 찬양하는 영상 뭐 이런게 뜬단 말이에요.

이게 왜 나한테 뜨지?

조회수 500짜리 영상이 막 뜨는데 이런 현상이 왜 일어나냐면 여러분들하고 취향이 비슷한 사람들을 유튜브에서 대충 그루핑을 해놓습니다.

이 사람들은 대충 취향이 비슷해 그러면은 어떻게 되냐면 나랑 비슷한 그룹에 있는 사람 누군가가 그 영상을 본거에요.

좋아요를 눌렀어요.

그러면은 너랑 너랑 같은 그룹이니까 얘가 좋아하면 너도 좋아하겠지 하고 여러분한테도 이제 보여주는 거에요.

그래서 자꾸 유튜버들이 구독 좋아요

알림설정 눌러주세요

이렇게 얘기하는데 그 이유 중에 하나는 여러분이 구독 좋아요

알림설정을 누르면 여러분한테만 영향을 미치는 게 아니라 그게 여러분이랑 같은 그룹에 있는 사람들한테도 영향을 미칩니다.

그러니까는 자꾸 여러분 좋아하시는 유튜버는 좋아요

자꾸 눌러주셔야 그 유튜버가 유명해지고 성공할 수 있어요.

그래서 열심히 해주시고 근데 이제 문제가 뭐냐면 고임들을 그렇게 그루핑을 할 때 이 사람하고 이 사람하고 취향이 비슷하다

이런 건 정답이 없거든요.

그렇죠?

그냥 정답이 없고 유튜브가 마음대로 묶어놓은 거에요.

왜 니 마음대로 묶어놓냐?

뭐 할 말은 없지만 근데 유튜브 입장에서 그걸 정당화 할 수 있는 수단은 뭐냐면 이 사람하고 이 사람하고 묶어놨더니 뭐 취향이 같다는 보장은 없는데 이 사람이 묶어놨더니 얘가 좋아요

누르는 거 얘도 좋아요 누르고 얘가 재밌어 하는 거는 끝까지 보는 영상 얘도 끝까지 보고 뭐 취향 같네

아닌데 취향 다른데 이렇게 해도 어쨌든 유튜브라는 그 사업이 잘 굴러가게 해준단 말이에요.

그런 한에서는 그런 가정이 뭐 사실인지 아닌지 모르죠.

취향이 같다

이런 거는 우리가 검증할 수 없는 영역이잖아요.

검증할 수는 없지만 어쨌든 작동하는 모델이라는 거죠.

그래서 이런 거를 이제 다 비지도학습이라고 부르게 됩니다.

그래서 이제 지도학습하고 비지도학습의 차이는 결국에 관찰된 어떤 Y가 있느냐.

관찰된 Y가 있어서 그거를 딱 하게 하면은 지도학습이고 관찰된 건 없는데 요렇다 치고 하면은 비지도학습 이렇게 됩니다.

## 강화학습

그래서 이건 넘어가고 그럼 이제 강화학습은 뭐냐 지도학습이 한 90% 차지하면 비지도학습이 한 9% 차지한다고 할 수 있고요.

강화학습은 그 중에서도 1%도 안 됩니다.

굉장히 적은 비중만 차지하는데 강화학습의 모델은 좀 더 복잡해요.

지도학습은 답을 다 가르쳐 주는 거고 비지도학습은 어떤 가정하에서 학습하게 하는 건데 강화학습은 그런 게 없습니다.

뭐만 있냐면 강화학습에는 행위자가 있어요.

행위자는 어떤 행동을 하는데 그 행위자는 주변을 자기를 둘러싸고 있는 환경이 있습니다.

예를 들면 강아지가 행위자다.

그럼 강아지를 둘러싼 환경에는 자기 주인이 있겠죠.

그래서 행위자는 일단 관찰을 해요.

내 주변 상황이 어떤가 관찰을 하고 그 관찰에 기반해서 어떤 행동을 합니다.

환경에 그 행동이 영향을 미치겠죠.

그러면 뭔가 환경이 변해요.

내 행동의 결과로 환경이 변하고 그 다음에 어떤 보상이 따라옵니다.

강아지가 주인이 막대기를 던졌는데 그걸 보고 막대기를 물어다가 주인한테 주면 주인이 잘했어

이러면서 먹이도 주고 칭찬도 해주고 보상을 얻고 주인이 기뻐하는 상황으로 바뀌는 거죠.

주인이 막대기를 던졌는데 이렇게 멀뚱이 보다가 주인 다리를 확 물어요.

그러면 주인이 열받으면서 이 자식 하면서 화를 내고 나는 괜히 얻어먹지도 못하고 이런 상황을 경험을 한단 말이에요.

그럼 강아지가 해야 될 건 뭡니까?

내가 어떤 보상을 받는데 이 보상을 장기간에 지금 한번 한번이 아니라 계속 이 보상을 쌓아 나가는데 이 보상들을 다 합친 거 이걸 수익이라고 하는데요.

이 수익을 최대화 시켜야 된단 말이에요.

어떻게 해야 내가 주인하고 좋은 관계 주인을 기뻐하고 결국 그러면 주인이 기뻐하면 나한테 잘해주고 이런 거잖아요.

그럼 장기간에 주인하고 어떤 좋은 관계를 쌓아 나갈 거냐

이런 문제인데 지도학습하고 다르죠.

지도학습하고 달리 답을 주지 않습니다.

주인은 그냥 칭찬과 보상 처벌 이런 걸 할 뿐이지 여러분들 강아지 키우시는 분들 강아지한테 막대기 던지면 공 던지면 물어오는 거 가르치면서 공 던진 다음에 여러분이 막 쫓아가서 공 물고 오는 거 보여주지 않잖아요.

그죠?

답을 알려주지 않습니다.

물어 와.

말로 하긴 하지만 뭐 걔가 말을 안 듣겠어요?

제일 약간 의미 없는 짓 중에 하나가 강아지한테 말하는 거거든요.

강아지는 사람의 언어를 못 알아들어요.

근데 약간 강아지 키우시는 분들 중에 되게 많은 분들이 강아지 뭐 내가 하지 말랬지 이렇게 하시는 분들이 있거든요.

어떻게 소용없는 짓입니다.

그런데 소용없는 줄 알면서도 하는 거죠.

약간 답답하니까.

근데 강아지는 이제 강아지가 알아들을 수 있는 방식으로 얘기를 해줘야죠.

잘하는 행동을 하면 보상을 주고 못하는 행동을 하면 처벌을 하고 이거를 잘 해야 되는데 보상이나 처벌을 안 줬고 자꾸 말로 내가 이거 하지 말랬지.

짓지 말랬지.

그러니까 걔가 안 짓냐고요.

못 알아들었는데.

어쨌든 보상이나 처벌을 줘야 되는 거죠.

그래서 그렇고 반대도 사실 마찬가지입니다.

그러면 주인은 강화학습을 안 받느냐?

주인도 강화학습을 받죠.

내가 이런 행동을 하면 강아지가 좋아하고 이러면 주인은 거기서 또 보상을 얻는 거죠.

서로 서로 약간 강화학습을 하는 거예요.

그런데 이제 그것도 마찬가지잖아요.

주인도 강아지가 예를 들면 내가 공을 던져요.

강아지가 물어 왔어요.

그러면 강아지한테 먹이를 줘야 되는데 그 강아지가 나한테 먹이 달라고 이렇게 끌고 가가지고 그 간식 봉지 앞에 가서 이거 나한테 줘.

그래야 내가 강화학습을 하지.

이러지 않는단 말이에요.

강아지가 우리한테 답을 보여주지 않습니다.

그런데 우리가 적절한 행동을 하면 강아지가 우리한테 보상을 주는 거죠.

재롱을 피운다든지 말을 잘 듣는다

이런 식으로 그러니까 보통 우리가 회사 같은 것도 마찬가지죠.

물론 회사가 어느 정도 답을 줄 때도 있지만 사실 우리가 눈치껏 알아서 잘하면 보상을 주고 아니면 눈치껏 눈치 좀 없이 행동하면 처벌을 주고 이렇게 하지만 이렇게 해라

저렇게 해라

딱 답을 알려주진 않거든요.

회사도 어떻게 보면 약간 강화학습입니다.

우리한테 보상이나 처벌을 줄 뿐이지 뭐 대략적인 방향은 주지만 디테일하게 이렇게 잘 알려주진 않는단 말이에요.

지도학습이 아닌 거죠.

그래서 이런 보상과 처벌에 의한 학습 방식이 강화학습입니다.

그래서 이제 강화학습이 좀 적용하기 어렵다

이렇게 아까 얘기를 드렸었는데 왜 그러냐면 일단 지도학습은 쉬워요.

그냥 우리가 원하는 답이 있으면 그거 가르쳐주면 되거든요.

강화학습은 우리가 보상이나 처벌을 줘야 되는데 이게 쉽지 않습니다.

예를 들면 우리가 인간관계에서도 어려워 결국 인간관계도 어떻게 보면 강화학습이거든요.

보상이나 처벌 예를 들면 나한테 되게 신경 거슬리게 굳은 사람한테는 처벌을 하고 내 마음에 들게 하는 사람한테는 보상을 주고 이렇게 해서 상대방이 내가 원하는 대로 행동을 하게 만들어야 되는데 다 경험상 아시지만 쉽지 않죠.

예를 들면 우리 연애 같은 거 젊은 분들 여기 많은데 연애 같은 거 하면 힘든 이유가 뭡니까?

내가 선물을 상대방한테 준다고 해서 상대방이 내 맘에 들게 행동하는 게 아니잖아요.

사실 그게 잘못된 행동일 수도 있거든요.

선물 주는 건 보통 보상인데 상대방이 자꾸 내가 하자는 대로 안 해요.

근데 내가 자꾸 선물을 줘.

그럼 어떻게 하는 겁니까?

내 말을 안 듣는 거에 보상을 주는 거죠.

그래서 대부분의 많은 사람들이 하는 실수가 그거거든요.

내 마음에 안 들게 할수록 내가 보상을 자꾸 줘요.

자꾸 선물을 준단 말이에요.

나한테 막 성질을 내면 내가 선물을 줘요.

미안해

이러면서 그러면 어떻게 돼요?

성질 낼 때마다 난 보상을 주니까 이 사람은 화내는 거에 강화학습이 되는 거죠.

화를 내면 처벌하고 잘해주면 보상을 해야 되는데 우리가 보통 반대로 많이 합니다.

이게 강화학습이 어렵다는 거예요.

보상과 처벌을 준다는 게 생각보다 굉장히 어려운 문제입니다.

그래서 애들도 마찬가지죠.

자녀 키우시는 분들은 애들이 땡깡 부리고 하면 부모가 보상을 주거든요.

왜 땡깡 부려?

이러면서 자꾸 뭐 하나씩 주고.

자꾸 땡깡 부리라고 하는 거예요.

결과적으로.

그래서 이게 되게 어렵습니다.

단기적으로는 입 맞고 넘어갈 수 있는데 장기적으로 보면 별로 안 좋은 겁니다.

잘할 때 보상을 주고 못할 때 처벌을 해야 되는데 생각보다 어렵다.

이런 얘기하면 애가 잘할 때가 있어야 말이죠.

그게 어려운 거예요.

잘하는 그 순간의 타이밍을 딱 찾아가지고 뭔가 애가 뭐 하나라도 잘하는 순간에 득달같이 보상을 줘야 되는데 우리가 타이밍 잡기가 힘들죠.

그러면 이제 잘하면 어이 잘하네.

아 평화롭고 집이 평화롭고 좋구나.

그 순간에 넘어간단 말이에요.

그러다가 애가 막 울고 때쓰고 이러면 그때가 나

왜 됐어

이러면서 이렇게 또 뭐 주고 약간 반대로 행동하는 거죠.

그래서 이제 강화학습의 예시를 보면은 우리가 이제 그 유명한 알파고 벌써 2016년이네요.

2016년이니까 8년 전인데 이제 바둑을 두는데 바둑도 이기고 지구가 있습니다.

어떻게 둬야 우리가 바둑을 이기는지는 몰라요.

알파고 팀에서 바둑을 제일 잘 두는 사람도 굉장히 하수였습니다.

자기들도 바둑을 어떻게 잘 두는 건지는 모르는데 어쨌든 이기고 지구는 규칙으로 결정이 나니까 이기면 보상, 지면 처벌 그러면 컴퓨터가 수천만 판을 두면서 어떻게 해야 이기는지를 스스로 찾는 거죠.

지도학습이 아닙니다.

지도학습이면 이렇게 돌아 저렇게 돌아 일일이 가르쳐 주는 건데 그렇게 가르쳐 주지 않고 난 모르겠고 하여간 니가 잘 둬 봐.

그럼 잘 두면 보상을 줄게.

약간 이런 거죠.

우리 회사 다니면 회사에서는 난 잘 모르겠고 하여간 실적을 내와.

그러면 내가 보너스를 주든지 할게.

뭐 어쩌라는 거야

이러면서 우리가 알아서 해야 되는 거죠.

그 다음에 우리 로봇 제어 같은 경우에도 로봇이 뭔가를 했으면 좋겠는데 우리도 어떻게 될지

잘 모르겠어요.

난 잘 모르겠고 하여간 저거 좀 잘 해봐.

그럼 잘하면 보상을 줄게.

못했어?

못하면 처벌이야.

이렇게 하면 알아서 잘 해와야 되는 거죠.

그리고 자율주행 이런 것도 마찬가지인데 난 잘 모르겠고 하여간 저기까지 가.

근데 가면 보상을 줄게.

이런 식이 됩니다.

그래서 지도학습하고 강화학습의 차이를 보면 지도학습은 X를 가지고 Y를 예측을 하는데 이 데이터가 다 있어야 돼요.

X와 Y에 대한 데이터가 있어야 되고 그래서 예측 오차를 최대한 줄이는 게 목표입니다.

그래서 이제 바주기다.

그러면 현재 상황에서 예를 들면 프로 기사들이 어떤 수를 두느냐.

이걸 데이터로 쌓아서 그러면 AI한테 지도학습을 시키는 거예요.

그럼 어떻게 되겠습니까.

이런 상황에서는 프로들은 여기 두더라.

이거를 학습을 하는 거죠.

그러면 얘가 둘 수 있는 최대한의 수는 프로처럼 두는 것 밖에 못합니다.

프로를 이기지는 못해요.

프로하고 똑같이 두는 거죠.

그 다음에 예를 들면 주식 투자를 지도학습을 시킨다.

그럼 이런 기업은 대충 주가가 이 정도 가더라.

그래서 주식 증권회사에 보면 애널리스트라는 사람들이 있어가지고 목표 주가를 제시하잖아요.

이 회사 주식은 이때까지 데이터로 볼 때 대충 비슷한 회사들이 이 정도 주가 가니까 이 회사도 이 정도 주가에 갈 거야.

맞을 때도 있고 틀릴 때도 있지만 대략 평균적으로 보면 맞는데 항상 맞는 건 아니죠.

오차를 줄이는 게 관건입니다.

강화학습은 달라요.

강화학습은 기본적으로 직접적으로 시행착오를 해야 됩니다.

그래야 이렇게도 해보고 저렇게도 해보면서 이쪽으로 가야 보상을 많이 받는지 저쪽으로 가야 보상을 많이 받는지 알 수가 있죠.

그래서 여기서는 예측을 잘하는 게 목적이 아니라 무적된 보상을 최대하는 게 목적입니다.

가능한 많은 보상을 받는 게 목적이고 이제 받으기다 그러면은 자기가 시행착오를 하는 거예요.

이렇게도 둬 보고 저렇게도 둬 보고 그래서 많은 보상을 받으면 이렇게 두는 거구나

이런 거죠.

투자의 경우에도 목표 주가를 맞추는 게 문제가 아니라 언제 사고 언제 팔고 가끔 보면 주식 잘하는 분 중에 아무것도 모르는 분이 있어요.

이 회사 주식 어떻게 생각해요?

잘 모르겠어.

그런데 팔아야 될 것 같아.

기가 막혀요.

딱 그 분 팔면 주가 막 떨어집니다.

아 이제는 살 타이밍 아닐까?

사면 많이 오를까요?

글쎄.

그런데 모르겠지만 지금 사면 좋을 것 같아요.

사면 또 기가 막히게 올라요.

그런 분들이 있거든요.

경제도 모르고 뉴스도 안 봐요.

어떻게 그냥 대충 감으로 사고 팔고 하는데 기가 막힌 타이밍이 있습니다.

근데 생각해 보면 우리가 쉽지 않겠다.

이런 생각을 할 수 있죠.

왜냐하면 지도학습은 우리가 답이 다 있는 걸 가르쳐 주는 거니까 얘가 학습을 하면 어떻게 될지가 우리가 예상이 됩니다.

아 이렇게 사고팔, 얘가 예를 들면 주식을 우리가 데이터를 학습을 해서 얘가 어느 정도 가격으로 예측을 하겠구나.

기존의 데이터가 있으니까 그 정도 예측을 할 수 있겠구나.

예상이 되는데 강화학습은 사실 예상이 안 돼요.

그러니까 바둑 같은 경우도 이 정도까지 실력이 올라갈 줄은 아무도 몰랐습니다.

2016년에 알파고 처음 나왔을 때 이세돌 구단이 알파고랑 두기 전에 인터뷰를 하면서 약간 이거 공짜로 돈 주신 참 감사합니다.

약간 이런 느낌으로 이기면 얼마 지면 얼마 이런 식으로 구글에서 조건을 걸었거든요.

약간 완전 공돈이네요.

감사합니다.

잘 먹겠습니다.

약간 이런 느낌으로 인터뷰를 했는데 이제 도보고 나서는 야 이건 대단하다

이런 얘기를 했는데 왜냐하면 1년 전에 알파고가 둔 거랑 이세돌 구단이 둘 때랑 실력 차이가 어마어마하게 많이 났습니다.

근데 이제 우리가 아무도 그거를 예측을 못 한 거죠.

그래서 앞에서 보면 이제 여러가지 얘기를 했는데 강화학습이 자율주행 이런 데서 의외로 잘 안 쓰는데 왜냐하면 예를 들면 여기서 여기까지 가는데 잘 가봐 잘 가면 보상 줄게

이러면은 막 가다가 할머니가 이렇게 길 걷는데 비켜

이러면서 갈 수도 있단 말이죠.

가면 보상 준다고 했죠.

할머니를 치지 말라고 한 적은 없거든요.

그러면은 아 안 되겠다 할머니 치면 처벌 할 거야

그러면 어떻게 할까요.

할머니가 지나가면 세월하더라도 아유 지나가시죠

어르신 천천히 가세요

옆으로 그냥 피해 가면 될 건데 피해 가세요

할머니 칠 수는 없죠.

이러면서 이것도 문제란 말이에요.

그러면 아니 할머니 그럼 우리가 이걸 세세하게 보상을 다 주다 보면은 에이 때려쳐 지도학습 하자 이렇게 될 수가 있습니다.

그러니까 이런 부분들이 강화학습에 어려운 점이다.

그래서 이제 뭐 제가 좀 초치는 얘기를 많이 했지만 그래도 이제 관심은 계속 커지고 있습니다.

왜냐하면 지도학습은 어느 정도 예상도 가능하지만 바꿔 말하면은 우리가 예상하는 만큼 밖에 못 한단 말이에요.

뭐 바둑을 두든 뭘 하든 우리가 상상할 수 있는 수준까지 밖에 못 하는데 강화학습은 잘만 하면 우리 상상 이상의 무언가를 할 수도 있습니다.

그래서 이제 얘를 잘 어떻게 길들여 가지고 우리 예상을 뛰어넘는, 우리를 놀라게 하는 이런 거를 해보자

이런 것들이 이제 자꾸 그래서 이제 관심이 높아지는 거죠.

그래서 강화학습에 대해서 지금 이게 강화학습 관련된 페이퍼들이 연도별로 얼마나 나오냐

이건데 엄청나게 증가를 하고 있는 걸 볼 수 있고요.

벌써 2016년 데이터인데 그때 벌써 1년에 만오천 편씩 논문이 나오고 있었어요.

그래서 이제 이 그림은 그 얀 르쿤이라고 해서 그 딥러닝 하면 3대 대가를 보통 꼽습니다.

제프리 힌턴, 그 다음에 얀 르쿤, 그 다음에 요슈아 벤지옥 이 사람을 뽑는데 뭐 다 이제 옛날 분들이에요.

나이 많으신 분들인데 르쿤, 그 다음에 벤지옥 이 세 사람이 제일 유명한데 옛날부터 딥러닝을 굉장히 오랫동안 파운 분들입니다.

사실 딥러닝이 80년대에 반짝 붐을 일으켰다가 90년대랑 2000년대 초반에는 좀 인기가 없었거든요.

근데 그 인기가 없던 시절에도 이제 배를 졸졸 굶으면서 열심히 이거 붙잡고 있다가 최근 와서 공로를 인정받아서 역시 당신들이 옳았어

이런 얘기를 했는데 옛날엔 욕도 많이 먹었어요.

가능성 없는 기술을 저렇게 붙잡고서 말도 안 되는 소리 한다 욕도 많이 먹었는데 역시 버티는 사람이 이기는 거죠.

그래서 이제 이 사람들이 다 80, 90년대부터 딥러닝 붙잡고 지금까지 있었던 분인데 르쿤이란 분이 2019년에 어디 학회에서 가져온 그림입니다.

그래서 우리가 어떤 케이크가 있으면 이 케이크의 윗바닥은 비지도학습이다.

윗바닥은 비지도학습이다.

왜냐하면 비지도학습은 기본적으로 아까도 얘기 드렸지만 어떤 데이터가 있으면 우리가 어떤 정답은 없어도 어떤 가정만 주면 학습이 될 수 있기 때문에 제일 쉬운 거다.

근데 지도학습은 거기에 발라져 있는 크림 같은 거다.

왜냐하면 지도학습은 우리가 정답을 주거든요.

그러니까 데이터가 상대적으로 한정될 수 밖에 없죠.

답이 없는 데이터는 많이 있지만 답이 있는 데이터는 조금밖에 없단 말이에요.

그래서 훨씬 더 한정되고 강화학습은 이 위에 올라가는 차례 같은 거다.

왜냐하면 강화학습은 결국에는 뭐 어쩌고저쩌고 지지고 복구 어쩌고 하면 보상 하나 처벌 하나.

예를 들면 내가 바둑 한 판을 두면 막 2시간 동안 바둑 떴더니 이겼음

이미 이겼음

끝.

이거란 말이에요.

그러니까 더 데이터가 한정될 수 밖에 없는 거죠.

그래서 이 사람 얘기는 강화학습 요즘에 약간 유행인데 여러분 비지도학습을 먼저 튼튼히 하시고 그 다음에 지도학습하고 강화학습은 올라가는 케이크 올라가는 체리 같은 거지 이거를 너무 그렇게 기대하지 마세요.

약간 이런 식의 얘기인 거죠.

이분은 약간 그 초치는 얘기 되게 좋아하는데 요즘에도 이제 LLM 챗GPT 이런 거에 자꾸 초치거든요.

그거 뭐 그게 생각만큼 잘 안 될 걸요.

그래서 항상 약간 부정적인 얘기를 해가지고 사람들한테 약간 그 희망을 좋아하는 사람들한테 자꾸 초친다.

아 영감님도 초치시네 약간 이런 얘기를 듣는데 어쨌든 그래서 강화학습이 굉장히 어떻게 보면 우리가 잘 되면 좋은 거지만 그만큼 잘하기 어렵다.

이런 얘기고요.

### 인접 분야

그 다음에 강화학습이랑 인접된 분야들이 있습니다.

그래서 그 우리 아까 이제 설문에도 많이 나왔는데 최적 제어 분야가 강화학습하고 많은 부분이 겹치는 부분이고 그 다음에 이제 산업공학의 운영과학 또는 보통 영어로 많이 하는데 operation research라고 있어요.

OR 이라는 분야가 있는데 최적 제어, 운영과학, 강화학습 이 세 분야가 조금씩 겹칩니다.

보통 최적 제어 이런 거는 크게 보면 기계공학이라든지 조선공학 이런 전통적인 엔지니어링 쪽에서 많이 하는 거죠.

그래서 최적 제어에서 주로 하는 거는 뭔가 복잡하지만 그래도 어떻게 돌아가는지 알려져 있어요.

물리법칙을 따른다든지 이런 거는 우리가 다 아는데 그 안에서 최적회를 찾는 이런 것이 최적 제어고 보통 산공과에서 하는 OR, 운영과학 이런 거는 뭔가 잘 모르는 부분이 있어요.

불확실한 부분이 있습니다.

예를 들면 우리가 어떤 물류 계획을 짜는데 물류라는 게 예상대로 안 되는 게 있다면 길이 어디 막힐지

이런 거는 좀 확률적인 부분이 있습니다.

그런 상황에서 가장 우리가 물류 계획을 잘 짜려면 최악의 경우 최선의 경우 이렇게 고려를 했을 때 어떻게 짜야 되느냐

이런 것들이 OR라고 할 수 있겠고요.

그래서 여기에는 둘 다 보상 이런 개념은 없죠.

그냥 우리가 만족해야 되는 어떤 조건들이 있을 뿐이지 보상 이런 건 없는데 강화학습에서는 그런 보상이 있는 상황에서 그 보상을 최대하는 이런 문제를 다루게 되고 그래서 이 세 분야가 서로 겹치는 부분들도 있고 좀 겹치지 않은 부분들도 있습니다.

그래서 아까 설문에서 이런 거에 강화학습을 적용해보면 좋겠다

이렇게 얘기하신 경우가 있는데 실제로 이제 아마도 겹치기 때문에 그렇게 생각을 하시겠지만 또 반대로 생각하면 꼭 강화학습으로 안 푸는 게 맞는 문제일 수도 있어요.

내 문제가 그냥 전통적인 OR 또는 전통적인 최적 제어가 더 잘 맞는 분야일 수도 있습니다.

예를 들면 이제 아까 강화학습 적용 가능한 분야 중에 로봇 제어 얘기를 드렸는데 로봇 제어도 강화학습을 하기가 되게 어려운 부분이 있어요.

이거를 최적 제어의 관점에서 로봇 제어를 하면 어떻게 하냐면 우리가 로봇 8이 움직일 때 어떤 식으로 예를 들면 여기에 모터를 작동시키면 로봇 8이 어떻게 움직일 때 이런 거는 기본적으로 아는 상태에서 요점에서 요점까지 가려면 그럼 어떻게 모터들을 서로 어떤 속도와 어떤 타이밍 어떻게 해야 되냐

이걸 계산하는 그런 문제로 접근하면 전통적인 엔지니어링에서 많이 접근하는 방식인데 강화학습에서 접근하는 방식은 난 모르겠고 모터 그게 뭐야

하여간 여기 집어 난 몰라 집으면 보상 줄게

이렇게 하는 게 강화학습이에요.

그러면은 잘 되면 우리가 상상도 못 했던 어떤 궤적과 경로로 물건을 집을 수도 있는데 반대로 말하면 우리가 상상도 못한 궤적과 경로로 이상한 짓을 할 수도 있습니다.

그러니까는 약간 우리가 원하는 어떤 뜻대로 제어하는 게 아니에요

강화학습은.

잘 해봐

이런 거에요

그러니까 약간 복불복이거든요

그래서 잘 될 수도 있고 안 될 수도 있다

그런 무책임한 게 어딨냐

그게 강화학습입니다.

그러니까는 제가 아까 얘기 드렸듯이 꼭 이걸 적용을 해야 될까

우리가 너무 잘 알면 그 문제를 너무 잘 알면 굳이 할 필요는 없어요.

우리가 답을 아는데 뭐하러 잘 해봐

이러겠습니까.

다 가르쳐주면 되지

그죠.

그러니까 강화학습을 좀 적용하는 거는 우리도 사실 잘 모르겠는데 하여간 좀 잘했으면 좋겠다

이런 바람이 있을 때 하시는 게 맞고 잘 아시면은 약간 최적재 전통적인 엔지니어링에서 많이 하는 방식이 더 나을 수도 있다.

물론 이제 이게 완전히 나눠지는 거 아니고 좀 섞을 수도 있어요.

전통적인 엔지니어링의 방식으로 하는데 어떤 미세한 부분에서 여기는 좀 잘 모르겠으니까는 강화학습을 좀 섞어서 쓰자던가 이렇게 할 수도 있겠죠.

### 짧은 역사

그래서 이 강화학습을 짤막하게 역사를 살펴보면은 기본적으로 아까도 얘기 드렸지만 이게 이제 기본적으로 심리학에서 나온 겁니다.

그래서 심리학에서 동물한테 어떤 보상을 줘요.

그래서 처벌 보상과 처벌을 주는데 그런 식의 학습을 강화학습이라고 합니다.

옛날 심리학자들은 지금 심리학자들하고 좀 다른데 20세기 초반에는 심리학의 행동주의라는 게 있었어요.

그래서 이 행동주의 심리학자들은 되게 특이한 얘기를 하는데 뭐냐 하면 심리학은 마음을 연구하는 게 아니다

이런 얘기를 해요.

이 사람들 주장은 뭐냐면 우리가 볼 수 있는 것은 어차피 겉보기에 행동밖에 없는데 그러면 행동을 보고 알 수 있는 것은 우리가 굳이 마음이라는 존재를 가정할 필요가 없는 거고 행동을 봐도 모르는 건 어차피 모르는 건데 그게 뭐 과학이냐.

그러니까 심리학은 마음을 연구하는 게 아니라 행동을 연구하는 거다

이런 얘기를 하거든요.

그래서 이 사람들은 요즘의 심리학자들은 설문도 하고 이러는데 이 행동주의 시대의 심리학자들은 설문 이런 거 안 합니다.

뭐 하느냐?

맨날 전기 충격을 줘요.

그래서 여러분이 얼마나 배가 고프냐

그거는 여러분한테 설문지로 얼마나 배가 고프십니까?

매우 배가 고프다

조금 배가 고프다

이렇게 물어보지 않아요.

그게 뭐야

행동주의 심리학자들은 전기 충격을 줍니다.

여러분들한테 음식을 주고 음식 바구니에 솔대면 전기 충격이 와요.

그러면 몇 볼트일 때 그 음식 바구니를 여느냐 내가 10볼트일 때도 연다

그러면 별로 배가 안 고프구나.

100볼트인데도 그걸 감수하면서 연다.

배가 많이 고프시군요.

이렇게 하는 거예요.

되게 보면 되게 엄밀하긴 하네.

약간 과학적이긴 해요.

그죠?

약간 미친 거 아닌가

이런 생각이 드는데.

그 사람들 맨날 보면은 논문 보면 전기 충격을 줬는데 그럼에도 불구하고 뭘 하더라.

약간 이런 거 하고 있거든요.

동물한테도 전기 충격 주고 사람한테도 전기 충격 주고 약간 좀 정신 나간 사람들입니다.

그래서 이 사람들은 이런 걸 강화학습이라고 해요.

그래서 이 사람들 연구가 어디에 많이 반영되어 있냐면 미군이 이걸 받아들입니다.

이거 엄청 과학적이네.

미군이 굉장히 좋아하면서 이걸 많이 받아들여요.

그래서 미군이 매뉴얼에 그걸 많이 반영을 하는데 우리 한국군도 그걸 보고 아, 이거가 이게 최첨단 군사 FM입니까?

이러면서 우리도 많이 받아들여요.

그래서 군대에 갔다 오신 분들은 일단 군대에 가면 어떻습니까?

뭘 못하게 자꾸 하잖아요.

왜 못하게 하느냐.

이것도 학습심리학에서 연구를 해보니까 나온 거예요.

우리가 보통 보면은 어떻게 하냐면 우리가 보통 생각하는 거는 뭔가

잘하면 가만히 있으면 아무것도 안 주고, 가만히 있으면 아무것도 안 주고, 잘하면 뭔가 더 주는 거.

이게 우리가 보통 사회에서 하는 방식이잖아요.

근데 군대는 어떻게 했습니까?

가만히 있으면 일단 사람을 괴롭혀요.

가만히 있으면 들들돌 보다가 잘하면 약간 본전입니다.

군대에서 잘하면 뭘 줘요?

휴가 주잖아요.

군대 안 갔던 분들, 군인들이 휴가 좋아하는 건 아시죠?

가만히 생각해보면 휴가 가면 본전이란 말이에요.

나는 원래 사회에 있던 사람이에요.

사회 내보내면 그냥 원래 본전이에요.

나 원래 본전 상태로 돌아오는 거잖아요.

일단 사람을 마이너스로 만들어 놓고 잘하면 플러스를 주는 게 아니라 0으로 되돌린단 말이에요.

근데 이게 학습심리학의 연구 결과입니다.

사람을 0에 있다가 플러스를 주는 거는 학습 효과가 약하다.

근데 일단 마이너스로 내려놓은 다음에 잘하면 제로로 만들면 학습 효과가 짱이야.

그래서 군대가 통제가 되는 이유가 그런 건데 그래서 이런 거를 미군에서 아이고 좋은데.

굉장히 생각해보면 비인간적인 과학이죠.

정말 과학적이기만 하고 비인간적이죠.

그래서 이제 문제는 제대하고 나서 군대가 참 좋았어

이런 사람 별로 없잖아요.

다 싫어한단 말이에요.

그러니까 오로지 약간 과학적으로 통제, 제어 이런 것만 하는 거고 마음은 생각 안 하니까 뭐 마음이 뭔 상관 있어.

제어만 잘 되면 돼.

시키는 대로 하기만 하면 되지.

이 사람이 속으로 무슨 생각하는 건지 따지지 않는, 오로지 보상과 처벌로만 작동하는 굉장히 냉혹한 과학입니다.

그래서 요즘에 잘 안 해요.

왜냐하면 단기적으로는 먹히는데 5년, 10년 가냐

이거죠.

장기적으로는 이게 유지가 안 되는 시스템이다.

징짓병들 한 1년, 2년 이것도 이거나 통하지.

안 된다

이런 건데.

어쨌든 이게 사실으로서 틀렸다

이런 거는 아니에요.

그래서 지금도 동물 실험 이런 거 할 때는 이렇게 합니다.

예를 들면 동물 연구하는 사람들은 이게 들키면 안 되는 굉장히 어두운 비밀 같은 건데 예를 들면 침팬지한테 어떤 걸 시킨다

그러면 어떻게 하냐면 전기 충격은 안 주고 그래도 옛날보다는 좀 인간적입니다.

전기 충격은 안 주고 먹이를 건조한 먹이만 줍니다.

한 일주일 동안 물을 안 주고 음식도 다 마른 먹이만 줘요.

그럼 얘가 엄청 목이 마를 거 아니에요.

일단 마이너스 상태로 만들어놓는 거예요.

그리고 뭔가 시키는 걸 잘하면 물을 한 방울씩 줍니다.

그러면 얘가 어떻게 써요?

엄청 시키는 대로 잘하거든요.

그래서 동물 연구하는 과학자들이 이런 방법을 쓰는데 이거를 대중이 알면 과학자들을 보는 시선이 굉장히 나쁘겠죠.

저런 쓰레기들 약간 이렇게.

그런데 지금도 어쨌든 동물 연구하거나 이럴 때는 잘하면 플러스를 주는 게 아니라 일단 마이너스로 만들어 놓고 잘하면 제로로 만들어 놓습니다.

그래서 이제 이런 연구들이 쌓이게 되고 그다음에 인공지능 분야가 발전을 하게 되겠죠.

인공지능 분야가 발전을 하면서 처음에는 지도학습하고 강화학습을 인공지능 연구자들이 구별을 못 했어요.

내가 답을 주는 것과 보상을 주는 거의 차이를 구별을 못 하다가 AI가 1950년대에 처음 생겨나는데 이 학습 심리학도 대략 1950년 60년대.

이때가 이제 피크거든요.

두 개가 시기가 비슷해요.

그래서 인공지능 연구자들이 심리학 쪽을 보니까 어 야 이거 그럴싸한데 이렇게 생각하면서 AI도 이런 식으로 만들 수 있지 않을까?

이러면서 강화학습이 지도학습에서 슬슬 분리되서 나오기 시작합니다.

그다음에 공학에서 최적재해와 관련된 분야가 있는데 이것도 비슷한 시기에 발전을 하거든요.

그래서 이 세 가지가 이제 융합을 합니다.

그래서 실제로 구현이 되기 시작하는 것은 1990년대 이때부터 뭔가

인공신경망이랑 결합되면서 구현이 되게 되고 성과가 나오기 시작한 거는 2010년대 이때부터 나오게 됩니다.

그리고 현재는 채취 비트라든가 이런 데에도 쓰이고 있어요.

그래서 제일 먼저 두각을 나타낸 것은 한 2015,14 이때쯤 네이처에 실리는데 컴퓨터 게임을 인간보다 더 잘한다.

그래서 여기 벽돌깨기 게임이 있는데 강화학습을 시키면 어떻게 하냐면, 처음에는 어떻게 하는지 모르니까 마구잡이로 패드를 흔들어요.

공이 오면 받아 쳐야 되는데 못 받아치고 공을 자꾸 놓칩니다.

놓칠 때마다 처벌을 받다가 우연히 한번 공이 여기 부딪힐 때가 있어요.

그러면 보상을 받거든요.

그러면 아 일단 저걸 받아 쳐야 되는구나.

받아치기 시작하다가 벽돌이 계속 깨지겠죠.

그러다가 이제 여기 영상 링크가 있는데 나중에 한번 보시면 어떻게 하냐면 자기가 오로지 시행착오만으로 옆으로 구멍을 파서 뒤로 공을 보내는 전략을 찾아납니다.

그래서 영상 보시면 마법이 일어났다

이런 말이 나오거든요.

아무도 그걸 가르쳐 준 적이 없어요.

벽돌 깨기를 하는데 옆에다 구멍을 판 다음에 공을 뒤로 보내면 여기서 막 공이 왔다 갔다 하면서 점수가 저절로 올라간다

이런 거 가르쳐 준 적이 없는데 오로지 시행착오만으로 이걸 찾아납니다.

그 다음에 이제 뭐 우리가 여러가지로 활용을 하는데 예를 들면은 그 에너지 분야에서 구글 같은 경우에 데이터 센터가 굉장히 많아서 그 데이터 센터 냉각이 굉장히 중요한 문제인데 그럼 에어컨을 적절한 타이밍에 껐다 켰다는 게 되게 중요하거든요.

여러분 우리 에어컨 켜보시면 알지만 너무 빨리 켜면 너무 춥고 또 너무 뜨기 크면 너무 덥고 근데 이게 에어컨 켠다고 바로 온도가 뚝 떨어지는 게 아니기 때문에 미리 좀 예측을 해서 에어컨을 켜고 끄고 해야 됩니다.

근데 이것도 이제 아까 주식하고 비슷한 거죠.

주식도 사고팔고 하듯이 에어컨도 껐다 켰다 해야 되는데 타이밍을 정확한 타이밍에 껐다 켰다 해야 되는데 이걸 강화학습을 할 수 있겠죠.

그래서 지금 보시면 이거는 기존 방식으로 냉각을 제어한 거고 강화학습을 온 하는 순간 에너지 소모량이 확 줄어들어요.

훨씬 더 효율적으로 냉방을 껐다 켰다 조절을 할 수 있는 거죠.

그 다음에 이제 로봇이라든가 협동 여러 개의 복수의 행위자들을 협동시키는 거 이런 것도 하게 되고 그 다음에 이제 이거는 구글에서 나온 논문인데 반도체 설계도 강화학습을 해보니까 바둑이랑 비슷한 거죠.

바둑도 내가 여기다 두면 바둑을 이기고 저기다 두면 바둑을 지는데 그럼 반도체를 이렇게 설계하면 효율이 안 나오고 저렇게 설계하면 효율이 잘 나오고 이거를 아주 많은 시행착오를 거쳐 가지고 반도체 설계를 수천만 번 하게 하면 우리가 더 인간보다 더 효율적인 설계를 하게 만들 수 있다.

근데 이제 사실 문제는 이런 거예요.

일단 두 가지 문제가 있는데 첫 번째는 수천만 번 시행착오를 해봐야 됩니다.

우리가 답을 알려주는 게 아니기 때문에 얘가 자기가 이렇게도 해보고 저렇게도 해보고 하니까 계산량이 굉장히 많이 들어가겠죠.

두 번째는 이 반도체를 설계를 했으면 효율을 측정을 해야 되는데 이거를 실제로 만들어 볼 수는 없잖아요.

그래서 이게 강화학습이 되려면 반도체 설계가 됐을 때 이 설계가 끝난 반도체의 효율성을 계산할 수 있는 무언가가 있어야 됩니다.

이게 돼야 보상을 우리가 원활하게 줄 수 있겠죠.

안 그러면 일일이 만들어 봐야 되니까 수천만 번 만들어 봐야 되는데 그걸 물리적으로 만들 시간은 없잖아요.

그런 문제가 있습니다.

그래서 로봇 제어 같은 경우도 강화학습을 적용하기 좀 어려운 문제가 뭐냐면 물리적인 로봇을 가지고 강화학습을 하려면 시간이 너무 많이 걸려요.

수천만 번 해봐야 되는데 언제 합니까?

그러다가 모터가 더 먼저 뻗어버리겠죠.

그래서 어떻게 하냐면 보통 로봇 시뮬레이터를 만들어 가지고 가상의 환경에서 실제하고 물리적으로나 환경적으로 거의 똑같은 시뮬레이터를 만들어서 그 안에서 수천만 번 시행을 시키게 됩니다.

그래서 강화학습을 하시려면 강화학습을 할 게 아니고 시뮬레이터를 먼저 만들어야 되는 경우가 대부분이에요.

그게 더 어려워요.

사실 강화학습은 별로 안 어렵거든요.

시뮬레이터 만드는 게 훨씬 어렵습니다.

그래서 이런 반도체 설계도, 간단한 반도체는 우리가 이게 효율성이 얼마나 하는지 아는데 복잡한 반도체는 직접 하드웨어를 만들어 가지고 테스트를 안 해보면 그 효율성을 계산해보기 힘들단 말이에요.

그래서 이런 문제가 있죠.

그래서 설계 이런 쪽은 아마 저도 잘은 모르지만 기계 쪽 설계 이런 분들도 아마 수치 해석해서 계산한 거랑 실제 실물이랑 완전히 똑같진 않을 거 아니에요.

그쵸?

똑같나요?

잘 모르겠지만 아마 똑같진 않을 겁니다.

그래서 직접 결국 만들어봐야 정확한 수치를 얻을 수 있는 경우가 있을 텐데 그런 경우가 되면 강화학습이 조금 어렵다.

그러니까 우리가 수치적으로 어쨌든 결과물이 있을 때 수치적으로 성능이 어느 정도 될지가 그래도 최소한 비슷하게라도 나와야 우리가 적절하게 강화학습을 할 수 있어요.

그 다음에 이제 또 행융합 이런 쪽에서도 행융합에서도 이게 우리가 행융합이 일어나면 얘가 온도가 너무 뜨겁기 때문에 1억 도씩 되거든요.

이게 어디 벽에 닿으면 안 돼요.

그래서 공중에다가 이렇게 자기장을 이용해서 둥둥 뛰어야 되는데 이게 굉장히 어려운 기술이란 말이에요.

그래서 보면은 우리나라가 이게 지금 세계에서 제일 기록이 높다 이런데, 끼케 한 5분 공중에 뛰어놓으면 그게 이제 꿈의 단계인데 아직 5분은 아니고 몇 분이더라도 몇 분 뛰어놓으면은 와 세계 기록 달성했어

막 이러면서 막 짝짝짝 이러는데 그래서 언제 됩니까?

한 2050년쯤 됩니다

이러고 있단 말이에요.

지금 5분 돌파하는 게 너무 힘들어가지고 2050년 이러고 있는데 지금 2024년이니까 그 얘기인즉슨 행융합 연구하시는 분들은 이제 다 은퇴할 때까지는 그냥 뭐 돈 달라 이런 거죠.

성과는 안 나오겠지만 50년쯤 나올 거니까 그때까지 돈 달라는 건데 그래서 이제 강화학습으로 해가지고 어느 정도 된다.

근데 이것도 이제 사실 문제가 시뮬레이션 상으로는 되는데 실제로 했을 때 그게 잘 되느냐.

이건 또 약간 별개의 문제입니다.

그 다음에 이제 우리가 코로나19 이런 거 요즘에 또 다시 도는데 테스트를 하면은 누구를 검사를 할 거냐.

누구를 찍어서 검사를 해야 잘 될 거냐.

이런 것도 강화학습을 적용할 수 있는 문제가 있고요.

문제가 되고요.

그러니까 아까 주식이나 냉방이랑 마찬가지죠.

냉방도 산다

아 냉방도 켠다 끈다.

주식도 산다 판다.

코로나19도 검사한다 안 한다.

그래서 보상은 뭡니까?

내가 검사를 해가지고 코로나 검사하면 빨리 찾아내면 잘하는 거고 검사를 안 했는데 놓치면 그거는 처벌할 대상이고.

이런 식으로 해서 누구를 검사해야 될지 모르겠고 네가 알아서 하는데 놓치면 너 나한테 죽는다.

이런 식으로 해가지고 AI가 알아서 잘 하면은 이런 것도 이제 강화학습이 됩니다.

### ChatGPT

그래서 강화학습이 최근에 쓰인 대표적인 사례로 Chat GPT를 예비를 할 수가 있는데요

Chat GPT는 이 그림 자체가 OpenAI에서 나온 그림입니다만 나누면 3단계, 실제로는 0단계가 하나 더 있습니다

Stack Zero는 어떻게 했냐면 아까 사실 얘기를 드리긴 했는데 인터넷에 있는 오만 때만 텍스트를 다 모아서 라지 랭디지 모델이라는 데에 학습을 시켜야 합니다

얘는 뭘 하는 거냐면 텍스트의 어떤 패턴을 예측을 합니다

이런 패턴으로 나오면 이런 답변이 나오더라 라는 것을 예측을 하고요

근데 얘를 그대로 쓸 수는 없어요

왜냐하면 여러분들이 친구한테 이거 어떻게 해?

이거 어떻게 하는 거야?

라고 물어보면 친구가 뭐라고 대답하고 있으니까 아 그래? 이거 내가 알려줄게

이럴까요?

아니죠 알아서 뭐하게? 왜?

이러잖아요

이게 자연스러운 인간의 답변이면 사실 좀 문제가 있습니다

특히 우리가 대부분의 데이터가 인터넷에서 모으는데 인터넷 같은 데 댓글 이런 데 보면 엉뚱한 소리 하고 헛소리 하고 싸우고 트집 잡고 유튜브 이런 데 댓글 보면 진짜 웃기거든요 별것도 아닌 것 가지고 싸우고 있어요

누가 이렇게 영상 올렸는데 너무 좋아요

저게 뭐가 좋냐?

자기들끼리 댓글에 따르면 싸운단 말이에요

그래서 그런 거를 학습을 하면 싸우고 시비 걸고 고툴이 잡고 이상한 소리 하고 이런 거를 배우겠죠

실제로 연구에 따르면 이런 대형 언어모델들이 굉장히 이상한 걸 많이 학습을 해서 예를 들면 여러분 지구가 둥글다고 인터넷에 글 써보신 적 있나요?

지구가 둥글다고 믿는 사람들은 인터넷에 글을 안 쓰거든요

지구가 평평하다고 믿는 분들은 인터넷에 글을 열심히 씁니다

음모론을 믿는 분들이 인터넷에 주로 글을 열심히 써요

안 믿는 사람들은 반대의 의견을 굳이 인터넷에 열심히 쓰지 않습니다

그래서 문제가 뭐냐면 텍스트만 놓고 보면 음모론이 비음모론 보다 훨씬 많아요

그래서 이 LLM이 그런 거를 되게 많이 학습을 합니다

잘못된 상식 이런 거 예를 들면 뭐 심장마비가 올 것 같으면 기침을 해라

이런 거 있거든요

잘못된 얘기인데 의사들은 굳이 그런 얘기를 하지 않겠죠

그런 거 안 통하니까 병원이나 빨리 오세요 라고 굳이 글을 쓰지 않아요

근데 그걸 믿는 분들은 심장마비가 올 것 같은데 기침을 세게 하면 심장마비를 막을 수 있대요

이러면서 막 그런 거 음모론까지는 아니지만 그 잘못된 상식을 좋아하면서 인터넷에 올리시는 분들이 있어요

그런 건 인터넷에 훨씬 올라온단 말이에요

그래서 그런 걸 제일 배우기 때문에 얘를 그대로 쓰면 문제가 많이 있습니다

이상한 소리하고 음모론 얘기하고 낭욕하고 이런단 말이에요

그래서 0단계에 학습시킨 걸 바로 쓰진 못하고 1단계로 학습을 시키는데 어떻게 하냐면 예를 들어 미세조정이라는 거예요

Fine Tuning이라는 걸 하는데 어떻게 하냐면 일단 사람이 모범 다방에 서서 보여줍니다

그래서 이거를 지도학습을 해요

Supervised라고 나오죠

그래서 예를 들면 6살짜리 아이한테 강화학습에 대해서 설명을 해주세요

그럼 제가 지난 시간에 1시간 동안 열심히 설명했던 거를 어린아이로 이해할 수 있게 사람들이 예문을 써서 그거를 줍니다

그래서 이 LLM을 약간 더 조정을 해가지고 이런 식의 답변을 예측할 수 있도록 산출할 수 있도록 추가적으로 학습을 시켜요

1단계 근데 이렇게 하면 답변을 잘 하느냐?

잘 합니다

이렇게만 해도 잘 하는데 아직 완벽하지는 않아요

왜냐하면 이 사람이 쓴 다반이 완벽한 다반이 아니기 때문에 바둑이랑 똑같습니다

바둑도 프로들이 잘 두긴 하지만 완벽한 수를 두느냐?

그렇진 않습니다

그래서 이 예문이 완벽하지 않기 때문에 어떻게 하냐면 2단계에서는 1단계에서 학습시킨 걸 가지고 어떤 지시를 준 다음에 답변을 여러 번 생성을 시킵니다

이런 LLM들은 답변을 어떻게 만드냐

하면 좀 단순화 시켜서 말하면 예를 들어서 안녕하세요 라는 말 다음에 그 뒤에 이어서 올 말이 있잖아요

그럼 뭐 저는 누구입니다

이렇게 할 수도 있고 식사는 하셨어요

이렇게 할 수도 있어요

이외에도 여러 가지가 있겠죠

둘만 가능하다고 합시다

저는 누군가가 확률이 60%고 식사는 하셨어요

가 40%다

그러면은 문장을 생성할 때 난수를 만듭니다

6대4의 비율로 이쪽에 난수에 걸리면 이 문장을 생성을 하고 이쪽에 난수에 걸리면 이 문장을 생성을 하는 거예요

그래서 사실 문장이 아니라 일종의 단어 단위로 이런 처리를 하기 때문에 할 때마다 매번 다른 문장이 생성이 됩니다

그래도 대체로 확률이 높은 쪽은 많이 생성되고 확률이 낮은 쪽은 덜 생성이 되겠죠

그래서 ABCD 여러 가지 문장을 생성을 한 다음에 레이블러 사람한테 이 문장을 순위를 매기게 시켜요

그래서 채취 GPT도 여러분들을 쓰시다 보면 가끔 물어봅니다

답을 두 개를 만들어서 어느 쪽이 더 나아요?

하고 물어봅니다

그러면 사람이 고를 수 있죠

어 난 뒤가 좀 나왔네

난 이쪽이 나왔네

이렇게 고를 수 있어요

그래서 이 선택을 가지고 지도학습을 시킨다

그래서 뭘 지도학습을 하냐면 사람한테 보여줬을 때 사람들은 어느 쪽 답변을 좋아하더라

이건 데이터를 있는 거잖아요

그래서 그거를 지도학습을 합니다

그래서 이제 3단계에서 드디어 강화학습을 해요

강화학습까지 오면 길이 멀었다고죠

강화학습을 하는데 바둑닭인 경우는 승패를 규칙을 가지고 가릴 수 있잖아요

지금은 승패를 규칙을 가지고 못 가리니까 지도학습 한 걸 가지고 승패를 가립니다

그래서 내가 이렇게 문장을 생성하면 그렇게 생성하면 사람들이 별로 안 좋아할 것 같은데 이렇게 하면 아 그래요?

죄송합니다

처벌 이렇게 하고 생성을 했는데 이거는 사람들이 좋아할 것 같은데

아 그래요?

앞으로 더 이렇게 생성해야겠다 그래서 이걸 계속 강화학습을 하면 정말 찰떡같이 사람들이 좋아할 만한 답변을 하게 되는 그런 식으로 이제 챗GPT가 만들어지게 됩니다

챗GPT 나왔을 때 사람들이 우와 너무 좋다

이렇게 반응을 하는 게 애초에 그런 반응을 하도록 강화학습이 되어 있어요

사람들이 너무 좋아하도록 근데 그거에 단점이 많이 있습니다

그 단점 중에 하나가 뭐냐면 사람들은 비위를 잡고 맞춰요

사용자의 비위를 맞추는 특성입니다

그래서 여러분이 챗GPT한테 어떤 말을 할 때 묘하게 그쪽으로 유도하면 그쪽으로 답변을 합니다

항상 여러분의 비위를 맞출 경우가거든요

챗GPT는 진실을 말하도록 강화학습되지 않았어요

여러분의 비위를 맞추도록 지도학습을 했습니다 눈치를 쓱 보고 여러분이 이쪽을 원하는 것 같다 그러면 그쪽으로 답변하게 되겠습니다

나는 그냥 궁금했을 뿐인데 묘하게 어쨌든 얘는 그쪽으로 강화학습이 되기 때문에 정말 최선을 다해서 여러분이 의도하지 않았어도 여러분의 비위를 맞추려고 합니다

그래서 할로시네이션이라고 해서 얘가 헛소리를 할 때가 많거든요

얘가 왜 헛소리를 하느냐?

그 이유 중에 일부도 여기 이런 게 있습니다

그래서 약간 사실관계를 물어보실 때는 주의하셔야 돼요

### 실패 사례

이거는 유튜브 영상 나중에 한번 보시는 재미 있는데 강학 수업을 시키면 이상한 짓들을 하는데 두 번째 테트리스 영상 같은 거는 처벌을 어떻게 해주냐면 게임 오버가 나오면 처벌을 하겠거든요

그래서 테트리스 아시죠?

이렇게 사타가 끝까지 차면 게임 오버 되잖아요

그래서 얘가 어떻게 하냐면 맨 마지막까지 했을 때 더 이상 둘 데가 없다 놓을 데가 없다 그러면은 게임을 포즈를 건 다음에 일시정지 시키고 게임을 다시 시작을 안 합니다

젊지?

옛날에 오락실 저는 오락실 생각했는데 오락실에서 스트릿 파이터 이런 거 하면 대전으로 하거든요 1대1로 하다가 내가 질 것 같으면 성격 나쁜 사람들은 게임기 끄고 도망가요

나 안 잤다 했었죠

인터넷으로 하는 게임도 옛날에 스타크래프트 이런 거 하다가 컨트롤 할 때 딜리트 눌러가지고 게임을 하다가 다운 시켜 버리면 패로 기록이 안 되거든요

그런 아주 악질 매너 사람들이 있었는데 강화학적도 그렇게 해야 됩니다

왜냐하면 얘는 게임 오버만 안 되면 처벌을 안 받아요

게임 오버만 안 시키면 되겠죠

일시정지 걸고 안 하면 되죠

그래서 이런 경우가 굉장히 많습니다

보상을 줄 때 아까도 얘기했지만 잘 안 주면 어떻게든 꼼수를 찾아가지고 처벌만 안 받으려고 한다든지 이런 수를 찾아야 됩니다

그래서 영상들을 한 번씩 보시면 되는데 강화학습이 쉽지 않다

이런 얘기를 하도록 하겠습니다

## 전문가 시스템과 강화학습

예전에 전문가 시스템이랑 퍼지 이런 거, 이런 게임하는 시스템이 많이 있었는데 그런 것도 지시를 가지고 있는 사업이니까 지시를 이렇게 룰화해서 만드는 걸로 알고 있는데 그거랑 강화학습이랑 차이점입니다

전문가 시스템이라는 게 있고요 강화학습이 있습니다

전문가 시스템은 요즘엔 사실 잘 안 해요

요즘엔 잘 안 하는데 왜 안 하느냐

전문가 시스템은 어떤 식으로 하냐면 아이디어 자체는 좋거든요

어떻게 하냐면 전문가를 관찰을 하거나 또는 인터뷰 같은 걸 합니다

이거는 지금도 많이 하거든요

전문가 인터뷰 이런 거 그래서 예를 들면 여러분이 어떤 업무 분야에 전문가가 있다

이 사람들은 도대체 일을 어떻게 하느냐 우리가 그럴 때 있잖아요

회사에서 선배 중에 일을 잘하는 선배가 있어요

나도 저 선배를 닮아서 일을 잘하고 싶어 그러면 일단 첫 번째 방법은 그 선배가 일을 어떻게 하는지 옆에서 세 번째로 하는 거죠

그 다음에 두 번째는 선배한테 물어보는 거죠

선배 이거 어떻게 하는 건가요?

술 먹으면서 물어보는 거죠?

노하우나 득 같은 거 그럼

선배가 가르쳐주겠다

그랬잖아요

그거를 이제 체계적으로 잘 합니다

그 다음에 전문가 시스템은 그거를 일일이 코딩을 해요

일일이 프로그램으로 구현을 합니다

일일이 구현을 하는데 약간 최근에 하는 프로그래밍이랑은 조금 다른데 최근에 하는 프로그래밍은 A를 한 다음에 B를 하고 B를 한 다음에 C를 하고 그 순서대로 사람이 다 지정을 하거든요

근데 보통 전문가 시스템에 들어가는 이 프로그래밍 방식은 조금 특이해 가지고 그렇게 일일이 하진 않고 약간 파편화해서 그러니까 전문가의 어떤 지식이나 행동 방식을 파편화해서 시스템에 넣어두면 그러면은 이 전문가 시스템이 하는 역할이 뭐냐면 그걸 조합을 합니다

우리가 조각조각 내가 전문가가 이럴 때는 이렇게 하고 저럴 때는 저렇게 하고 그거를 막 조각조각 내서 전문가 시스템에 넣어두면 상황에 맞게 그걸 조합하는 거는 전문가 시스템이 역할이에요

그래서 이걸 이제 아 이때는 이렇게 하고 이때는 이렇게 하는데 지금 상황을 보니까 이러니까 이걸 이렇게 조합을 해가지고 이렇게 하면 되겠지

짜잔 이렇게 하는 게 전문가 시스템이 하는 거예요

그래서 이게 한 80, 90년대에 유행하는 방식인데 아이디어만 보면 굉장히 괜찮은데 잘 안 됐어요

잘 안 된 게 뭐냐면 단계 단계별로 어려운 게 너무 많습니다

첫 번째로 전문가를 관찰하고 인터뷰하는 게 시간이 너무 많이 걸려요

왜냐하면 전문가가 말로 설명할 수 없는 그 뭔가를 가지고 있거든요

그러니까 전문가를 관찰하고 있는 그냥 아는 게 많아요

전문가는 보면 그냥 아는 게 너무 많아요

그래서 소방관 연구 이런 거 보면은 그 어디 건물에 불이 났는데 어떤 소방대장이 잠깐 들어가지 마

이런 거예요

근데 건물이 몇 초에 딱 붕괴했어요

이 사람은 그거 어떻게 알았냐 감지 물어보고 감지에 딱 보면 약간 느낌이 세하다고 그거를 어떻게 우리가 프로그램으로 짤 수가 없어요

느낌이 세하다고 이렇게 쓸 수 없잖아요

그러니까 그 어떤 그 미묘한 감, 센스 우리가 회사는 아 센스가 있어 이런 거 있잖아요

그거를 알아내는 게 너무 힘들어요

연구를 많이 해야 됩니다

일단 1단계가 너무 힘들고 알아냈어요

이 사람이 사실은 뭘 보고 알았냐면 연구를 해보니까 소방관 연구에 열기는 느껴지는데 연기가 거꾸로 안 나오더라고요

근데 그거를 자기가 느낌으로 안 거죠 열기에 비해서 연기가 너무 적다

그러니까 우리는 보면 아무것도 몰라요 열기가 평소보다 세, 이 사람은 오랫동안 출동 경험이 있으니까 평소보다 열기는 세, 이 정도 열기면 연기가 어느 정도 나야 되는데 그게 안 간다는 거는 뭐가 뭔가

근데 그게 본인도 모르지만 자기가 열기를 직접적으로 안 계산해 본 거 아니지만 평소가 뭐가 다르다 이상하다

이게 딱 간으로 오는 거죠

그거를 우리가 다 알았어요 프로그램을 짰어요 생각해 보면 이거 굉장히 프로그램 짠 게 까다롭잖아요

열기 넣었고 연기 넣었고 프로그램 짜는 게 굉장히 까다롭고 그래서 이제 퍼펙트 라인에서 시스템을 넣어뒀다 거기까지는 갔다 오케이 문제는 이제 시스템이 조합을 해야 되는데 과거의 전문가 시스템은 그 조각이 다 있어야 됩니다

그 모든 조각이 다 있어야 이거를 딱딱딱딱 끼워 맞춰가지고 하는데 조합이 없으면 어떡하죠?

하나라도 빠지면은 이게 안 돌아가고 근데 그걸 아까도 얘기했지만 뭐 그런 상황 하나만 있겠어요 전문가의 수십까지 수백 가지 어떤 상황이라는 게 있단 말이에요

우리가 산업 현장에서 보통 요즘에 보면은 풀뿌리 산업이 어렵다

뭐죠

주물 뭐 이런 쪽의 그런 산업들이 나이 드신 분들이 자꾸 은퇴하면 그 노하우들이 다 유실된다

이런 얘기 하는데 특히 이제 그런 산업들 보면 대충 감으로 온도가 뜨겁고 이렇게 사부를 퍼넘고 이제 좀 차가운 감을 근데 그거 굉장히 정확하단 말이에요

근데 그거를 왜 어떻게 합니까

이거를 다 모든 상황별로 다 찾아내가지고 전문가 시스템에 넣어 두면 되는데 넣어 둘 수가 없다는 거죠

그래서 잘 안 됐다 이런 거고 강화학습은 이 중간에 이 모든 단계가 없습니다

강화학습은 그 오로지 결과만 있어요

그래서 AI가 알아서 적당히 알아서 잘 딱 깔끔하고 센스 있게 합니다

그리고 우리가 하는 건 뭐냐면 그냥 고상과 처벌만 줘요

잘했네

오케이 못했으면은 뭐야 다시와 이렇게 하는 거예요

정말 그 상사로 모시기 싫은 최악의 상사 있잖아요

그 역할을 우리가 하는 겁니다

어떻게 가르쳐 주지도 않고 해서면 마음에 안 들면 야이 뭐야 다시와

이러고 마음에 들면 어이 잘했어 도대체 뭔지 모르겠지만 우리의 취향을 알아서 맞춰 줘야 돼

그러니까 보면은 우리가 편한 방식이죠

전문가 시스템은 인간이 괴롭습니다 조사도 하고 연구도 하고 코딩도 하고 헉헉헉헉 하고 힘들게 하는 건데 강화학습은 인간은 편한 거예요

아 알아 해봐

난 몰라 헤어든지 말겠지 바둑도 봐 이겼어?

어이 잘했네 졌어?

다시 더?

이렇게 이것만 하는 거예요

그러니까는 전문가 시스템보다 우리가 더 편하죠

편한데 이제 문제가 뭐냐면 이거를 많이 해야 된단 말이에요

반복을 굉장히 많이 왜냐면은 우리가 뭘 원하는지 AI가 알 수가 없으니까 굉장히 많은 시행착오를 해야 됩니다

굉장히 많은 시행착오가 필요합니다

여기서 이제 한 가지 문제가 생기는데 시행착오를 많이 해야 되니까 대부분은 물리적인 시행이 어려워요

예를 들면 아까 불 끄는 얘기를 했는데 알아서 불 잘 꺼봐 인명피해 최소화하고 그래 잘해봐

이런 거죠

예를 들면 우리 소방 소방 아까 얘기했는데 소방서는 어디 행정안전부 소속인 행정안전부 장관이 예를 들면 소방청장한테 일일이 뭐라 하겠어요

불 잘 끄고 인명피해 없고 그러다가 대형사고 나면 지금 뭐 하는 겁니까?

똑바로 안 해요?

이러긴 하지

뭐 어떻게 하라고 하겠어요

자기도 모르는데 근데 장관이 하는 일이 그런 거죠

불러서 조지고 잘하면 좀 칭찬해 주고 그거를 우리가 하는 건데 물리적인 시행으로 하면 시행의 수를 충분히 하기가 힘들기 때문에 그래서 이제 보면 알파고 유명한 사례가 알파고인데 알파고는 바둑두잖아요

바둑이 물리적인 그런게 제약이 없죠

물리적 제약이 없고 그 다음에 챗GPT 제가 얘기 드렸는데 챗GPT도 언어기 때문에 물리적 제약이 없어요

물리적 제약이 없는 환경에서 제일 유지합니다

그리고 만약에 물리적 환경이 필요하다 그러면은 시뮬레이션을 하셔야 돼요

굉장히 정교한 시뮬레이션이 필요합니다 완전히 똑같은 필요는 없지만 어쨌든 큰 틀에서는 실제 물리적 환경과 비슷한 시뮬레이션 환경이 있어야 그 안에서 시뮬레이션에서는 시행착오를 많이 할 수 있잖아요

그리고 뭐 시행착오 하다 실패해도 큰 문제 없고 올 초에도 한 번 제가 다른 회사에서 강화학습 거기도 조선협회에서 한 건데 했는데 거기서 수업들 하러 오신 분들 중 한 팀이 하시는게 선박이 좌초됐을 때 좌초된 선박을 짐을 안에서 옮겨 가지고 더 넘어지지 않게 이런 거 하시는데

여기다가 강화학습을 적용해 보면 어떨까

이런 얘기 하시더라고요

그래서 그걸 하려면 어떻게 되냐면 실제로 배를 좌출시켜 볼 수 없잖아요

우리가 배를 수천만적 좌출 시켜보면 아 배가 좌초됐을 때 이렇게 하면은 피해를 최소화 할 수 있다

경험으로 알 수 있겠지만 그거를 실전하면 돈이 어마어마하게 들겠죠

그러면 어떻게 되나요?

그 배가 좌초된 다양한 암초와 다양한 배의 어떤 형태에서 어떻게 하면은 이거를 할 수 있는지 그 시뮬레이터가 있어야겠죠

파도도 구현해야 될 거고 바람도 구현해야 될 거고 기상 이런 거가 있는 환경에서 해봐야 되니까 사실 강화학습보다 시뮬레이터 만드는 게 더 어려운 작업이 돼요

강화학습은 그냥 있는 알고리즘을 가져다 쓰면 되거든요

그래서 이게 좀 어려운 상황이 되고 만약에 이런 어떤 추상적인 거 바죽이나 언어 이런 추상적인 거면 애초에 시뮬레이션도 필요 없지만 보통 물리적 환경에는 이런 게 어렵게 되고 그 다음에 이제 이거를 했다고 치면은 그 다음에 또 어려운 거 하나는 보상처벌을 어떻게 줄 거냐

이게 좀 어려운 문제가 됩니다

이게 생각보다 이제 항상 회사로 이런 게 문제잖아요

보상처벌을 어떻게 항상 이게 민감한 문제죠

그래서 마찬가지인데 항상 보면은 승진 그 다음에 고과 보너스 이런 거 항상 갈등 있는 게 뭡니까

아 이거 불공정하다 불공정한다는 게 뭐야 정확하게 보상이 주어지지 않는다는 거죠

이게 회사에 좋은 일을 내가 분명히 했는데 왜 그거에 보상을 주지 않느냐

근데 이게 항상 이견이 있을 수밖에 없다는 거예요

그래서 바둑 같은 거는 쉬워요

바둑은 이기면 장땡이다

근데 첫 GPT만 해도 아까도 제가 얘기해 줬듯이 사용자가 좋아하는 거에 보상을 준다

근데 이게 최선이냐

이거예요

그래서 이제 어려운 문제가 됩니다

보상 처벌이 생각보다 어려워요

그래서 이제 정리를 하면은 전문가 시스템은 약간 우리가 좀 힘들게 일을 다 해주는 건데 뭔가 이제 구멍이 나면 좀 어렵다

근데 완벽하게 못 하기 때문에 이제 더 이상 전문가 시스템은 좀 잘 안 하는 거고 근데 그 흔적이 좀 남아 있습니다

요즘에 이제 우리가 RAG라고 하는 건데 검색 증강 행성 이런 건데 예를 들면 책 GPT 같은 걸 쓸 때 지금은 그냥 윗독 끝독 물어보는데 책 GPT에다가 검색을 붙이는 거예요

그래서 내가 질문하면 그냥 네 생각으로 답변하지 말고 일단 검색을 한 다음에 검색 결과를 놓고 내 질문에 대해서 답을 해라

이런 겁니다

그러니까 약간 전문가 시스템하고 좀 비슷한 거죠

기존의 전문가들이 이 문제에 대해서 어떻게 답을 했는지를 보고 근데 완벽한 답이 없으면 그 부분은 네가 메꾸는지만 기본적으로 기존의 텍스트 데이터를 참고해서 답변을 해 이런 건데 약간 과거의 전문가 시스템이 못했던 부분을 메꾸는 거라고 할 수 있겠죠

근데 과학습은 그런 건 아니고 그냥 우리는 시키기만 하고 좋다 나쁘다만 해주는 거예요

근데 그거에 극복해야 되는 점은 시행착오 많이 하는 거랑 보상착오를 주는 거는 우리가 생각을 해야 된다

이렇게 얘기할 수 있어요

다 정리 되시죠?

상황에 따라서는 전문가 시스템이 아직도 유효한 경우가 있을 수 있습니다

예를 들면 빈틈없이 우리가 모든 어떤 전문가의 노하우를 다 구현할 수 있다

그러면 전문가 시스템으로도 할 수 있어요

실제로도 일부에서는 그렇게 합니다

아주 특히 일부 주로 어디냐면 도메인 범위가 굉장히 제한적이에요

전자상거래를 하는데 고객의 환불 요청을 어떻게 처리할 것인가

뭐 약간 이런 아주 제한적인 상황 빈틈없이 할 수 있다 그러면은 되겠죠

현실적으로는 그런 상황이 많지는 않아요

## 퀴즈

<iframe src="https://tally.so/embed/wdAqLr?alignLeft=1&hideTitle=1&transparentBackground=1&dynamicHeight=1" loading="lazy" width="100%" height="1500" frameborder="0" marginheight="0" marginwidth="0" title="강화학습 퀴즈 (01) 머신러닝의 종류"></iframe>