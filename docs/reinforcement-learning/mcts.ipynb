{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCTS\n",
    "\n",
    "## 알파고\n",
    "\n",
    "그래서 강화학습을 지금 그냥 무작위로 초기화된 상태에서 강화학습을 시키잖아요\n",
    "\n",
    "DQN 실습할 때 보면은 딱히 뭘 안해주고 그냥 바로 냅다 네가 시행차고 해라\n",
    "\n",
    "이렇게 한단 말이에요\n",
    "\n",
    "근데 이제 알파고에서 한 방법은 그렇게 하지 않고 일단 프로들이 둔 거를 지도학습을 시킨 다음에 그 다음에 강화학습을 시킵니다\n",
    "\n",
    "이렇게 하면 장점이 뭐냐면 프로들이 그래도 바둑을 아주 못 두지는 않으니까 완전히 먼데서부터 시작해서 시행차고를 해가지고 정책이 여기까지 가는 게 아니라 어쨌든 프로들이 이 근처 어딘가에 있겠죠 그러니까 조금 더 학습을 빨리 할 수 있다\n",
    "\n",
    "이런 아이디어예요\n",
    "\n",
    "여러분들도 강화학습을 하실 때 좀 잘 안 되면 지도학습을 먼저 시킨 다음에 하실 수가 있습니다\n",
    "\n",
    "예를 들면 우리가 배를 제어하는 아침에 하는 거 하면 처음에 이제 학습이 잘 안 되거든요\n",
    "\n",
    "그러면 무작위로 하면 왜 학습이 안 되냐면 여기가 꼬리인데 얘가 이렇게 막 무작위로 움직이다가 어쩌다가 여기 닿아야 아 여기 가는 거구나\n",
    "\n",
    "이걸 알아요\n",
    "\n",
    "그러니까 이게 맵이 크면은 지도가 크면은 여기까지 무작위로 가서 닿기가 쉽지 않단 말이에요\n",
    "\n",
    "그럼 어떻게 하냐면 일단 사람이 이 배를 몰아서 다양한 상황에서 어떤 궤적으로 배를 몰아서 가는지를 데이터를 만들어가지고 그걸로 궤적을 만들어서 일단 지도학습을 시킵니다\n",
    "\n",
    "사람을 흉내 내게 그러면 강화학습을 시킬 때 좀 더 쉽겠죠\n",
    "\n",
    "어쨌든 처음에 사람을 흉내 내다가 서서히 자기가 시행착오를 반복하면서 정책을 계산합니다\n",
    "\n",
    "그래서 알파구에서 썼던 방식이 이 방식이고 그 다음에 이제 알파구 리이를 거쳐서 여기까지는 비슷하게 발전하는데 알파구 제로로 나갑니다\n",
    "\n",
    "여기 원래 정책망하고 가치망이 분리가 돼 있었는데 정책망하고 가치망을 하나의 네트워크로 바꾸고 그 다음에 이제 점점 기술이 발전하면서 이 롤 아웃망이 따로 필요가 없어요\n",
    "\n",
    "그래서 원래는 롤 아웃할 때는 가벼운 네트워크로 했는데 그냥 하나의 네트워크로 가치도 예측하고 정책도 결정하고 그 정책 결정하는 것으로 롤 아웃도 하고 이렇게 하게 됩니다\n",
    "\n",
    "그 다음에 이제 알파 제로라고 해서 원래는 바둑만 둘 수 있었는데 그래서 이 알파구 제로에는 바둑에 특화된 여러 가지 몇 가지가 들어 있어요\n",
    "\n",
    "근데 알파 제로에서는 바둑에 특화된 걸 다 빼버립니다\n",
    "\n",
    "그래도 바둑을 잘 주더라 실제로 바둑 실력 자체는 알파구 제로가 알파 제로보다 더 잘 도요 특화된 모델이기 때문에 특화시키지 않아도 잘 된다는 걸 입증을 하게 되고요 이 알파 제로를 마지막으로 이게 다 구글 딥마인드에서 만든 건데 딥마인드는 알파구에서 손을 떼게 됩니다\n",
    "\n",
    "바둑은 우리 이제 더 이상 연구할 게 없다\n",
    "\n",
    "우리는 그 다음으로 가겠다\n",
    "\n",
    "그래서 이제 뭐 요즘에 여러 가지 하는데 요즘에 유명한 거는 알파 폴드라고 있습니다\n",
    "\n",
    "뭐냐면 단백질이 이렇게 우리가 A,T,C,G 뭐 이런 식으로 그 유전자 DNA가 있으면 얘가 이제 아미노산을 만들고 그 아미노산이 어떤 순서로 조합이 되면 얘네가 이렇게 그 자기네 애들끼리 인력하고 청력이 작용해서 얘네가 이렇게 가다가 갑자기 이렇게 팍 접히거든요\n",
    "\n",
    "그래서 단백질이 접힌 모양에 따라서 단백질의 어떤 특성이 결정되는데 우리가 그걸 미리 예상하기가 되게 힘들어요\n",
    "\n",
    "우리는 알 수 있는 게 뭐 A,T,C,G 이렇게 DNA 서열이 어떻게 되고 그 DNA에 따라서 아미노산이 스무 종이 나오는데 그 DNA가 어떻게 아미노산으로 번역되는지 정확하게 압니다\n",
    "\n",
    "근데 그 아미노산이 어떤 시퀀스를 만들 때 걔네가 아미노산이 연결되면 단백질이 되거든요\n",
    "\n",
    "어디서 접히고 어디가 펼쳐지는지를 예측을 못해요\n",
    "\n",
    "물론 물리학적으로 시뮬레이션 하면 알 수 있는데 물리학적 시뮬레이션이 너무 어렵습니다\n",
    "\n",
    "왜냐하면 아미노산이 한 두 개가 있는 게 아니잖아요\n",
    "\n",
    "단백질 하나에 아미노산이 막 수십, 수백 개가 있단 말이에요\n",
    "\n",
    "그럼 걔네끼리 서로 밀고 당기고 밀고 당기고 하면 우리가 그 삼체 문제 이런 거 하면 세 개만 돼도 예측하게 되게 힘든데 이건 다체 문제라서 멀티 바디 풀업을 해서 잘 안 풀린단 말이에요\n",
    "\n",
    "근데 알파 폴드는 그거를 이제 딥러닝을 이용을 해가지고 단백질이 어떻게 접힐까\n",
    "\n",
    "이런 걸 예측을 하는 거죠\n",
    "\n",
    "그래서 만약에 우리가 단백질 모양이 어떻게 될지 알 수 있으면 신약 개발 같은 거 할 때 막 단백질이 되는 그런 약을 만들면 얘네가 어떤 성질을 결국 띄게 될지를 예측할 수 있습니다\n",
    "\n",
    "실제로 시험을 해보지 않아도 정확하게 맞는 건 아닌데 정확하게 맞지 않더라도 우리가 후보물질의 범위를 굉장히 많이 줄일 수 있겠죠\n",
    "\n",
    "그래서 딥마인드는 그런 거 연구하고 있고 요즘에 바둑은 접었어요\n",
    "\n",
    "근데 이제 그럼 끝났냐 하면 그건 아니고 찾아보시면 그래서 이런 알파고 논문이 다 있으니까 논문을 보고 구현한 바둑 모델들이 굉장히 많이 있습니다\n",
    "\n",
    "요즘에 프로 바둑 기사들은 바둑 선수를 바둑 기사라고 하는데 바둑 기사들은 집에 되게 좋은 고성능의 딥러닝 돌릴 수 있는 컴퓨터들을 다 한씩 가지고 있고 다 AI랑 바둑을 두면서 연습을 해요\n",
    "\n",
    "그래서 요즘에 프로 바둑 기사들의 실력이 굉장히 상향편준화가 됐습니다\n",
    "\n",
    "왜냐하면 옛날에는 그 옛날에 이제 여러분 이름 기억하실지\n",
    "\n",
    "모르죠\n",
    "\n",
    "이창호 구단이라고 있는데 이창호 구단이 바둑을 어떻게 배웠냐면 어렸을 때부터 조인원 구단이라고 당대 최고의 바둑 기사 집에 들어가서 10살 때인가?\n",
    "\n",
    "되게 어렸을 때, 아기 때부터 그 집에서 완전 먹이고 재우고 씻기고 옷 입히고 다 했어요\n",
    "\n",
    "근데 그래서 스승하고 집에서 같이 먹고 자고 하면서 바둑을 배워가지고 나중에 스승이 가지고 있던 타이틀을 다 뺐죠 호랑이 새끼를 키운 건데 옛날에는 되게 고수한테 배우지 않으면 바둑을 배울 방법이 없는데 지금은 AI가 바둑 어느 고수보다도 바둑을 잘 두니까 그냥 AI한테 배우면 됩니다\n",
    "\n",
    "그리고 AI는 이수를 들면 승률이 몇 퍼센트까지인지도 알려주거든요\n",
    "\n",
    "내가 잘못 들면 그거는 80%짜리 쓰고 여기다 두면 90%야\n",
    "\n",
    "이런 거 알려주니까 공부가 엄청 되겠죠\n",
    "\n",
    "그래서 바둑 기사들의 실력이 전반적으로 굉장히 많이 올라갔습니다\n",
    "\n",
    "그래서 이제 알파고 같은 경우는 우리가 바둑의 실력을 평가할 때 엘로 평점이라는 걸 쓰는데 엘로라는 사람이 만든 방식이에요\n",
    "\n",
    "그래서 여기 알파고 제로에서 트리서치를 하면 여기까지 되고 이게 어차피 정책만 가지고 하는 거니까 트리 탐색을 안 해봐도 기본 정책으로 보면 내가 어디다 두면 좋은지 알거든요\n",
    "\n",
    "그래서 트리 탐색을 안 해도 수를 둘 수 있는데 트리 탐색을 안 하면 실력이 이 정도밖에 안 됩니다\n",
    "\n",
    "트리 탐색을 하면 내가 최고의 실력을 가지고 있지만 그 실력으로 100번 1000번 도보 내린 결론이니까 더 정확하겠죠\n",
    "\n",
    "실력이 더 오르는 거고 인간 최고수의 실력은 이 수준밖에 안 됩니다\n",
    "\n",
    "그래서 엄청난 격차가 있죠\n",
    "\n",
    "근데 이것도 지금 멈춰있는 게 아니라 계속 올라가고 있어요\n",
    "\n",
    "알파고 제로는 이 정도 수준밖에 안 됩니다\n",
    "\n",
    "그 뒤에 나온 카타고가 훨씬 더 잘 도요\n",
    "\n",
    "그래서 끝도 없이 올라가고 있다\n",
    "\n",
    "## 엘로 평점\n",
    "\n",
    "이런 얘기고 그 다음 엘로 평점 얘기를 잠깐 드리면 엘로 평점은 사람 이름입니다\n",
    "\n",
    "엘로가 약자 아니에요\n",
    "\n",
    "엘로라는 사람이 만들어서 엘로인데 이 사람은 또 신기하게 물리학자에요 물리학자인데 이 사람은 체스 광이었거든요\n",
    "\n",
    "체스 선수들 간의 실력을 어떻게 평가할까 하다가 이런 공식을 만들어요\n",
    "\n",
    "그래서 s가 이기는 횟수고 e가 진 횟수인데 이 두 개를 차이를 한 다음에 여기다가 k를 곱해요\n",
    "\n",
    "그 다음에 원래 평점이 있었는데 이 공식대로 평점을 수정을 하면 이걸 계속 수정을 하면 자기 실력으로 평점이 수렴한다\n",
    "\n",
    "이런 얘기입니다\n",
    "\n",
    "근데 여러분 이 공식 어디서 많이 본 거 같지 않으세요\n",
    "\n",
    "m분의 1 한 다음에 r 빼기 v 이렇게 하면 v가 여기가 있죠\n",
    "\n",
    "우리 같이 산정할 때 맨날 이 공식으로 하죠\n",
    "\n",
    "그래서 엘로 평점 시스템을 쓰는 공식이 의도한 건 아닌데 강화학습에서 우리 같이 수정하는 시기랑 똑같습니다\n",
    "\n",
    "그래서 몬테카를로로 하면 여기가 g가 돼 가지고 우리가 g를 많이 하면 결국 몬테카를로에서 이 업데이트하는 시기랑 똑같게 되는 거고 사실 생각해보면 논리가 똑같을 수밖에 없어요\n",
    "\n",
    "엘로의 생각은 뭐냐면 평점이 높은 사람은 이기고 평점이 낮은 사람은 지는데 결국에는 많이 하다 보면 자기 평점 으로 수렴을 하게 된다\n",
    "\n",
    "자기 승률이 있으니까 그리고 우리가 몬테카를로에서 같이 수정도 내가 확률은 잘 모르겠지만 하여간 많이 하면 내 가치로 수렴한다\n",
    "\n",
    "이 얘기니까 사실 아이디어가 똑같거든요\n",
    "\n",
    "그래서 서로 굉장히 다른 분야에서 굉장히 다른 백그라운드의 사람들이 똑같은 결론에 도달하게 됩니다\n",
    "\n",
    "그래서 이제 이렇게 되고 그래서 엘로 평점 시스템에서 자기 점수를 알면 여기 앞에 있는 공식에다 넣으면 내 평점하고 상대방 평점을 여기다 넣고 계산하면 승률이 몇 퍼센트가 나올지를 알 수 있습니다\n",
    "\n",
    "그래서 여기 보통 400으로 나누고 이렇게 하니까 한번 계산해 보시면 점수 차이가 이 정도 나버리면 거의 승률이 0이에요\n",
    "\n",
    "사람 이길 수 없죠 사람은 절대 못 이긴다\n",
    "\n",
    "이렇게 되고 그다음에 이거를 또 응용을 할 수 있는데 어떻게 응용할 수 있냐\n",
    "\n",
    "## 문항 반응 이론\n",
    "\n",
    "면 직접적으로 응용한 건 아닌데 이건 교육학에서 나온 문학 반응 이론이라는 게 있거든요\n",
    "\n",
    "그래서 이것도 서로 다른 사람들이 다른 문제를 가지고 풀었는데 같은 결론에 도달한 건데 문학 반응 이론은 뭐냐면 학생들한테 시험 문제를 낼 때 자기 수준에 맞는 문제를 내자\n",
    "\n",
    "이거 자기 수준에 맞는 문제를 내자는 건데 그러면 학생의 실력도 알아야 되고 문제의 수준도 알아야 되는 거죠\n",
    "\n",
    "두 개를 알아야 그 학생 수준에 맞는 것을 알게 되겠죠\n",
    "\n",
    "그래서 문학 반응 이론은 어떻게 생각하냐면 똑같습니다\n",
    "\n",
    "이런 식으로 해가지고 내가 맞추면 이긴 거고 지면 틀린 거잖아요\n",
    "\n",
    "그럼 그걸 반복을 하면 학생의 자기 실력으로 평점이 수렴하겠죠\n",
    "\n",
    "근데 문제도 마찬가지로 문제를 내가 문제 입장에서 보면 학생이 틀리게 하면 그 문제가 학생을 이긴 거고 문제가 학생한테 맞춤 당하면 그 문제가 진 거예요\n",
    "\n",
    "그럼 문제도 평점을 매길 수 있겠죠\n",
    "\n",
    "서로 그렇게 굉장히 많은 학생이 굉장히 많은 문제를 내보면 결국 모든 문제가 자기 수준을 평점으로 가지게 됩니다\n",
    "\n",
    "그리고 학생도 그걸 굉장히 많이 하면 자기 수준을 평점으로 가지게 돼요\n",
    "\n",
    "그러면 자기 수준에 맞는 문제를 많이 내보면 사람도 그렇잖아요\n",
    "\n",
    "자기 실력하고 비슷한 사람하고 겨뤄봐야 자기 실력을 파악할 수 있겠죠\n",
    "\n",
    "너무 잘하는 사람하고 해보면 맨날 지니까 실력이 뭔지 알 수가 없어요\n",
    "\n",
    "왜냐하면 조금 못해도 지고 많이 못해도 지고 그러니까 그냥 지기만 한단 말이에요\n",
    "\n",
    "반대도 마찬가지고 여러분이 운동을 연습하는데 초등학생이랑 맨날 시합해보면 여러분 실력을 잘하는지 못하는지 알 수가 없어요 실력이 비슷한 사람끼리 겨뤄봐야 실력 평가가 됩니다\n",
    "\n",
    "그래서 문학 반응 이론이라고 해가지고 문학 난이도를 이런 식으로 해서 딱 정확하게 학생 수준에 맞춰서 볼 수 있는 시스템이 있습니다\n",
    "\n",
    "그래서 보면 강화학습, 엘로 평점, 문학 반응 이론 전부 다른 문제를 푸는데 결론이 똑같다\n",
    "\n",
    "그래서 여러분들이 강화학습을 공부하시면 강화학습만 아는 게 아니라 다른 분야의 것들도 이렇게 아실 수 있는 거죠\n",
    "\n",
    "이 얘기는 며칠 전에 한번 드리긴 했는데\n",
    "\n",
    "근데 이 문학 반응 이론이 이론인 이유가 있어요 이론이 항상 현실에 안 맞는데 다 논리가 좋지만 잘 안 통하는데 특히 한국 사람들이나 중국 사람들 같이 항상 룰 브레킹을 좋아하는 우리 같은 사람들한테는 잘 안 맞습니다\n",
    "\n",
    "왜냐하면 우리는 시험을 잘 보는데 너무나 특화되어 있기 때문에 그러면 초반에 내 평점을 좀 높여놔가지고 시스템을 속이면 내 능력을 실제 보다 더 높게 평가를 받을 수 있거든요\n",
    "\n",
    "그래서 게임 같은 거 해보신 분들은 아시게 들어보셨을 텐데 버스 태워준다고 그러잖아요\n",
    "\n",
    "이 용어 아시는 분 있나요?\n",
    "\n",
    "롤 같은 데서 많이 하는 거 어떻게 하는 겁니까?\n",
    "\n",
    "초반에 내 친구가 게임 대신 해가지고 아니면 게임 같이 해줘가지고 내 평점을 확 높여주면 내가 리그가 높아지니까 거기서는 좀 져도 별로 점수가 많이 안 떨어진단 말이에요\n",
    "\n",
    "근데 초반에 내가 좀 못해서 이상한 리그 가 있으면 거기서 아무리 발버둥 쳐봤자 빠져나가기가 쉽지 않아요\n",
    "\n",
    "그런 것도 약간 어르신이죠\n",
    "\n",
    "그래서 룰을 속이는 건데 우리나라 사람들은 절대 자기 실력대로 시험 점수 받으려고 하지 않습니다\n",
    "\n",
    "항상 자기 실력보다 높은 점수를 받는데 온갖 꼼수를 다 내기 때문에 문학 반응 이론이 한국에서는 별로 적용이 안 된다\n",
    "\n",
    "그래서 실제로 컴퓨터 어댑티브 테스팅을 하는 경우가 거의 없고 언제 하냐면 수능 끝나면 교육과정 평가원에서 사후적으로 이걸로 분석해가지고 수능 문제, 강 문제의 난이도를 분석을 해요\n",
    "\n",
    "그래서 4점짜리가 실제로 어려웠냐 3점짜리가 실제로 적당했냐\n",
    "\n",
    "뭐 그걸 이제 이런 식으로 평가하는 거죠\n",
    "\n",
    "그래서 이런 걸로도 한다\n",
    "\n",
    "이런 얘기고요 그다음에 우리가 실제로 실습을 해 볼 건데 mtsc는 mcts는 베이스 라인에 구현이 안 되기 때문에 직접 구현해야 됩니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tic-Tac-Toe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, namedtuple\n",
    "import numpy as np\n",
    "\n",
    "PLAYER1 = 'X'\n",
    "PLAYER2 = 'O'\n",
    "EMPTY = ' '\n",
    "\n",
    "# tup: 배치, turn: 턴, winner: 승자, terminal: 종료 여부\n",
    "_TTTB = namedtuple(\"TicTacToeBoard\", \"tup turn winner terminal\")\n",
    "class TicTacToeBoard(_TTTB):\n",
    "    @classmethod\n",
    "    def new(cls):\n",
    "        return cls(tup=(EMPTY,) * 9, turn=PLAYER1, winner=None, terminal=False)\n",
    "    \n",
    "    def available_moves(self):  # 가능한 수\n",
    "        return [i for i, value in enumerate(self.tup) if value == EMPTY]\n",
    "\n",
    "    def find_children(self):  # 자식(후속 수) 찾기\n",
    "        if self.terminal:\n",
    "            return set()\n",
    "        return {self.make_move(i) for i in self.available_moves()}\n",
    "\n",
    "    def random_move(self):  # 무작위로 두기\n",
    "        return self.make_move(np.random.choice(self.available_moves()))\n",
    "    \n",
    "    def reward(self, player):\n",
    "        if self.winner is None:\n",
    "            return 0.5 # 무\n",
    "        elif player == self.winner:\n",
    "            return 1 # 승\n",
    "        else:\n",
    "            return 0 # 패\n",
    "\n",
    "    def make_move(self, index):  # index에 수 두기\n",
    "        tup = self.tup[:index] + (self.turn,) + self.tup[index + 1 :]\n",
    "        turn = PLAYER2 if self.turn == PLAYER1 else PLAYER1  # 다음 턴\n",
    "        winner = self.find_winner(tup)  # 승자\n",
    "        terminal = winner is not None or EMPTY not in tup  # 종료 여부\n",
    "        return TicTacToeBoard(tup, turn, winner, terminal)\n",
    "\n",
    "    def find_winner(self, tup):\n",
    "        winning_comobos = [   # 일렬이 되는 경우\n",
    "            [0, 1, 2], [3, 4, 5], [6, 7, 8], [0, 3, 6], [1, 4, 7], [2, 5, 8], [0, 4, 8], [2, 4, 6]]\n",
    "        for i, j, k in winning_comobos:  # 일렬을 만들면 이김\n",
    "            if tup[i] != EMPTY and tup[i] == tup[j] == tup[k]:\n",
    "                return tup[i]\n",
    "        return None\n",
    "\n",
    "    def to_pretty_string(self):  # 예쁘게 출력\n",
    "        lines = ['  1 2 3']\n",
    "        for i in range(0, 9, 3):\n",
    "            row = self.tup[i:i+3]\n",
    "            lines.append(f'{i+1} {\" \".join(row)}')\n",
    "        return '\\n'.join(lines) + '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCTS 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS:\n",
    "    def __init__(self, exploration_weight=2):\n",
    "        self.Q = Counter()  # 보상 합계\n",
    "        self.N = Counter()  # 방문 횟수\n",
    "        self.children = dict()  # 자식 노드\n",
    "        self.exploration_weight = exploration_weight # 탐색 가중치\n",
    "\n",
    "    def search(self, node, budget=50):\n",
    "            for _ in range(budget):  # 예산 내에서 롤아웃\n",
    "                self.do_rollout(node)\n",
    "\n",
    "            def score(n):\n",
    "                if self.N[n] == 0:\n",
    "                    return -np.inf  # 방문하지 않은 경우는 -무한대\n",
    "                return self.Q[n] / self.N[n]  # 평균 보상\n",
    "\n",
    "            return max(self.children[node], key=score) # 평균 보상이 큰 자식 선택\n",
    "    \n",
    "    def do_rollout(self, node):\n",
    "        path = self._select(node)\n",
    "        leaf = path[-1]\n",
    "        self._expand(leaf)\n",
    "        reward = self._simulate(leaf, node.turn)\n",
    "        self._backpropagate(path, reward)\n",
    "\n",
    "    def _select(self, node):\n",
    "        path = []\n",
    "        while True:\n",
    "            path.append(node)\n",
    "            if node not in self.children or node.terminal: # 방문한 적이 없거나, 종료 노드이면 끝\n",
    "                return path\n",
    "            unexplored = self.children[node] - self.children.keys()\n",
    "            if unexplored: # 방문한 적이 있으면, 자식 노드 중에 탐색 안한 노드 추가\n",
    "                n = unexplored.pop()\n",
    "                path.append(n)\n",
    "                return path        \n",
    "            node = self._uct_select(node) # 모든 자식 노드를 방문했으면 UCT로 선택\n",
    "\n",
    "    def _expand(self, node):\n",
    "        # 자식 노드를 확장\n",
    "        if node not in self.children:\n",
    "            self.children[node] = node.find_children()\n",
    "\n",
    "    def _simulate(self, node, player):\n",
    "        while not node.terminal:  # 끝날 때까지 기본 정책으로 진행\n",
    "            node = node.random_move()  # 기본 정책은 무작위로 두기\n",
    "        return node.reward(player) # 보상 계산\n",
    "\n",
    "    def _backpropagate(self, path, reward):\n",
    "        for node in reversed(path):\n",
    "            self.N[node] += 1\n",
    "            self.Q[node] += reward\n",
    "            reward = 1 - reward  # 상대방의 보상\n",
    "\n",
    "    def _uct_select(self, node):\n",
    "        Ns = np.log(self.N[node])\n",
    "        def uct(n):\n",
    "            return self.Q[n] / self.N[n] + self.exploration_weight * np.sqrt(Ns / self.N[n])\n",
    "        return max(self.children[node], key=uct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "승률 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 2 3\n",
      "1      \n",
      "4     X\n",
      "7      \n",
      "\n",
      "Q=5.5, N=7, Q/N=78%\n",
      "  1 2 3\n",
      "1   O  \n",
      "4     X\n",
      "7      \n",
      "\n",
      "  1 2 3\n",
      "1   O  \n",
      "4     X\n",
      "7   X  \n",
      "\n",
      "Q=8.0, N=11, Q/N=72%\n",
      "  1 2 3\n",
      "1   O  \n",
      "4     X\n",
      "7 O X  \n",
      "\n",
      "  1 2 3\n",
      "1   O X\n",
      "4     X\n",
      "7 O X  \n",
      "\n",
      "Q=14, N=17, Q/N=82%\n",
      "  1 2 3\n",
      "1 O O X\n",
      "4     X\n",
      "7 O X  \n",
      "\n",
      "  1 2 3\n",
      "1 O O X\n",
      "4   X X\n",
      "7 O X  \n",
      "\n",
      "Q=30, N=30, Q/N=100%\n",
      "  1 2 3\n",
      "1 O O X\n",
      "4 O X X\n",
      "7 O X  \n",
      "\n",
      "Winner: O\n"
     ]
    }
   ],
   "source": [
    "mcts = MCTS()\n",
    "board = TicTacToeBoard.new()\n",
    "while not board.terminal:\n",
    "    if board.turn == PLAYER1:\n",
    "        board = board.random_move()  # 플레이어 1은 무작위로\n",
    "    else:\n",
    "        board = mcts.search(board)  #플레이어 2는 MCTS로\n",
    "        win_rate = int(100*mcts.Q[board]/mcts.N[board])\n",
    "        print(f'Q={mcts.Q[board]}, N={mcts.N[board]}, Q/N={win_rate}%')\n",
    "    print(board.to_pretty_string())\n",
    "print(f'Winner: {board.winner}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 퀴즈\n",
    "\n",
    "<iframe src=\"https://tally.so/embed/mYYRq6?alignLeft=1&hideTitle=1&transparentBackground=1&dynamicHeight=1\" loading=\"lazy\" width=\"100%\" height=\"1100\" frameborder=\"0\" marginheight=\"0\" marginwidth=\"0\" title=\"[RL] MCTS\"></iframe>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
