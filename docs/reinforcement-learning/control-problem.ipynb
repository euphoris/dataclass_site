{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제어 문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!hide\n",
    "from collections import defaultdict\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class MCPrediction:\n",
    "    def __init__(self, env, gamma=0.9):\n",
    "        self.gamma = gamma\n",
    "        self.env = env\n",
    "        self.V = np.zeros(env.observation_space.n)\n",
    "        self.returns = defaultdict(list)\n",
    "\n",
    "    def select_action(self, policy, state):\n",
    "        return policy[state]\n",
    "    \n",
    "    def generate_episode(self, policy):\n",
    "        episode = []\n",
    "        state, info = self.env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = self.select_action(policy, state)\n",
    "            next_state, reward, done, _, _ = self.env.step(action)\n",
    "            episode.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "        return episode\n",
    "\n",
    "    def update_value(self, episode):\n",
    "        G = 0\n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, reward, next_state, done = episode[t]\n",
    "            G = self.gamma * G + reward\n",
    "            if state not in [x[0] for x in episode[:t]]:\n",
    "                self.returns[state].append(G)\n",
    "                self.V[state] = np.mean(self.returns[state])\n",
    "\n",
    "    def evaluate_policy(self, policy, num_episodes=1000):\n",
    "        nS = self.env.observation_space.n\n",
    "        self.V_track = np.zeros((nS, num_episodes))\n",
    "        for e in range(num_episodes):\n",
    "            episode = self.generate_episode(policy)\n",
    "            self.update_value(episode)\n",
    "            self.V_track[:, e] = self.V.copy()\n",
    "        return self.V, self.V_track\n",
    "\n",
    "class SlipperyWalk(gym.Env):\n",
    "    def __init__(self, length):\n",
    "        super().__init__()\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "        self.observation_space = gym.spaces.Discrete(length)\n",
    "\n",
    "        self.start_state = length // 2  # 2로 나눈 몫\n",
    "        self.state = self.start_state\n",
    "        self.done = False\n",
    "        self.truncated = False\n",
    "        self.length = length\n",
    "        self.P = self.set_transition()\n",
    "\n",
    "    def set_transition(self):\n",
    "        P = {}\n",
    "        transition_probs = {'MOVE': 1/2, 'STAY': 1/3, 'OPPOSITE': 1/6}\n",
    "        goal = self.length - 1\n",
    "\n",
    "        for s in range(self.length):\n",
    "            P[s] = {}\n",
    "            for a in range(2):\n",
    "                if s == 0:\n",
    "                    P[s][a] = [(1.0, s, 0, True)]  # 확률, 상태, 보상, 종료 여부\n",
    "                elif s == goal:\n",
    "                    P[s][a] = [(1.0, s, 1, True)]\n",
    "                else:\n",
    "                    transitions = []\n",
    "                    for transition, prob in transition_probs.items():\n",
    "                        if transition == 'MOVE':\n",
    "                            next_state = s + 1 if a == 1 else s - 1\n",
    "                        elif transition == 'STAY':\n",
    "                            next_state = s\n",
    "                        elif transition == 'OPPOSITE':\n",
    "                            next_state = s - 1 if a == 1 else s + 1\n",
    "                        next_state = max(0, min(goal, next_state))   # 범위 제한 표현\n",
    "                        reward = 1 if next_state == goal else 0      # 삼항 연산자\n",
    "                        done = next_state == 0 or next_state == goal\n",
    "                        transitions.append((prob, next_state, reward, done))\n",
    "                    P[s][a] = transitions\n",
    "\n",
    "        return P\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.start_state\n",
    "        self.done = False\n",
    "        self.truncated = False\n",
    "        self.steps = 0\n",
    "        return self.state, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        transitions = self.P[self.state][action]\n",
    "        probs = np.array([t[0] for t in transitions])\n",
    "        i = np.random.choice(np.arange(len(transitions)), p=probs)\n",
    "        _, self.state, reward, self.done = transitions[i]\n",
    "        self.steps += 1\n",
    "        self.truncated = self.steps >= 100\n",
    "        return self.state, reward, self.done, self.truncated, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        grid = ['X'] + ([' '] * (self.length - 2)) + ['O']  # 리스트 연산\n",
    "        grid[self.state] = 'A'\n",
    "        print('|' + '|'.join(grid) + '|') # 문자열 연산\n",
    "\n",
    "    @property\n",
    "    def unwrapped(self):\n",
    "        return self\n",
    "\n",
    "def policy_evaluation(pi, P, gamma=1.0, theta=1e-10):\n",
    "    nS = len(P)  # 상태의 수\n",
    "    prev_V = np.zeros(nS)  # ns와 크기가 같은 영(0)벡터\n",
    "    while True:\n",
    "        V = np.zeros_like(prev_V)  # prev_V와 모양이 같은 영벡터\n",
    "        for s in range(nS):\n",
    "            for prob, next_state, reward, done in P[s][pi[s]]:\n",
    "                rtrn = reward + gamma * prev_V[next_state] * (not done)\n",
    "                V[s] += prob * rtrn\n",
    "        if np.max(np.abs(prev_V - V)) < theta: # 기존 V와 차이가 작으면 중단\n",
    "            break\n",
    "        prev_V = V.copy()\n",
    "    return V\n",
    "\n",
    "def rmse(x, y):\n",
    "    return np.sqrt(np.mean((x - y)**2))\n",
    "\n",
    "def plot_value_track(V_track, V_true, env):\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, env.length - 2))\n",
    "\n",
    "    for i in range(env.length - 2):\n",
    "        j = i + 1\n",
    "        plt.plot(V_track[j], color=colors[i])\n",
    "        plt.hlines(V_true[j], 0, 1000, colors=colors[i], linestyles='dashed')\n",
    "        plt.text(0, V_true[j], f'{i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On-Policy\n",
    "\n",
    "### MC 제어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCControl(MCPrediction):\n",
    "    def __init__(self, env, gamma=0.9, epsilon=0.1):\n",
    "        super().__init__(env, gamma)\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "        self.returns = defaultdict(list)\n",
    "\n",
    "    def select_action(self, policy, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(self.Q[state])\n",
    "\n",
    "    def update_value(self, episode):\n",
    "        G = 0\n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, reward, next_state, done = episode[t]\n",
    "            G = self.gamma * G + reward\n",
    "            if (state, action) not in [(x[0], x[1]) for x in episode[:t]]:\n",
    "                self.returns[(state, action)].append(G)\n",
    "                self.Q[state][action] = np.mean(self.returns[(state, action)])\n",
    "\n",
    "    def improve_policy(self):\n",
    "        policy = {}\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            policy[state] = np.argmax(self.Q[state])\n",
    "        return policy\n",
    "\n",
    "    def evaluate_policy(self, policy, num_episodes=1000):\n",
    "        nS = self.env.observation_space.n\n",
    "        nA = self.env.action_space.n\n",
    "        self.Q_track = np.zeros((nS, nA, num_episodes))\n",
    "        for e in range(num_episodes):\n",
    "            episode = self.generate_episode(policy)\n",
    "            self.update_value(episode)\n",
    "            self.Q_track[:, :, e] = self.Q.copy()\n",
    "        return self.Q, self.Q_track\n",
    "\n",
    "    def control(self, policy, num_episodes=1000):\n",
    "        for _ in range(num_episodes):\n",
    "            episode = self.generate_episode()\n",
    "            self.update_value(episode)\n",
    "        return self.improve_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0, 1: 0, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = SlipperyWalk(9)\n",
    "agent = MCControl(env)\n",
    "LEFT, RIGHT = 0, 1\n",
    "pi = {s: LEFT for s in range(env.observation_space.n)}\n",
    "agent.evaluate_policy(pi)\n",
    "new_pi = agent.improve_policy()\n",
    "new_pi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA(MCControl):\n",
    "    def __init__(self, env, gamma=0.9, alpha=0.1, epsilon=0.1):\n",
    "        super().__init__(env, gamma, epsilon)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def generate_episode(self, policy):\n",
    "        episode = []\n",
    "        state, info = self.env.reset()\n",
    "        action = self.select_action(policy, state)\n",
    "        done = False\n",
    "        while not done:\n",
    "            next_state, reward, done, _, _ = self.env.step(action)\n",
    "            next_action = self.select_action(policy, next_state)\n",
    "            episode.append((state, action, reward, next_state, next_action, done))\n",
    "            state, action = next_state, next_action\n",
    "        return episode\n",
    "\n",
    "    def update_value(self, episode):\n",
    "        for t in range(len(episode)):\n",
    "            state, action, reward, next_state, next_action, done = episode[t]\n",
    "            td_target = reward + (self.gamma * self.Q[next_state][next_action] * (not done))\n",
    "            td_error = td_target - self.Q[state][action]\n",
    "            self.Q[state][action] += self.alpha * td_error\n",
    "\n",
    "    def control(self, policy, num_episodes=1000):\n",
    "        for _ in range(num_episodes):\n",
    "            episode = self.generate_episode(policy)\n",
    "            self.update_value(episode)\n",
    "        return self.improve_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = SARSA(env)\n",
    "agent.evaluate_policy(pi)\n",
    "new_pi = agent.improve_policy()\n",
    "new_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Off-Policy\n",
    "\n",
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning(SARSA):\n",
    "    def generate_episode(self, policy):\n",
    "        episode = []\n",
    "        state, info = self.env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = self.select_action(policy, state)\n",
    "            next_state, reward, done, _, _ = self.env.step(action)\n",
    "            episode.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "        return episode\n",
    "    \n",
    "    def update_value(self, episode):\n",
    "        for t in range(len(episode)):\n",
    "            state, action, reward, next_state, done = episode[t]\n",
    "            td_target = reward + (self.gamma * np.max(self.Q[next_state]) * (not done))\n",
    "            td_error = td_target - self.Q[state][action]\n",
    "            self.Q[state][action] += self.alpha * td_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = QLearning(env)\n",
    "agent.evaluate_policy(pi)\n",
    "new_pi = agent.improve_policy()\n",
    "new_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이중 Q-학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleQLearning(QLearning):\n",
    "    def __init__(self, env, gamma=0.9, alpha=0.1, epsilon=0.1):\n",
    "        super().__init__(env, gamma, alpha, epsilon)\n",
    "        self.QA = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "        self.QB = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    def update_value(self, episode):\n",
    "        for t in range(len(episode)):\n",
    "            state, action, reward, next_state, done = episode[t]\n",
    "            if np.random.rand() < 0.5:\n",
    "                Q1, Q2 = self.QA, self.QB\n",
    "            else:\n",
    "                Q1, Q2 = self.QB, self.QA\n",
    "\n",
    "            best_next_action = np.argmax(Q1[next_state])\n",
    "            td_target = reward + (self.gamma * Q2[next_state][best_next_action] * (not done))\n",
    "            td_error = td_target - Q1[state][action]\n",
    "            Q1[state][action] += self.alpha * td_error\n",
    "        self.Q = (self.QA + self.QB) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = DoubleQLearning(env)\n",
    "agent.evaluate_policy(pi)\n",
    "new_pi = agent.improve_policy()\n",
    "new_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA(λ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSALambda(SARSA):\n",
    "    def __init__(self, env, gamma=0.9, alpha=0.1, epsilon=0.1, lambd=0.9):\n",
    "        super().__init__(env, gamma, alpha, epsilon)\n",
    "        self.lambd = lambd\n",
    "        self.E = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    def update_value(self, episode):        \n",
    "        self.E.fill(0)\n",
    "        for t in range(len(episode)):\n",
    "            state, action, reward, next_state, next_action, done = episode[t]\n",
    "            td_target = reward + (self.gamma * self.Q[next_state][next_action] * (not done))\n",
    "            td_error = td_target - self.Q[state][action]\n",
    "            self.E[state][action] += 1\n",
    "            for s in range(self.env.observation_space.n):\n",
    "                for a in range(self.env.action_space.n):\n",
    "                    self.Q[s][a] += self.alpha * td_error * self.E[s][a]\n",
    "                    if next_action is not None:\n",
    "                        self.E[s][a] *= self.gamma * self.lambd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = SARSALambda(env)\n",
    "agent.evaluate_policy(pi)\n",
    "new_pi = agent.improve_policy()\n",
    "new_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 왓킨스의 Q(λ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WatkinsQLambda(SARSA):\n",
    "    def __init__(self, env, gamma=0.9, alpha=0.1, epsilon=0.1, lambd=0.9):\n",
    "        super().__init__(env, gamma, alpha, epsilon)\n",
    "        self.lambd = lambd\n",
    "        self.E = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    def update_value(self, episode):\n",
    "        self.E.fill(0)\n",
    "        for t in range(len(episode)):\n",
    "            state, action, reward, next_state, next_action, done = episode[t]\n",
    "            best_next_action = np.argmax(self.Q[next_state])\n",
    "            td_target = reward + (self.gamma * self.Q[next_state][best_next_action] * (not done))\n",
    "            td_error = td_target - self.Q[state][action]\n",
    "            self.E[state][action] += 1\n",
    "\n",
    "            for s in range(self.env.observation_space.n):\n",
    "                for a in range(self.env.action_space.n):\n",
    "                    self.Q[s][a] += self.alpha * td_error * self.E[s][a]\n",
    "                    if best_next_action == next_action:\n",
    "                        self.E[s][a] *= self.gamma * self.lambd\n",
    "                    else:\n",
    "                        self.E[s][a] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = WatkinsQLambda(env)\n",
    "agent.evaluate_policy(pi)\n",
    "new_pi = agent.improve_policy()\n",
    "new_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모형기반 강화학습\n",
    "\n",
    "### Dyna-Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class DynaQ(QLearning):\n",
    "    def __init__(self, env, gamma=0.9, alpha=0.1, epsilon=0.1, n_planning_steps=5):\n",
    "        super().__init__(env, gamma, alpha, epsilon)\n",
    "        self.n_planning_steps = n_planning_steps\n",
    "        self.model = defaultdict(lambda: defaultdict(lambda: (0, 0, 0)))\n",
    "\n",
    "    def update_model(self, state, action, reward, next_state, done):\n",
    "        self.model[state][action] = (reward, next_state, done)\n",
    "\n",
    "    def planning_step(self):\n",
    "        for _ in range(self.n_planning_steps):\n",
    "            if len(self.model) == 0:\n",
    "                continue\n",
    "            # 상태, 행동을 무작위로 샘플링\n",
    "            state = random.choice(list(self.model.keys()))\n",
    "            action = random.choice(list(self.model[state].keys()))\n",
    "            reward, next_state, done = self.model[state][action]\n",
    "\n",
    "            td_target = reward + (self.gamma * np.max(self.Q[next_state]) * (not done))\n",
    "            td_error = td_target - self.Q[state][action]\n",
    "            self.Q[state][action] += self.alpha * td_error\n",
    "\n",
    "    def update_value(self, episode):\n",
    "        for t in range(len(episode)):\n",
    "            state, action, reward, next_state, done = episode[t]\n",
    "            td_target = reward + (self.gamma * np.max(self.Q[next_state]) * (not done))\n",
    "            td_error = td_target - self.Q[state][action]\n",
    "            self.Q[state][action] += self.alpha * td_error\n",
    "            self.update_model(state, action, reward, next_state, done)\n",
    "            self.planning_step()\n",
    "\n",
    "    def control(self, policy, num_episodes=1000):\n",
    "        for _ in range(num_episodes):\n",
    "            episode = self.generate_episode(policy)\n",
    "            self.update_value(episode)\n",
    "        return self.improve_policy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = DynaQ(env)\n",
    "agent.evaluate_policy(pi)\n",
    "new_pi = agent.improve_policy()\n",
    "new_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trajectory Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynaQTrajectory(DynaQ):\n",
    "    def __init__(self, env, gamma=0.9, alpha=0.1, epsilon=0.1, n_planning_steps=5, \n",
    "                 trajectory_length=5):\n",
    "        super().__init__(env, gamma, alpha, epsilon, n_planning_steps)\n",
    "        self.trajectory_length = trajectory_length\n",
    "\n",
    "    def sample_trajectory(self, start_state, start_action):\n",
    "        trajectory = []\n",
    "        state = start_state\n",
    "        action = start_action\n",
    "        for _ in range(self.trajectory_length):\n",
    "            if state not in self.model or action not in self.model[state]:\n",
    "                break\n",
    "            reward, next_state, done = self.model[state][action]\n",
    "            trajectory.append((state, action, reward, next_state, done))\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "            action = np.argmax(self.Q[state])\n",
    "        return trajectory\n",
    "\n",
    "    def planning_step(self):\n",
    "        for _ in range(self.n_planning_steps):\n",
    "            if len(self.model) == 0:\n",
    "                continue\n",
    "            start_state = random.choice(list(self.model.keys()))\n",
    "            start_action = random.choice(list(self.model[start_state].keys()))\n",
    "            trajectory = self.sample_trajectory(start_state, start_action)\n",
    "            for t in range(len(trajectory)):\n",
    "                state, action, reward, next_state, done = trajectory[t]\n",
    "                td_target = reward + (self.gamma * np.max(self.Q[next_state]) * (not done))\n",
    "                td_error = td_target - self.Q[state][action]\n",
    "                self.Q[state][action] += self.alpha * td_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = DynaQTrajectory(env)\n",
    "agent.evaluate_policy(pi)\n",
    "new_pi = agent.improve_policy()\n",
    "new_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 퀴즈\n",
    "\n",
    "<iframe src=\"https://tally.so/embed/wgZDV1?alignLeft=1&hideTitle=1&transparentBackground=1&dynamicHeight=1\" loading=\"lazy\" width=\"100%\" height=\"1600\" frameborder=\"0\" marginheight=\"0\" marginwidth=\"0\" title=\"[RL] 제어 문제\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
