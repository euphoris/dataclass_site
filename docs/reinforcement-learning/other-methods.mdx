# 다른 방법들

이번 시간에는 우리가 다양한 강화학습 방법을 알아봤는데 뭐 다른 방법들도 있어요

## 진화 알고리즘

여러 가지 다른 방법들인데 우리가 이제 대표적인 방법이 진화 알고리즘입니다

강의자료 463쪽 우리가 이제 진화가 여러 가지 메커니즘으로 돌아가는데 진화의 메커니즘이 여러 가지가 있는데요

그 중에 하나가 이제 자연 선택입니다 자연 선택은 다 아시다시피 여러 명이 이렇게 있으면 그 중에 어떤 녀석은 죽고 어떤 녀석은 자식을 많이 낳고 그러면은 그 다음 세대에는 좀 살아남은 녀석의 자식들이 많겠죠

그 유전자를 많이 물려받을 거고 살아남지 못하면 유전자가 넘어가지 않습니다

먼저 그 전에 진화라는 단어를 정확한 뜻을 설명을 드리면 이거는 그 세대에 따른 유전자 빈도의 변화에요 정확한 생물학적 의미는 비율의 변화라고 해야겠죠

그래서 진화는 지금 이 순간에도 계속 일어나고 있습니다

왜냐면 예를 들면은 AB형 AB5식 혈액형인데 A형이 30%였고 B형이 25%였고 O형이 10%고 AB형이 몇% 이렇게 돼 있는데 다음 세대에도 이 비율이 정확하게 일치하지는 않겠죠

그죠 뭐 A형이 자식을 좀 더 많이 낳는다든가 B형이 자식을 좀 더 많이 낳는다든가 아주 미세한 이유 때문에 29대 26대 이렇게 돼도 생물학적인 의미에서는 진화가 일어난 겁니다

실제로 이런 진화도 되게 중요해요

왜냐면 우리 멸종위기종들 있죠

멸종위기종이 있는데 개체수가 10상 밖에 안 남았어요

그래서 수컷 10상 암컷 10상인데 수컷이 10마리 있다고 칩시다

수컷 10마리가 유전자들이 있는데 이 중에 한 마리가 어디 저기 물 먹다가 갑자기 목에 산에 들려서 죽어버렸어요

그러면 유전적 다양성이 10에서 9로 확 줄어드는 거죠

이것도 진화입니다

그리고 생각보다 이 힘이 굉장히 강해요

왜냐면 대부분 동식물들이 개체수가 생각보다 그렇게 많지가 않거든요

저기 뭐...

무슨...

뭐 모기, 바퀴벌레 이런 건 개체수가 많지만 인간도 지금 개체수가 이렇게 많이 늘은 거는 굉장히 극히 최근입니다

사실 그죠 우리가 몇십억이 인구가 지구상에 산 적이 없어요

심지어 지구상에 지금까지 인류가 탄생한 이래로 존재했던 인간들을 다 세 보면 그 인간들 중 대부분이 현재 살아 있습니다

무슨 말인지 이해가 되시나요?

인간 증가가 이렇게 증가했기 때문에 우리가 현재 살아있는 사람들이 대부분 이 시기에 있었던 사람들보다 더 많아요

충격적이죠

그 정도로 지금 많은 사람들이 살고 있는데 바꿔 말하면 과거에는 인간이 개체수가 굉장히 적었다는 거예요

아주 희박하거든요

그래서 인간들 유전적 다양성을 조사를 해보면 거의 멸종위기종 수준의 유전적 다양성을 가지고 있습니다

그래서 추측에 따르면 한 20만 년 전쯤에 인간이 멸종위기를 한번 겪었다는 추측이 있어요

왜냐면 유전자의 종류를 세 보니까 몇 종류 안 돼요

사람은 80억 명인데 그러면 그 정도 유전자 수면은 아 우리 조상들이 몇 명 없었다

몇 명 없었던 시기를 한번 크게 겪었다는 걸 알 수 있고 그다음에 이제 그런 데서 약간 파생되는 생물학에 재밌는 게 있는데 생물학 퀴즈 하나 내볼까요?

아프리카가 있고 우리 동아시아가 있죠

그럼 어느 쪽이 더 유전적 다양성이 높을까요?

보통 우리가 이제 아프리카 하면 뭔가 아프리카를 한 덩어리를 얘기하는데 아프리카의 다양성이 훨씬 높습니다

왜냐하면 여기가 인류의 발상지이기 때문에 인류가 최초에 가지고 있던 유전자는 아프리카에는 다 있어요

거의 있어요

물론 중간에 없어진 유전자도 있겠지만 그 중에 일부가 떨어져 나와서 중간에 여러 가지 경로를 거쳐서 우리한테까지 오거든요

굉장히 소수의 사람들이 이쪽 동아시아 동아시아는 거의 대륙의 제일 끝단이잖아요

여기까지 온 우리 조상들은 굉장히 소수였어요

근데 이 사람들이 자식을 낳아서 지금 중국인도 되고 한국인도 되고 일본인도 되고 이런 거거든요

그래서 우리는 굉장히 이 유전자들 중에 일부만 가지고 있습니다

물론 그 중에 오다가 변이가 생겨가지고 피부색도 좀 달라지고 했지만 그래서 유전적 다양성이 이쪽이 더 많아요

훨씬 더 다양한 사람들이 있습니다

어쨌든 생물학에서 진화와 관련해서 재미난 사실들이 많이 있는데 그 중에서 자연 선택 그러니까 살아남는 어쨌든 환경에 잘 뿌려놓으면 살아남는 놈들이 있을 것이다

그럼

그 살아남은 놈들이 그 환경에 잘 맞는 것이다

이런 거 알 수 있죠

우리가 이제 역으로도 알 수 있는데 이렇게 생겨먹은 걸 보니까 얘는 어떤 환경에 적응했겠구나

이런 걸 알 수 있죠

그래서 우리가 고래하고 물고기 같은 걸 보면 고래하고 물고기는 개통이 다르죠

고래는 포유류고 물고기는 어류인데 생긴 게 비슷하게 생겨서 옛날 사람들은 구별을 못 했단 말이에요

왜 그러냐면 물이라는 환경의 압력이 굉장히 강하기 때문에 거기 적응하려면 조상이 포유류고 어류고 뭐 상관없고 다 똑같이 생겨야 된단 말이에요

그래서 자연 선택이 굉장히 강력한 힘이고 그래서 지금 강화학습을 할 때는 보면 정책을 어떻게 짜고 이렇게 해가지고 데이터를 어떻게 모아서 Q를 추정을 하고 온갖 복잡한 방법을 쓰는데 진화 알고리즘은 그냥 쿨하게 자식을 많이 만듭니다

그리고 환경에 뿌려놔요

그럼 뭔가 보상을 받겠죠

그럼 보상을 적게 받은 애들은 그냥 다 죽여요 보상이 많으니 받은 애들은 일종의 자식을 만듭니다

자식을 만드는데 그대로 복사를 하는 게 아니라 약간 랜덤하게 조금씩 정책을 바꿔줘요 그러면 그 짓을 계속하면 약간 자연 선택 같은 게 일어나서 결국엔 살아남는 놈이 환경에 딱 맞는 놈이다

이거죠

그러면 여러 가지 장점이 있는데 일단 탐색을 굉장히 강하게 할 수가 있습니다

그리고 우리가 강화학습에서는 기본적으로 내가 한 행동에 따라서 정책에 따라서 행동이 결정되고 행동에 따라서 데이터가 결정되고 다시 데이터를 바탕으로 정책을 만드니까 뭐 우리가 성능 붕괴라든가 이런 현상도 생기고 그걸 줄이기 위해서 여러 가지 알고리즘들을 만들어 놓고 하는데 진화 알고리즘은 그냥 막 뿌려놓고 사람은 이기는 편 우리 편 잘한다

잘한다 이러고 얘는 이제 보상 못 받으면 얘네 죽여요

얘네가 또 받으면 얘는 이제 자식을 낳는데 똑같이 낳는 게 아니라 약간씩 다르게 낳는단 말이에요

또 이기는 편 우리 편 잘한다

잘한다

이래서 또 못하는 애들은 다 죽여요

잘하는 애들만 남겨서 조금씩 다르게 하고 이렇게 하다 보면 예를 들면 우리가 여기 최적점이다 계속 도련 변이가 생겨나다가 죽이고 도련 변이가 생기고 하다 보면 여기까지 가는 애가 나오겠죠

그러면 얘가 제일 잘하면 얘만 남기면 됩니다

굉장히 간단하고 우리가 이걸 하기 위해서 무슨 경사상승법이니 복잡한 알고리즘 하나도 필요 없어요

그러면 좋은 점만 있느냐

반대로 굉장히 표집 비용이 큽니다

즉 시행착오를 훨씬 많이 해야 되겠죠

왜냐하면 되게 똘똘한 녀석인데 운이 나빠가지고 죽을 수도 있잖아요

항상 이 불확실한 환경에서 하기 때문에 그러면 얘가 정말 환경에 잘 만질 부르면 굉장히 시행착오를 많이 해야 되는데 이게 지금 굉장히 방향성 없는 과정이란 말이에요 눈먼 과정이기 때문에 그래서 이 샘플링을 굉장히 많이 해야 되는 것이 단점이다

그러면 샘플링을 적게 하면 어떻게 되느냐 하면 집단의 크기를 줄이면 계산량은 줄어드는데 좀 이상한 애들, 삐리리한 애들이 적합한 애가 운이 나빠서 죽을 수가 있기 때문에 이상하게 삐리리한 애들이 늘어날 수도 있어요

그래서 이건 유전적 부동이라고 하는데 지네틱 드리프트 아까 그 얘기했죠

멸종위기종이 10마리 남았는데 원래 얘가 제일 똑똑하고 잘 났는데 우연히 발을 헛짖어서 죽었어요

그럼 얘 유전자에 문제가 있는 거 아니잖아요

그냥 얘가 운이 없었던 거지

그래도 그 유전자는 이제 없어졌습니다

그러면 유전자의 빈도가 변했는데 얘가 못해서 없어진 게 아니란 말이죠

## 모방학습
그래서 이렇게 하는 방법이 진화 알고리즘이고 또 다른 방법은 모방학습인데 모방학습은 우리가 지금까지는 강화학습으로 어떻게 해보려고 했는데 어쨌든 인간이 잘 한단 말이에요

대부분의 문제에서 사람이 굉장히 잘합니다

그래서 사람하고 좀 비슷하게 해보자 라는 것이 이제 다 모방학습에 속하고요 이 모방학습의 장점은 우리 아침에도 그 얘기를 했는데 이 보상을 결정하기 굉장히 어렵거든요

배의 항로를 결정을 해야 되는데 예를 들면 여기 장애물이 있어요 장애물에 갖다 박으면 이거 몇 점이냐 항구까지 가면 100점 장애물에 갖다 박으면 마이너스 100점 마이너스 1000점 애매하단 말이야

그래서 이런 보상을 매기는 게 굉장히 까다로운 문제인데 전문가를 모방하면 전문가가 이렇게 가면 그냥 가면 되는 거죠

보상 이런 거를 우리가 신경 쓸 필요가 없습니다

### 행동복제
그래서 다시 모방학습을 나누면 보통 한 세 가지 정도 그 외에도 여러 가지가 있지만 여기서 세 가지 정도 소개해드리면 제일 처음에 행동복제는 이제 이름은 멋인데 behavior cloning 이름은 멋있죠

사실은 그냥 되게 간단한데 그냥 지도학습을 하는 거예요

그래서 전문가가 시연을 보여줍니다

배를 아 놔 봐

내가 배 10년 몰았어

이럴 때는 이 코스로 가는 게 좋아

왜요 그게 좋아 날 믿어

그래서 전문가가 시연을 해서 보여주는 거야

딱 보여주고 이때는 어떻게 하시겠습니까

이때는 이렇게 가는데 약간 더 트는 게 좋을 것 같은데 이렇게 코스를 만드는 그런 데이터를 많이 쌓아 가지고 지도학습을 시키면 똑같이 따라 하겠죠

근데 이거의 단점은 뭐냐면 예를 들면 우리가 데이터를 여기서 여기까지 가는 걸로 전문가한테 데이터를 받아 와서 AI를 지도학습을 시켰더니 갑자기 여기에서 이리로 가야 돼요

그러면은 우리 데이터가 없잖아요

그러면은 어떻게 가야 되냐

이런 문제가 생기게 됩니다

그래서 시연 데이터에 포함되지 않은 상태는 적용이 안 되겠죠

근데 이렇더라도 보통 어떤 우리가 출발점으로 쓸 수가 있습니다

지금까지는 강화학습을 할 때 그냥 밑도 끝도 없이 강화학습도 했는데 이렇게 행동복제를 일단 해놓고 우리도 그렇잖아요

뭐 배울 때 선생님한테 선생님이 자 보고 따라 하세요

리슨 앤 리피트 하면서 똑같이 따라 하다가 그 다음에 자신이 생기면 자기가 새로운 문제도 풀어보고 경험을 쌓아가는 거잖아요

행동복제를 먼저 하고 강화학습을 하는 것이 괜찮은 선택일 수 있습니다

실제로도 많이 하고요 알파고도 그런 식으로 만들었죠

### 역강화학습

그 다음에 이거는 굉장히 어려운 문제인데 역강화학습이라는 게 있어요

우리가 강화학습을 할 때 어려운 게 보상을 결정하는 거거든요

아까 벽에 갖다 박으면 몇 점을 줘야 되고 꼴에 가면 몇 점을 줘야 되냐

우리가 인위적으로 점수를 매겼는데 그 점수는 우리 머릿속에서 나온 거니까 굉장히 주관적이고 사람마다 생각이 다를 거란 말이에요

저는 항구에 도착하면 100점 암초에 갖다 박으면 마이너스 1000점 이랬는데 10배 밖에 안 되죠

어떤 사람은 아니 그게 왜 10배 밖에 안 되냐 100배는 줘야지

이런 사람도 있을 거고 아니 항구에 가나 암초에 박으면 똑같이 플러스 마이너스 100점 하면 되지 왜 어떤 건 10배 주냐

누구 마음이냐

이럴 수도 있어요


그래서 이 보상함수가 구하기가 어려운데 역강화학습은 어떻게 하냐면 지도학습에서는 전문가의 시연을 그대로 따라 하는데 역강화학습은 역으로 하는 겁니다

이 전문가도 뭔가 강화학습을 했을 거란 말이에요 생물학적 또는 심리학적 강화학습을 했겠죠

우리가 추측을 하는 겁니다

이 사람의 행동을 보고 도대체 이 사람은 어떤 보상함수를 따르고 있을 것인가

그래서 이제 가정을 하는데 어떻게 가정을 하냐면 저 사람이 최적으로 행동을 하고 있을 것이다

최적 행동을 하고 있을 것이다

이렇게 가정을 합니다

그러면 예를 들면 어떤 선장님한테 이럴 때 배를 어떻게 몰아야 될까 하면 이렇게 몰면 되지 라고 대답을 한다면 그럼 이 사람은 요것도 아니고 요것도 아니고 요것도 아니고 요것도 아니고 이걸 고른 거잖아요

그러면 이 사람의 행동이 베스트일 거라고 일단 가정을 합니다

물론 베스트 아닐 수도 있지만 베스트일 거라고 가정하면 그럼 이게 보상이 100이면 이 옆으로 가면 90이고 더 멀면 80이고 이렇게 되겠죠

점점 여기서 멀어질수록 보상이 떨어질 거다

그러면 그런 거를 만족하는 어떤 수학적인 함수를 찾아내는 겁니다

우리가 직접 찾아내는 거 아니고 이제 머신러닝으로 찾아내는 거죠

그래서 이제 그렇게 하는 건데 이게 생각보다 어려운 문제가 보상함수가 유일하지 않은 경우가 생기거든요

무슨 말이냐면 예를 들어서 선장님이 이렇게 가서 갔는데 이게 100점이다

근데 이 옆으로 가는 것도 100점이어도 상관없다는 거죠

왜냐하면 둘 다 100점이면 이리로 가는 건 자기 마음이잖아요

그래서 모든 행동에다가 똑같은 보상을 주면 전부 최적이 될 수도 있습니다

그래서 약간 제약 조건을 걸어요 보상함수라는 것은 모름직이 예를 들면 선형이어야 된다든가 어떤 형태야 된다든가 우리가 이제 요로례로 형태야 돼 예를 들면 이렇게 가운데가 볼록한 형태가 돼야 돼

이런 식으로 제약 조건을 줘서 약간 인위적인 보정을 하는 거죠

그래서 보상함수가 유일한 형태가 되도록 제안을 하게 됩니다

그다음 생성적 적대 모방학습 아주 어려운 이름의 방법인데요

### 생성적 적대 모방학습

이거는 어떻게 하는 거냐면 원래 생성적 적대 신경망이라는 게 있습니다

GAN이라고 하는데요 요거의 원래 기원은 Adversarial Attack이라고 해서 적대적 공격이라는 것에서 나오는데 이제 어떤 AI 연구자들이 어떤 발견을 하냐면 예를 들면 여기 판다 그림이 있는데 판다

그림에다가 아주 미세한 노이즈를 섞은 다음에 AI한테 보여주니까 원숭이라고 잘못 분류하는 경우가 있더라는 것 그래서 이런 걸 적대적 공격이라고 합니다

그래서 재밌는 거 하나 보여드리면 이게 이제 YOLO라는 기술인데 이미지 처리에서 오브젝트 디텍션 기술이에요

사람이 있으면 사람을 탐지를 해줍니다

그래서 이렇게 박스가 쳐져 있는 건 여기가 사람이다 라는 건데 지금 이 사람은 탐지가 안 되고 있죠

테두리가 안 쳐져 있습니다

왜냐하면 이 사람 배에 어떤 그림이 그려져 있는데 이게 어떤 일종의 AI를 속이는 노이즈예요

이 노이즈를 인위적으로 알고리즘으로 찾아낸 겁니다

특정 AI를 주면 AI를 속이는 노이즈를 만들 수 있어요

이걸 배에다 이렇게 붙이고 있으면 YOLO 알고리즘을 적대적 공격을 해서 AI가 못 알아보입니다

그러니까 AI로 우리가 감시카메라 같은 걸 달아서 금고 이런 데 앞에 달아놓으면 이거 하나 들고 들어가면 AI는 안 보여요

못 봐요

그래서 의자 이런 거 다 인식하는데 이 사람만 인식을 못하죠

아마 대학원생들이거나 이렇게 했죠

그래서 이걸 자기 배에서 떼서 돌리면 사람으로 인식이 됩니다

종이에 대충 프린트해서 너무 허접하다 다시 하면 이렇게 하고 그래서 이걸 옆 사람한테 주면 이 사람은 인식이 되고요 옆 사람이 받으면 옆 사람이 인식해서 풀려납니다

신기하죠

그래서 이걸 적대적 공격이라고 하는데 이게 굉장히 무시무시한 게 예를 들면 자율주행차가 있는데 자율주행차를 속일 수 있는 문의가 있다

그럼 내 차 뒤에다가 붙이고 다니면 남들이 가서 갖다 봤겠죠

그럼 보험사기 같은 것도 할 수 있고 아니면 누굴 죽이고 싶다

근데 증거 없이 죽이고 싶어요

그 사람이 자율주행차를 탄다고?

그럼 그 사람 자율주행차의 AI를 분석을 해서 어디 절벽에다가 스티커 딱 붙여놓으면 그 사람 어디 갈 때?

절벽 지날 때 코너에서 그냥 점프로 점프할 수도 있겠죠

굉장히 위험한 기술이 됩니다

그래서 이게 밝혀지고 나니까 사람들이 그 생각을 하는 거예요

어떤 적대적 공격이 들어오면 이게 진짜인지 리얼인지 적대적 공격인지를 AI로 구별하면 되지 않겠냐 AI를 만들어서 진짜와 가짜를 거짓을 구별하는 AI를 만들자

이 생각을 합니다

그러니까 사람들이 그 생각을 하죠 뭔 소리야

그럼 얘를 또 속이는 적대적 공격을 하면 돼

얘도 AI인데 AI를 속이는 또 AI를 만들면 되지

그러니까 그래?

AI를 속이는 AI를 탐지하는 AI를 만들면 되지

장난하냐?

그러면 그 AI를 그래서 이제 약간 모순의 문제죠

창과 방패가 있는데 AI를 뚫는 창과 그 창을 막는 방패를 만들어가지고 둘이 싸움을 시키면 누가 이겨요?

이런 거예요

계속 싸우면 창은 점점 더 날카롭고 예리해질 거고 방패는 점점 더 강해질 거죠

그러면 이 노래의 끝은 어디냐

이런 거예요

그래서 끝이 어디냐 해서 이 아이디어가 생성적 적대 신경망이다

둘이서 경쟁을 시키는 구조로 학습을 시키면 얘도 엄청 세지고 얘도 엄청 세지지 않겠어?

약간 이런 거예요

그래서 얘가 세지면 굉장히 사람도 구별할 수 없는 지금 아까는 우리가 적대적 공격하고 있던 티가 너무 나잖아요

앞에 뭐 이상한 거 붙이고 있고 그런 티가 전혀 안 나고 AI를 속일 수 있는 뭔가가 만들어져 정말 사람이 없는 것처럼 보이는 누가 봐도 저기 아무도 없는데?

이렇게 보이는 뭔가를 만들 수 있을 거고 얘는 그 와중에도 아무도 없긴 여기 보라고 사람 있지

알아서 찾을 수 있는 그런 게 나올 겁니다

그래서 이 생성적 적대 신경망 이 아이디어가 원래는 주로 어디서 쓰느냐 하면 이미지 생성 이런 데서 쓰는데 예를 들면 이 사람 얼굴은 적대적 그 방법으로 만든 거야

사람 얼굴처럼 보이는 이미지를 만드는 애가 있고 가짜로 만드는 애가 있고 그거를 이게 진짜 사람인지 아니면 AI가 만든 가짜 얼굴인지를 탐지하는 AI가 있어 둘이 경쟁을 계속하면 정말 구별하기 힘들 정도로 리얼한 이미지를 만듭니다

보통 이런 거 구별하는 방법은 귀를 보시면 돼요

지금 보시면 귀에 있는 귀걸이 모양이 다르거든요

아니면 눈동자를 보시면 눈동자에 반사되는 빛의 형태가 다릅니다

그런 거 여러분들이 나중에 이제 뭐 보이스피싱 이런 거 당하실 때 이제 점점 그게 가능성이 있거든요

여러분 얼굴로 AI 딥페이크로 만들어서 음성합성 해가지고 엄마 돈 좀 빌려줘

나 교통사고 났어

뭐 이러면 눈동자를 잘 보세

요 눈동자와 귀모양 왜 그러냐 하면 약간 현재 AI 이미지 생성 방식 특징상 가까운 데 끼리 서로 영향을 주면서 만들거든요

약간 이렇게 멀리 떨어져 있으면 걔네끼리 통일성 유지하는 게 조금 어려운 기술입니다

그래서 보면 이렇게 약간 멀리 떨어진 것들이 통일성이 깨져요

그것도 언젠가는 또 극복이 되겠죠

보시면 눈동자에 빛 반사 패턴도 좀 다르고 이 사이트는 새로 고침할 때마다 얼굴을 하나씩 이렇게 근데 보면 항상 귀걸이 모양이 다르죠

여러분들도 남자도 귀걸이를 차야 될 시대가 올지도 보안을 위해서 그렇고 그래서 이제 이런 기술이 있는데 이미지 생성하고 사람 얼굴을 가짜로 만들고 다 좋은데 이거를 강학습에다가 적용을 해보자 이거죠

생성적적 대모강학습 그래서 앞에 행동 복제는 그냥 지도학습으로 똑같이 사람을 따라 하는데 여기서 어떻게 하냐면 이거는 어떤 사람의 행동이고 이거는 AI의 행동이에요

그럼 우리가 딱 보면 사람은 사람과 같이 행동하는데 AI는 뭔가 로봇 같고 어색하고 그럴 거 아니에요

그럼 이 앞단에 AI를 붙여서 이걸 구분자라고 하는데 구분자가 딱 보고 이건 사람이 했네

이건 네가 딱 봐도 AI네

그러면 이 AI가 절대 속여주겠어

이러면서 얘를 좀 속일 수 있는 방향으로 학습을 합니다

그럼 얘가 또 구별하고 얘가 속이고 얘가 구별하고 얘가 속이고 그러다 보면 이 AI의 행동하고 사람의 행동하고 진짜 완전히 똑같아지면 얘가 구별을 못하게 되겠죠

물론 그렇게 되기는 쉽지 않은데 만약에 그렇게 된다고 하면 AI가 인간을 완전히 모방을 할 수 있게 될 것 같아요

그래서 이제 이런 게 생성적적 대모강학습 그래서 generative adversarial imitation learning 해서 Gale이라고 하는데 이런 아이디어가 되겠습니다

아직까지 이런 아이디어들이 그렇게 성공적이지는 못해요

기존에 강학습보다 잘 되면 더 쉬운데 잘 되기가 또 이거 나름대로 어려운 점들이 있습니다

예를 들면 Gale 같은 경우는 AI 이쪽 공격하는 AI 그러니까 창과 방패의 싸움이라서 얘가 처음부터 너무 빨리 뾰족해지면 방패가 해보기도 전에 뚫려 버려가지고 아무것도 안 돼요

내가 도저히 알 수가 없고 처음부터 방패가 너무 튼튼해 버리면 방패가 좀 뭔가 칼이 들어가야 어?

이리로 들어가는 하면서 이쪽을 더 날카롭게 할 텐데 어디를 찔러도 안 들어간다 그러면 학습할 수가 없습니다

그래서 생각보다 학습이 좀 잘 안 돼요

근데 어쨌든 이런 방식의 시도들이 이루어지고 있다

이렇게 정리할 수가 있겠습니다

질문 있으시면 질문해 주세요

Gale에서 배시를 그렇게 비셨는데 창을 아무리 찔러도 방패가 안 뚫린다고 하셨는데 저희는 앞전에 배웠던 그 길로 가고 싶지 않았는데 그 길로 갔으니까 어?

그 길이야?

나 활동을 했으니까 그것처럼 저기도 뚫리지 않았는데 꼭 뚫려 버렸네?

라고 해주시면 안 돼요?

그걸로 테이크로 교육을 시키는 거예요?

원래 그런 알고리즘은 아니지만 또 어떤 문제가 생기냐면 약간 사람도 마찬가지인데 우리 어렸을 때 친구들 중에 보면 연습장에 만화 그리는 친구들이 있거든요

연습장에 만화 그리는 친구들 꼭 한반에 한두 명 있잖아요

근데 보면 맨날 얼굴을 같은 구도로 밖에 못 그려요 옆으로 막 이런 구도는 그리는데 정면 구도는 못 그린다든가

근데 왜 이렇게 안 그려?

왜 이렇게 안 그려?

옆으로 막 이런 구도는 그리는데 정면 구도는 못 그린다든가 왜냐면 옆으로 그리면 사람들이 다 친구들이 너 그림 잘 그린다

그러니까 맨날 이 구도만 그리는 거야 창과 방패도 마찬가지인데 그림 그리기 해보면 어떤 경우가 생기냐면 예를 들면 여기가 뚫리면 이쪽으로도 뚫어보고 이쪽으로도 뚫어보고 해야 되는데 얘는 어쨌든 속이기만 하면 땡큐거든요

이쪽만 계속 뚫는 거야

그래서 햄버거를 그려봐 하면 맨날 똑같은 구도에 햄버거 밖에 못 그립니다

왜냐하면 그게 속으니까 햄버거 구도 좀 바꿔봐 하면 그건 못 그려요

그런 문제도 생기고 그래서 이게 이론상은 그럴싸한데 잘 안 되는 점이 많아요

그래서 요즘에 그림 그리기 할 때도 이 방식으로는 더 이상 잘 안 그립니다

요새 AI가 그림 그려라 하면 그림 그리는 방식이 좀 달라요


## 퀴즈

<iframe src="https://tally.so/embed/wdjNRy?alignLeft=1&hideTitle=1&transparentBackground=1&dynamicHeight=1" loading="lazy" width="100%" height="1300" frameborder="0" marginheight="0" marginwidth="0" title="[RL] 다른 방법들"></iframe>